[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "con-gen-csu",
    "section": "",
    "text": "Welcome!\nWelcome to the website for the National Marine Fisheries Service Linux, Slurm, and Bioinformatics training to be held virtually over three days:"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "con-gen-csu",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course will be using the Sedna high-performance computing cluster located at the Northwest Fisheries Science Center. This cluster (and, hence, this course) is only available to NMFS employees and affiliates. If you are a NMFS employee and you are interested in this course, please see here for information about how to get an account on the cluster.\nThis course is intended for people who have already had some exposure to Unix or Linux. You should be reasonably comfortable navigating around the Unix filesystem using the command line. For a refresher, please read this chapter from my online bioinformatics book.\nMy goal is to:\n\nteach the shell programming constructs and the text processing tricks that I find myself using all the time in my day-to-day work\nprovide an introduction to how to use SLURM to do cluster computing\nOn the last day, show how Snakemake works, and how it can be used on the Sedna cluster to simplify your bioinformatics life."
  },
  {
    "objectID": "index.html#course-topics-and-sessions",
    "href": "index.html#course-topics-and-sessions",
    "title": "con-gen-csu",
    "section": "Course Topics and Sessions",
    "text": "Course Topics and Sessions\n\nDay 1: Intro, Unix-review, shell programming, awk\n\nIntroduction to the Sedna cluster (15 minutes Krista and Giles)\n\nCluster infrastructure and configuration.\nScientific software and the installation requests\n\nQuick Unix Review (25 minutes)\nShell Programming (50 Minutes)\nA Brief awk Intro Processing text files with awk (30 minutes)\n\nDay 2: A little bash stuff, then Sedna and SLURM\n\nBash scripts and functions (20 minutes)\nSedna and SLURM intro (40 minutes)\nSubmitting jobs with sbatch (40 minutes)\nSlurm Job Arrays (40 minutes)\n\nDay 3: Job Arrays, then an introduction to Snakemake\n\nSlurm Job Arrays (25 minutes)\nSnakemake Tutorial Introduction (90 minutes)"
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#what-is-bash",
    "href": "nmfs-bioinf/quick-unix-review.html#what-is-bash",
    "title": "1  Quick Unix Review",
    "section": "1.1 What is bash?",
    "text": "1.1 What is bash?\nWe start by acknowledging that there are many different flavors of Unix and Linux. I will refer to them all simply as Unix or unix.\nAlso, there are a number of different shells for Unix. The shell is the part that interprets commands.\nWe will be talking about the bash shell. This is the default shell on Sedna, and it is also the most popular shell for bioinformatics.\nBash stands for “Bourne-again shell”. It is an update to an earlier shell called the Bourne shell."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#setting-up-our-workspace",
    "href": "nmfs-bioinf/quick-unix-review.html#setting-up-our-workspace",
    "title": "1  Quick Unix Review",
    "section": "1.2 Setting up our workspace",
    "text": "1.2 Setting up our workspace\n\nI have prepared a repository with a few different example data files that we be using.\nIt also contains all these notes.\nI want everyone to download it to their home directory and then cd into its playground directory, where we will be playing today and tomorrow.\n\nAfter logging onto Sedna:\n\n\nPaste this into your shell\n\ncd ~\ngit clone https://github.com/eriqande/nmfs-bioinf-2022.git\ncd nmfs-bioinf-2022/playground\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen we say “Paste this into your shell” or “Type this at your command prompt” we also implicitly mean “Hit RETURN afterward.”\n\n\n\nThis is where our working directory will be for the next two days.\nUse the tree utility to see the files that we have to play with within this playground:\n\n\n\nType this command at your prompt\n\ntree\n\nThe data directory has a few things that we will be using for examples.\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ncd: change directories\ngit: run git subcommands, like clone with this. In the above case it clones the repository that is found at the GitHub URL.\ntree: Super cool “text-graphical” directory listing"
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#a-motivating-example",
    "href": "nmfs-bioinf/quick-unix-review.html#a-motivating-example",
    "title": "1  Quick Unix Review",
    "section": "1.3 A motivating example",
    "text": "1.3 A motivating example\n\nThe data/samtools_stats directory has gzipped output from running the samtools stats program on 30 different samples.\nThis provides information about reads that have been mapped to a reference genome in a BAM file.\n\nTo see what those files look like:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\n\n\nHit the SPACE-bar to go down a screenful, the b key to go back up\nMost terminal emulators let you use up-arrow and down-arrow to go one line at a time, too.\nHit the q key to quit out of the less viewer.\n\nTo see it without lines wrapping all over the place try this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\n\nNow you can use the left and right arrows to see different parts of lines that are not wrapped on the screen.\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ngzip -cd: decompress gzipped file to stdout (this is what zcat does, but zcat is not portable).\nless: page view. Great to pipe output into. (SPACE-bar, b, q, down-arrow, up-arrow)\n\nless -S: option to not wrap lines. (left-arrow, right-arrow)\n\n\n\n\n\n\n1.3.1 (One of) Our Missions…\nIt is pretty typical that Bioinformatic outputs will be spread as small bits of information across multiple files.\nOne motivating example is summarizing the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped, in all 30 samples, in a table."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#the-anatomy-of-a-unix-command",
    "href": "nmfs-bioinf/quick-unix-review.html#the-anatomy-of-a-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.4 The anatomy of a Unix command",
    "text": "1.4 The anatomy of a Unix command\nNearly every line in a bash script, or every line you type when banging away at the Unix terminal is a command that has this structure:\ncommand options arguments\n\nThe command is the name of the command itself (like cd or less).\nThe options are often given:\n\nwith a dash plus a single character, like -l or -S or -a, -v, -z.\n\nIn most commands that are part of Unix, these single options can be combined, so -cd, is the same as -c -d.\n\nwith two dashes and a word, like --long or --version\nSometimes options take arguments, like --cores 20, but sometimes, they stand alone.\n\nWhen they stand alone they are sometimes called flags.\n\n\nThe arguments are typically file or paths.\n\n\n\n\n\n\n\nSelf-study question\n\n\n\nIdentify the command, options, and arguments in:\ntree -d ..\n\n# and\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz\n\n\n\n\n\n\n\n\nSelf-study answer\n\n\n\n\n\nFirst case:\n\ntree is the command\n-d is the option (print only directory names, not files)\n.. is the argument (one directory level up)\n\nSecond case:\n\ngzip is the command\nThe options are -c and -d, contracted into -cd\ndata/samtools_stats/s001_stats.tsv.gz is the argument\n\n\n\n\n\n1.4.1 What do all these options mean?\nEverything you need to know about any Unix command will typically be found with the man command. For example:\n\n\ntype this at your terminal\n\nman tree\n\n\nThat gives you more information than you will ever want to know.\nIt starts with a synopsis of the syntax, which can feel very intimidating.\n\n\n\n\n\n\n\nBonus Tips:\n\n\n\n\n\n\nMan uses the less viewer for presenting contents of the man pages.\nWhen you are viewing man pages, you can scroll down with SPACE-bar and up with b, and get out with q, just like in less\nTo search for patterns in the manual pages, you can type / then the string you want and then RETURN.\n\nWhen in pattern-searching mode, use n to go to the next occurrence, and N to the previous.\nIf searching for a single letter option try searching with [, ] afterward.\nFor example, to search for the -d flag you would type /, then -d[, ], then hit RETURN. Try it on the tree man page.\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nLearn about gzip\n\nUse man to read information about the gzip command\nFind information about the -c and the -d options.\n\nMaybe even search for those using the “slash-pattern” bonus tip from above.\n\n\nLearn about the ls command\n\nUse man to see information about the ls command, which lists directories and their contents\nFind out what the -R option does. Maybe even look for it using the Bonus Tip above.\nDo the same for the -Q option.\nLook at what those do by doing ls -RQ on your terminal.\n\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\n\n\nman gzip\nTo search for -c, type /-c + return. You might have better results with /-c[, ].\n\nuse n or N to go forward or backward through the occurrences of -c.\n\n\n\nYou would do man ls\nTo search for -R in the man pages, a good way to do it would be to type /-Q + RETURN, or maybe /-Q[, ] + RETURN."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#streams-and-redirection",
    "href": "nmfs-bioinf/quick-unix-review.html#streams-and-redirection",
    "title": "1  Quick Unix Review",
    "section": "1.5 Streams and redirection",
    "text": "1.5 Streams and redirection\n\nWhen you’ve executed the unix commands above, they have typically responded by writing text or data to the terminal screen.\nThe command is actually writing to a stream that is called stdout, which is short for “standard output.”\nIt turns out that, by default, the stdout stream gets written to the terminal.\n\nAha! But here is where it gets fun:\n\nYou can redirect the stdout stream to a file by using &gt; or &gt;&gt; after the command, options, and arguments.\n\nFor example:\n\n\nPaste this into your terminal\n\nmkdir outputs\ntree -d .. &gt; outputs/repo-tree.txt\n\nNow, you can use the less viewer to see what got into the file outputs/repo-tree.txt:\n\n\nType this at the terminal\n\nless outputs/repo-tree.txt\n\nAha! Instead of writing the output to the screen, it just puts it in the file outputs/repo-tree.txt, as we told it to.\n\n\n\n\n\n\nDanger!\n\n\n\nIf you redirect stdout into a file that already exists, the contents of that file will get erased!!!\nFor example, if you now do:\n\n\nPaste this into the shell\n\necho \"New content coming through...\" &gt; outputs/repo-tree.txt\n\nThen you will no longer have the output of the tree command in the file outputs/repo-tree.txt. Check it out with the less command.\n\n\nIf you want to merely append stdout to an existing file, you can use &gt;&gt;. For example:\n\n\nPaste this into your terminal\n\necho \"Add this line\" &gt;&gt; outputs/repo-tree.txt\necho \"And then add another line\" &gt;&gt; outputs/repo-tree.txt\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\nmkdir: make a new directory\n\n(check out the -p option, which means “make any necessary parent directories and don’t complain if the directory already exists.”)\n\necho: print the argument (usually a string) to stdout."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "href": "nmfs-bioinf/quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.6 Pipes: redirecting into another Unix command",
    "text": "1.6 Pipes: redirecting into another Unix command\nAs we have said, many Unix utilities take files as their arguments, and they operate on the contents of that file. They can also receive input from streams, and almost all Unix utilities are set up to accept input from the stream called stdin, which is short for standard input.\n\nThe most important way to pass the stdin stream to a Unix command is by piping the stdout from one command in as the stdin to the next command.\nThis uses the | which is called the “pipe”.\n\nWe have already used the pipe when we did:\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\nPipe syntax is pretty simple:\ncommand1 | command2\nmeans pipe the stdout output of command1 in as stdin input for command2."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "href": "nmfs-bioinf/quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "title": "1  Quick Unix Review",
    "section": "1.7 stderr: The stream Unix uses to yell at you",
    "text": "1.7 stderr: The stream Unix uses to yell at you\n\nIf a Unix command fails, typically the program/command will bark at you to tell you why it failed. This can be very useful.\nThe stream it writes this information to is called stderr, which is short for standard error.\nSome bioinformatics programs write progess and log output to stderr, in addition to actual error messages.\n\nIf you are running a program non-interactively, it is extremely valuable and important to redirect stderr to a file, so you can come back later to see what went wrong, if your job failed.\n\nstderr is redirected with 2&gt;.\nThink of the 2 as meaning that stderr is the second-most important stream, after stdout.\n\n\n\n\n\n\n\nBonus side comment:\n\n\n\n\n\nAs you might imagine, you could redirect stdout by using 1&gt; instead of &gt;, since stdout is stream #1.\n\n\n\nFor example, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. &gt; outputs/repo-tree.txt\n\n\nAha! We get a warning note printed on the screen,\nBecause, stderr gets printed to the terminal by default.\nAlso outputs/repo-tree.txt has been overwritten and is now a file with nothing in it.\n\nSo, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. &gt; outputs/repo-tree.txt 2&gt;outputs/error.txt\n\nNow, look at the contents of both outputs/error.txt and outputs/repo-tree.txt:\n\n\nPaste this into your shell\n\nhead outputs/repo-tree.txt outputs/error.txt\n\n\n\n\n\n\n\nStream operators and commands that we just saw:\n\n\n\n\n\n\n&gt; path/to/file: redirect stdout to file at path/to/file. This overwrites any file already at path/to/file.\n&gt;&gt; path/to/file: redirect stdout to append to file at path/to/file. If path/to/file does not exist, it creates it and then adds the contents of stdout to it.\n2&gt; path/to/file: redirect stderr to the file at path/to/file.\n|: the uber-useful Unix pipe. (Just as an aside, when R finally got a similar construct—the %&gt;% from the ‘magrittr’ package—it became much easier for Unixy people to enjoy coding in R).\nhead: print the first ten lines of a file to stdout. If multiple file arguments are given, they are separated by little ==&gt; filename &lt;== lines, which is super convenient if you want to look at the top of a lot of files.\n\nhead -n XX: print the first XX lines (instead of 10).\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nDecompress data/samtools_stats/s001_stats.tsv.gz onto stdout using the gzip -cd command and pipe the output into wc to count how many lines words, and characters are in the file.\nDo the same that you did above, but redirect the stdout to a file so.txt and stderr to a file se.txt in the current working directory.\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nThese could be done like this:\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc &gt; so.txt 2&gt;se.txt\n\nAs an interesting side note, this will only redirect stderr for the wc command into se.txt. If the first command fails, its stderr will to to the screen. Try this:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc &gt; so.txt 2&gt;se.txt\n\nAn interesting fact is that you can redirect stderr from the first command before the pipe. So, to redirect stderr for the gzip command into a file called ze.txt, we could do:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz 2&gt;ze.txt | wc &gt; so.txt 2&gt;se.txt\n\nHave a look at the contents of ze.txt.\n\n\n\n\n\n\n\n\n\nPro-tip: Redirect stdout and stderr to the same place\n\n\n\n\n\nThe astute reader might note that if you redirect stdout to a file, and then redirect stderr to the same file, you might end up overwriting the contents of stdout with stderr.\nIf you want to redirect stdout and stderr to the same place then you first redirect stdout to a file, and then after that, you say “redirect stderr to wherever stdout has been redirected to,” by using 2&gt;&1.\nSo it looks like this:\ncommand &gt; file.out 2&gt;&1"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variables",
    "href": "nmfs-bioinf/shell-prog.html#variables",
    "title": "2  Shell Programming",
    "section": "2.1 Variables",
    "text": "2.1 Variables\nThe bash shell can store strings into variables. These variables can be accessed later.\n\n2.1.1 Assigning variable values\n\nbash is remarkably picky about how to assign a value to a variable.\nUse the equals sign with no spaces around it!\n\nHere, we can assign a value to a variable called PHONE:\n\n\nPaste this into your shell\n\nPHONE=974-222-4444\n\nIn this case, the digits and dashes 974-222-4444 are treated as just a string of characters and assigned to the variable PHONE.\n\n\n\n\n\n\nLet’s make some mistakes\n\n\n\nPaste these commands into the shell and see what happens:\n\n\nPaste these mistakes into the shell and think about what is happening.\n\nPHONE = 974-222-4444\nPHONE =974-222-4444\nPHONE= 974-222-4444\n\nWhat does this tell us about how bash interprets commands with an equals sign?\n\n\n\n\n2.1.2 Accessing variable values\n\nThe process of accessing the values stored in the variable is called “variable substitution.\nIt means: “Substitute the value for the variable where it appears on the command line.”\nIn many programming languages, you can just write a variable’s name and know that its value will be accessed, like in R:\n\nVariable &lt;- 16\nsqrt(Variable)\n\nHowever, in bash, variable substitution is achieved by prepending $ to the variable’s name.\n\nWitness:\n\n\nPaste this into your shell\n\necho The value of PHONE is: $PHONE\n\nCool!\n\nRemember: if you make a substitution to the menu at a fancy restaurant, it is going to cost you some dollars. Same way when you make a variable substitution in bash: it costs you a dollar sign and you have to pay up front.\n\n\n\n2.1.3 Valid bash variable names\nThe bash shell demands that the names of variables:\n\nStart with _ (an underscore) or a letter\nThen include only _, letters, or numbers\n\n# good variable names\nMY_JOBS\n_Now\nSTRING\ni\ni_2\ni2\n\n# cannot be variable names\n1_node\n4THIS\nBIG-VAR\nfile-name\nSh**t!\n\n\n\n\n\n\nSelf-study\n\n\n\nChoose one of the good variable names from the list above and assign the value Good to it.\nChoose of the bad variable names from the list above and try to assign the value Bad to it.\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nHere is an example. {.sh filename = \"This works\"} _Now=Good\nHere is an example. {.sh filename = \"This does not work\"} BIG-VAR=Bad"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#strings-with-spaces-etc-quoting.",
    "href": "nmfs-bioinf/shell-prog.html#strings-with-spaces-etc-quoting.",
    "title": "2  Shell Programming",
    "section": "2.2 Strings with spaces, etc: Quoting.",
    "text": "2.2 Strings with spaces, etc: Quoting.\n\nIf you want to assign a string to a variable that has spaces in it you can quote the string, which holds it together as one “unit.”\n\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWhen the bash shell is interpreting a line of input, it breaks it into chunks called tokens which are separated by white space (spaces and TABS). If you wrap a series of words in quotation marks, it turns them all into a single token.\n\n\n\nFor example:\n\n\nPaste this into your shell\n\nMandela_Quote=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\necho $Mandela_Quote"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variable-substitution-and-vs",
    "href": "nmfs-bioinf/shell-prog.html#variable-substitution-and-vs",
    "title": "2  Shell Programming",
    "section": "2.3 Variable substitution and \" vs '",
    "text": "2.3 Variable substitution and \" vs '\nWe have two types of quotes:\n\nsingle quotes, like '\ndouble quotes, like \"\n\nThey both chunk their contents into a single unit, but they behave very differently with respect to variable substition.\n\nSingle Quotes: surly and strict, you can’t substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho 'Dessert tonight is $DESSERT'\n\n\nDouble Quotes: soft and friendly, you CAN substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho \"Dessert tonight is $DESSERT\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nAssign values to three shell variables, NAME, FOOD, and ACTIVITY, so that when you run the following command, it makes sense:\n\n\nAfter assigning values to the three variables, run this command\n\necho \"My name is $NAME. I like to eat $FOOD, and I enjoy $ACTIVITY.\" \n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nFor my case, I could put:\nNAME=Eric\nFOOD=\"steamed broccoli\"\nACTIVITY=\"inline skating long distances\""
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variables-are-not-just-to-be-echoed",
    "href": "nmfs-bioinf/shell-prog.html#variables-are-not-just-to-be-echoed",
    "title": "2  Shell Programming",
    "section": "2.4 Variables are not just to be echoed!",
    "text": "2.4 Variables are not just to be echoed!\nInvariably, when learning how to use shell variables, all the examples have you using echo to print the value of the variable. How boring and misleading.\nIt is important to understand that after a value gets substituted onto the command line, the shell goes right ahead and evaluates the resulting command line.\nSo, you can record shell variables that are command lines that do something, themselves, once they are run as a command line.\nFor example, here we make a variable whose value is the command to decompress a file to stdout:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz\"\n\nAnd now if you just substitute that variable onto the command line\n\n\nType this at the command line\n\n$MyComm\n\nthe uncompressed contents of the file data/samtools_stats/s001_stats.tsv.gz go zooming by on your screen.\n\n2.4.1 Some subtlety about evaluation of substituted values\nIf the value of the variable that is being evaluated includes pipes, redirections, or variable assignment statements, then if you just substitute it into the command line, it won’t properly be evaluated as a command line in full. For example, if MyComm was trying to decompress the file and pipe it to less, it doesn’t work as expected:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\"\n\nAnd now if you just substitute that variable onto the command line the shell gets confused, because it doesn’t recognize the pipe as a pipe!\n\n\nType this at the command line\n\n$MyComm\n\nHowever, you can use the eval keyword before $MyComm to ensure that the shell recognizes that you intend for it to evaluate pipes, redirects, shell variable assignment, etc. in the substituted variable value as it normally would:\n\n\nType this at the command line\n\neval $MyComm\n\nWe will end up using this later.\nRemember, you can hit q to get out of the less page viewer."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#multiple-commands-on-one-line-with",
    "href": "nmfs-bioinf/shell-prog.html#multiple-commands-on-one-line-with",
    "title": "2  Shell Programming",
    "section": "2.5 Multiple commands on one line with ;",
    "text": "2.5 Multiple commands on one line with ;\n\nYou can put a ; after a command, and it will behave like a line ending—the shell will run that command, and then go to the next.\n\nExample:\necho \"Let us do this command and 2 others\"; echo \"here is number 2\"; echo \"and the third\"\nThis comes in handy.\n\n\n\n\n\n\nMore about line endings like ;\n\n\n\nThere are two other things you might find at the end of a line: & and &&\n\n& at the end of the line means “run the command, but don’t wait for it to finish.\n\nThis runs the command “in the background” in some sense.\nThis is not used very often when doing bioinformatics in a SLURM-driving system like that on Sedna\n\n&& at the end of a command means “only run the next command if the previous one did not fail.\n\nThis is very useful for making sure that you don’t keep running later commands if an earlier one failed.\nThere is also a || that is useful in this context, which is all about “exit status” of Unix commands, which is beyond our purview today.\n\n\nExamples:\n\n\nWith just semicolons...Paste it into your shell\n\necho \"Yawp before it fails.\"; ls --not-option data ; echo \"Yawp after it fails.\"\n\n\n\nWith the &&'s after each line...Paste it into your shell\n\necho \"Yawp before it fails.\" && ls --not-option data && echo \"Yawp after it fails.\""
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#repetition",
    "href": "nmfs-bioinf/shell-prog.html#repetition",
    "title": "2  Shell Programming",
    "section": "2.6 Repetition",
    "text": "2.6 Repetition\nLet’s face it, bioinformatics, or any sort of data analysis or processing often involves doing the same thing to a number of different inputs.\nMost unix utilities are designed so that if you give it multiple inputs it will do the same thing to each and report the results in a way that is easy to understand.\nFor example, to see how many lines, words, and characters are in each Quarto (the successor to RMarkdown) document that I used to make this website, we can use wc on all the files with a .qmd extension that are one directory level above where we are currently:\n\n\nPaste this into your shell\n\nwc ../*.qmd\n\nThis is nice."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#for-loops",
    "href": "nmfs-bioinf/shell-prog.html#for-loops",
    "title": "2  Shell Programming",
    "section": "2.7 For loops",
    "text": "2.7 For loops\nSometimes, however, we have more complex operations to do, so we can’t just provide multiple files to a single Unix utility.\nFor example, let’s say we want to know how many lines are in each of the samtools stats files in the data/samtools_stats directory. We can’t use wc directly, because these files are gzipped, and the result we get won’t be equal to the number of lines, words, and characters in each file.\nFor repetition in these cases, bash has a for loop. Its syntax looks like this:\nfor VAR in thing1 thing2 ... thingN; do\n  one or more commands where the value of VAR is set to each of the N things in turn\ndone\nThe important “structural” parts of that are:\n\nthe for\nthe in\nthe semicolon after all the things\nthe do\nthe done\n\nHere is an example:\n\n\nPaste this into your terminal\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do \n  echo \"I like $LIKE.\"\ndone\n\nNote that this is written over multiple lines, but we can substitute ; for the ends of statements and put it all on one line. (Useful if we are just hacking away on the command line…)\n\n\nThis does the same as the above one\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do echo \"I like $LIKE.\"; done\n\nNote! Don’t put a semicolon after do.\n\n\n\n\n\n\nSelf study\n\n\n\nWe want to give the number of text lines, words, and characters from all the samtools_stats files.\nPrep: Here is a command that prints the name of each file.\n\n\nPaste this into the terminal\n\nfor FILE in data/samtools_stats/*.gz; do echo $FILE; done\n\nTask: I have added the -n option to the echo command which makes it not print a line ending. Your task is to replace YOUR_STUFF_HERE with an appropriate shell command to decompress each file and then print the number of lines, words, and characters in it:\n\n\nPaste this, edit YOUR_STUFF_HERE, and run it\n\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; YOUR_STUFF_HERE; done\n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nYour edited command line should look like this:\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; gzip -cd $FILE | wc; done"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#redirect-stdout-from-the-done",
    "href": "nmfs-bioinf/shell-prog.html#redirect-stdout-from-the-done",
    "title": "2  Shell Programming",
    "section": "2.8 Redirect stdout from the done",
    "text": "2.8 Redirect stdout from the done\nHere is something that is not always obvious: you can redirect or pipe the stdout of the whole for loop by using &gt; or | immediately after the done keyword.\nUsing the example from the self study above:\n\n\nPaste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do \n  echo -n $FILE; gzip -cd $FILE | wc; \ndone &gt; word_counts.txt\n\nNow look at what is in word_counts.txt."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#a-few-useful-topics-rapidly",
    "href": "nmfs-bioinf/shell-prog.html#a-few-useful-topics-rapidly",
    "title": "2  Shell Programming",
    "section": "2.9 A few useful topics, rapidly",
    "text": "2.9 A few useful topics, rapidly\n\n2.9.1 basename\n\nIf you have a path name to a file this_dir/that_dir/my_file3.txt, but you want to have the string for just the file name, my_file3.txt, you can use the basename command:\n\n\n\nTry these\n\nbasename this_dir/that_dir/my_file3.txt\nbasename ~/Documents/git-repos/CKMRpop/R/plot_conn_comps.R\n\n\n\n2.9.2 Capture stdout into a token to put on the command line\nThis is a pretty cool one, and is really nice if you want to capture a bit of output for use at a later time.\nBasically, if you run a command inside parentheses that are immediately preceded by a $, like $(command), then the stdout output of command gets put onto the command line as a single token.\nObserve:\n\n\nPaste this into your terminal\n\nSTART_TIME=$(date)\nsleep 3\nSTOP_TIME=$(date)\necho \"We started at $START_TIME, and finished at $STOP_TIME, and it is now $(date)\"\n\nOr even:\n\n\nPaste this into your terminal\n\nWCOUT=$(gzip -cd data/samtools_stats/s016_stats.tsv.gz | wc)\necho $WCOUT\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ndate: prints the current time and date to stdout.\nsleep: causes the shell to pause for however many seconds you tell it to, like sleep 3 for three seconds, sleep 180 for three minutes."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#fancier-variable-substitution",
    "href": "nmfs-bioinf/shell-prog.html#fancier-variable-substitution",
    "title": "2  Shell Programming",
    "section": "2.10 Fancier variable substitution",
    "text": "2.10 Fancier variable substitution\n\n2.10.1 Wrap it in curly braces ${VAR}\nEspecially if you want to substitute a variable into a string adjacent to a letter or number or underscore, you can wrap it in curly braces.\n\n\nTry this\n\nsample=001\n# this works:\necho \"The sequences are in the file called ${sample}_seqs.fq.gz\"\n\n# this does not work the way you want it to\necho \"The sequences are in the file called $sample_seqs.fq.gz\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nWhy do you think the second echo line above produced the output that it did?\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nSince _ is a valid character for a variable name, $sample_seqs.fq.gz gets broken up by the shell as $sample_seqs plus .fq.gz, and there is no variable named sample_seqs, so the shell just substitutes an empty string into $sample_seqs.\n\n\n\n\n\n2.10.2 Variable modification while substituting it\nbash has a whole lot of tricky syntaxes for manipulating variable values when substituting them onto the command line.\nThe one I use more than any other is ${VAR/pattern/replacement}. This looks for a text pattern pattern in the variable VAR and replaces it with the text replacement.\nHere are some examples.\n\n\nFancy substitution fun. Paste into your terminal.\n\nfile=myfile.eps\necho ${file/eps/pdf}\n\n# or maybe you want to get the sample name, s007\n# out of a file path like data/samtools_stats/s007_stats.tsv.gz\npath=data/samtools_stats/s007_stats.tsv.gz\necho $(basename ${path/_stats.tsv.gz/})\n\nWhoa! On that last one we nested a ${//} inside a $()!\n\n\n\n\n\n\nHot tip!\n\n\n\nYou can use * the way that you might when globbing filenames on the command line in the pattern for variable substitution with ${var/pattern/replacement}:\n\n\nIf we want to extract just the s001 part...\n\nSTRING=\"A-whole-lot-of-junk-before-then_s001_and-a-whole-lot_of-other-garbage.63713973\"\n\n# remove all the garbage in the beginning\nN1=${STRING/*then_/}\n\n# see what we have at this point\necho $N1\n\n# remove the remaining junk off the end\nN2=${N1/_and-*}\n\n# see what we ended up with\necho $N2\n\n\n\n\n\n2.10.3 Grouping multiple commands with (...)\n\nSometimes it is convenient lump a number of commands together into a group.\nThe main reason I do this is to capture stdout or stderr from all of them into a single file with one redirect (as opposed to redirecting (&gt;) output from the first command, and then redirect-appending (&gt;&gt;) output from successive commands to the same place).\nWhen you wrap a series of commands in a pair of parentheses, they get executed as a group and you can redirect that from the right side of the last parenthesis:\n\n\n\nPaste this into your terminal\n\n(\n  echo \"This\"\n  echo \"that\"\n  echo \"and the\"\n  echo \"other\"\n) &gt; group_it.txt\n\nCheck out the result with cat group_it.txt.\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWith grouping parentheses, you can also redirect stderr to a single place even if there are pipes involved. Comparing to the Self-Study answer at the end of the Quick Unix Review session:\n\n\nA stderr example\n\n(gzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc) &gt; so.txt 2&gt;se.txt\n\nEven though the error happened with the gzip command, the error message gets relayed through to be redirected into se.txt."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#leaving-here-for-now",
    "href": "nmfs-bioinf/shell-prog.html#leaving-here-for-now",
    "title": "2  Shell Programming",
    "section": "2.11 Leaving here for now…",
    "text": "2.11 Leaving here for now…\nYou might be thinking, “Wow, could I use a for loop or something like it in bash to cycle over the lines of a text file to process each line in turn?”\nThe answer, is, “You can, but bash is not always the best tool for processing text files…especially if they are large.”\nThere is a Unix utility called awk that is much better for that.\nThat is where we are heading next."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#awks-philosophy-and-basic-syntax",
    "href": "nmfs-bioinf/awk-intro.html#awks-philosophy-and-basic-syntax",
    "title": "3  A Brief awk Intro",
    "section": "3.1 awk’s philosophy and basic syntax",
    "text": "3.1 awk’s philosophy and basic syntax\nawk is a utility that:\n\nTakes text input from a file or from stdin\nIt automatically goes line-by-line. Treating each line of text as a separate unit.\nWhen it is focused on a line of text, it breaks it into fields which you can think of as columns.\nBy default, fields, are broken up on whitespace (spaces and TABs), with multiple spaces or TABs being treated as a single delimiter.\nYou can specify the delimiter.\n\nLet’s look through a file together. I will do:\n\n\nYou can do this if you want\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\nand we will discuss how awk sees such a file.\n\n3.1.1 Naming fields\nOnce awk has read a line of text into memory, and split it into fields, you can access the value of those fields with special names:\n\n$1 refers to the first field (column)\n$2 refers to the second column.\n\nIf you are beyond the ninth column, the number has to be wrapped in parentheses:\n\n$(13) refers to the thirteenth column\n\n\n\n\n\n\n\nImportant\n\n\n\nThese variables within an awk script are not related to substituted variables in bash. They just happen to share a preceding $. But they are being interpreted by different programming languages (one by bash the other by awk).\n\n\n\n\n3.1.2 So what?\nSo far, this doesn’t seem very exciting, but now we learn that…\n\nYou can tell awk the sorts of lines you want it to pause at and then do some action upon it.\n\nYou tell awk which lines you want to perform an action on by matching them with a logical statement called a pattern in awk parlance.\n\nThis turns out to be incredibly powerful.\n\n\n3.1.3 awk syntax on the command line\nThe syntax for running awk with a script on the command line is like this:\n\n\nDon't try running this. It is just for explanation.\n\nawk '\n  pattern1  {action1}\n  pattern2  {action2}\n' file\n\nWhere file is the name of the file you want to process.\nOr, if you are piping text into awk from some command named cmd it would look like this:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk '\n  pattern1  {action1}\n  pattern2  {action2}\n'\n\nNote that you can pass options (like -v or -F) to awk. They go right after the awk and before the first single quotation mark.\nAlso, the carriage returns inside the single quotation marks are only there for easy reading. You could also write the last example as:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk 'pattern1  {action1} pattern2  {action2}'\n\n…which is great if you are hacking on the command line, but once you have a lot of pattern-action pairs, it gets harder to read.\n\n\n\n\n\n\nSelf-study\n\n\n\nThinking back to the previous session when we talked about the difference between grouping strings with ' vs with \", why do you think it is important that the awk script is grouped with '?\n\n\n\n\n\n\n\n\nBrief answer\n\n\n\n\n\nAs we saw, we will be referring to different fields like $8 within the awk script. If we used \" to group the script, the shell might try to do variable substitution on any $’s in there."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#enough-talking-lets-start-doing",
    "href": "nmfs-bioinf/awk-intro.html#enough-talking-lets-start-doing",
    "title": "3  A Brief awk Intro",
    "section": "3.2 Enough talking, let’s start doing",
    "text": "3.2 Enough talking, let’s start doing\nAll of this will make more sense with a few examples.\n\n3.2.1 Print all the lines in which the first field is SN\nFor our first foray, let’s just pick out and print a subset of lines from our samtools stats file:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '$1==\"SN\" {print}'\n\nThat is cool.\n\n\n\n\n\n\nSelf-study\n\n\n\nMake sure that you can identify the pattern and the action in the above awk script.\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nThe pattern is $1==\"SN\"\nThe action is print\n\nThis brings up the important point that the “is equals” operator in awk is == (two consecutive equals signs…just like in R)\n\n\n\n\n\n3.2.2 Printing the lines we are interested in\nHow about if we wanted to pick out just a few particular lines from there?\nWell, we can also match lines by regular expression (which you can think of as a very fancy form of Unix word-searching.)\nLet’s say that we want information on the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped using the (cigar) criterion.\nWe can see those lines in there. And we can target them by matching strings associated with them. The awk syntax puts these regular expressions in the pattern between forward slashes.\nSo, we want to match lines that have the first field equal to SN and also match other strings. We do that like this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print}\n  $1==\"SN\" && /reads properly paired:/ {print}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print}\n'\n\n\n\n\n\n\n\nBig Note:\n\n\n\nRegular expressions are lovely and wonderful, but occasionally frustrating. In awk’s case, parentheses have special meanings in the regular expressions, so we have to precede each one in the pattern with a backslash.\nRegular expressions are a bit beyond the scope of what we will be talking about today (entire books are devoted to the topic) but I encourage everyone to learn about them.\nThey are incredibly useful and they are used in multiple programming languages (R, python, perl, etc.)\n\n\n\n\n3.2.3 Printing just the values we are interested in\nThat is nice, but remember, we really just want to put those three values we are interested in into a table of sorts.\nSo, how do we print just the values?\nUse the fields! Count columns for each line and then print just that field:\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print $4}\n  $1==\"SN\" && /reads properly paired:/ {print $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print $5}\n'\n\n\n\n3.2.4 Use variables inside awk\nWe can also assign values to variables inside awk.\nThis lets us store values and then print them all on one line at the end. The special pattern END gives us a block to put actions we want to do at the very end.\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print rm, rpp, bmc}\n'\n\n\n\n\n\n\n\nFor the programmers out there\n\n\n\nVariables in awk are untyped and do not be declared. Basically you can put them anywhere. If code calls for the value from a variable that has not has anything assigned to it yet, the variable returns a 0 in a numerical context, and an empty string in a string context.\n\n\n\n\n3.2.5 That’s great. Can we add the sample name in there?\nYes! We can pass variables from the command line to inside awk with a -v var=value syntax.\nTo do this, we use some shell code that we learned earlier!\n\n\nStudy this, then paste this into your terminal\n\nFILE=data/samtools_stats/s001_stats.tsv.gz\ngzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print samp, rm, rpp, bmc}\n'\n\n\n\n3.2.6 OMG! Do you seen where we are going with this?\nWe can now take that whole thing and imbed it within a bash for loop cycling over values of FILE and get the table talked about wanting in our Motivating Example when we started.\n\n\nStudy this, then paste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nYowzers! That is pretty quick, and it sure beats opening each file, copying the values we want, and then pasting them into a spreadsheet."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "href": "nmfs-bioinf/awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "title": "3  A Brief awk Intro",
    "section": "3.3 Another example: the distribution of mapping qualities",
    "text": "3.3 Another example: the distribution of mapping qualities\nHere is a fun awk example that just came up a couple of days ago for me.\n\nA colleague was telling me that she has started filtering her whole-genome sequencing BAM files so that she does not use any reads that map with a mapping quality less than 30.\nThe hope is that this will lessen batch effects.\nQuestions:\n\nWhat is the distribution of mapping quality scores in my own data\nIf we imposed such a filter, how many reads would we discard?\n\n\nIt turns out that none of the samtools programs stat, idxstats, or flagstats provide that distribution.\nThere are some other more obscure software packages that provide it, but also a lot of convoluted python code on the BioStars website for doing it.\nHa! It’s quick and easy with awk! And a great demonstration of awk’s associative arrays.\n\n3.3.1 Let’s look at an example bam file\nWe have an example bam file in the repository at data/bam/s0001.bam.\nIt only has only about 25,000 read in it so that it isn’t too large.\nLet’s have a look at it with:\n\n\nPaste this into your terminal\n\nmodule load bio/samtools\nsamtools view data/bam/s001.bam | less -S\n\nThe module load bio/samtools line gives us access to the samtools program, which we need for turning BAM files into text-based SAM files that we can use. Once we have given it in our shell, we have that access until we close the shell. Much more on that tomorrow!\n\n\n\n\n\n\nIf that failed you might need to define your MODULEPATH\n\n\n\n\n\nHere we check to see if the bioinformatics paths on in the MODULEPATH:\n\n\nPaste this in your shell\n\necho $MODULEPATH | grep bioinformatics\n\nIf that command did not return anything to stdout, then you need to add a command to your ~/.bashrc that will add /opt/bioinformatics/modulefiles to your MODULEPATH. You can do that (as detailed in the Sedna Google Doc) like this:\n\n\nIf you need to, paste this into the shell\n\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' &gt;&gt; ~/.bashrc\n\nAnd when you are done with that, reload your .bashrc to get the change to take effect:\n\n\nIf you need to, paste this into the shell\n\nsource ~/.bashrc\n\n\n\n\nThe mapping quality is in the 5th field. It is a number that ranges from 0 to 60. We can count up how many times each of those numbers occurs using awk.\n\n\nHere it is all on one line as I wrote it\n\nsamtools view data/bam/s001.bam | awk 'BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} {n[$5]++} END {for(i in n) tot+=n[i];  for(i=60;i&gt;=0;i--) {sum+=n[i]; print i, n[i], sum/tot;}}'\n\nAnd\n\n\nHere it is broken across lines. Paste that in your shell.\n\nsamtools view data/bam/s001.bam | awk '\n  BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} \n  {n[$5]++} \n  END {for(i in n) tot+=n[i];  for(i=60;i&gt;=0;i--) {sum+=n[i]; print i,n[i],sum/tot;}}\n'\n\nIt’s really compact and requires very little memory to do this. (You couldn’t read a whole BAM file into R and hope to deal with it).\n\n\n\n\n\n\nAll arrays in awk are associative arrays\n\n\n\nIf you come from an R programming background, you will typically think of arrays as vectors that are indexed from 1 to n, where n is the length of the vector.\nThis is not how arrays are implemented in awk. Rather all arrays are associative arrays, which are also called hash arrays, or, in Python dictionaries. Or, if you are familiar with R, you can think of an associative array as an array that has elements that can only be accessed via their names attribute, rather than by indexing them with a number.\nSo, in awk, if we write:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[\"this\"] = \"Boing!\"\n\nThis will create an array called var (if one does not already exist) and then it will set the value of element in var that is associated with the string \"this\" to the string \"Boing!\".\nAt the same time, if you do this:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[30] = 67\n\nthen we are not assigning the value of 67 to the 30-th element of var. Rather, we are assigning the value 67 to the element of var that is associated with the string \"30\".\nIt can take a little getting used to, but it is very useful for counting things."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#wrap-up",
    "href": "nmfs-bioinf/awk-intro.html#wrap-up",
    "title": "3  A Brief awk Intro",
    "section": "3.4 Wrap-Up",
    "text": "3.4 Wrap-Up\nThat was just a brief whirlwind tour of how one can use bash and awk together to automate tasks that come up on an everyday basis when doing bioinformatics."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#looking-toward-tomorrow",
    "href": "nmfs-bioinf/awk-intro.html#looking-toward-tomorrow",
    "title": "3  A Brief awk Intro",
    "section": "3.5 Looking toward tomorrow",
    "text": "3.5 Looking toward tomorrow\nThe bulk of day #2 is going to be focused on working within a cluster environment, and specifically on using SLURM for launching jobs on the Sedna cluster.\nTo prepare for tomorrow, please be sure to read Chapter 8 from beginning and up to and including section 8.2. This is just a small bit to read, but it should set you up for an understanding of why and how computing clusters work differently than your desktop machine when it comes to allocating resources for computation."
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#prepare-for-this",
    "href": "nmfs-bioinf/scripts-and-functions.html#prepare-for-this",
    "title": "4  Bash scripts and functions",
    "section": "4.1 Prepare for this",
    "text": "4.1 Prepare for this\nSync your fork of the repository. Then, make sure that you have all the latest updates from the repository by pulling them down with git. use git to pull down any new changes.\n\n\nUse something like this in to be sure you have the most up-to-date resources\n\ncd YOUR-CLONE-OF-YOUR-FORK-OF-THE-REPO\ngit pull origin main"
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#bash-shell-scripts",
    "href": "nmfs-bioinf/scripts-and-functions.html#bash-shell-scripts",
    "title": "4  Bash scripts and functions",
    "section": "4.2 Bash shell scripts",
    "text": "4.2 Bash shell scripts\nWe have been doing all of our bash scripting by writing commands on the command line.\nUseful bash code can be stored in a text file called a script, and then run like a normal Unix utility.\nTo illustrate this, we will copy our samtools-stats-processing commands from before into a file using the nano text editor.\nAt your command line, type this:\n\n\nType this at the command line\n\nnano sam-stats.sh\n\nThis opens a file called sam-stats.sh with a text editor called nano.\nThe convention with bash shell scripts is to give them a .sh extension, but this is not required.\nNow we copy our commands into nano.\n\n\nCopy this onto your clipboard and paste it into nano in your terminal\n\n#!/bin/bash\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nThen:\n\ndo cntrl-X\nAnswer Y when asked if you want to save the file\nHit return to save the file as sam-stats.sh\n\nVoila! That should exit the nano editor, and now you have a script containing that bash code.\nCheck out its contents with:\n\n\nType this at the command line\n\ncat sam-stats.sh\n\n\n\n\n\n\n\nWhat’s this #! at the top of the file?\n\n\n\nThat is colloquially referred to as the shebang line.\nA # usually tells the shell to “ignore everything to the right of the # on this line”\n# is used to precede comments in your code.\nHowever, in this case, at the top of the file and followed by a ! it tells the computer what language to use to interpret this file.\nIn our case /bin/bash is where the bash shell interpreter typically is found on most Unix or Linux system.\n(If bash is the default shell on your system, you may not always need to have the shebang line, but it is good practice to do so.)\n\n\n\n4.2.1 Script files must be executable\nThe Unix operating system distinguishes between files that just hold data, and files that can the run or be “executed” by the computer.\nFor your bash commands in a script to run on the computer, it must be of an executable type.\nWe can make the file executable using the chmod command, like this:\n\n\nPaste this into your shell\n\nchmod u+x sam-stats.sh\n\nIf we then use ls -l to list the file in long format like this:\n\n\nType this in\n\nls -l sam-stats.sh\n\nwe see:\n-rwxrw-r-- 1 eanderson eanderson 317 Oct 14 13:40 sam-stats.sh\nThe x in the first field of that line indicates that the file is executable by the user.\n\n\n4.2.2 Running a script\nTo run a script that is executable, you type the path to it.\nOn some clusters, by default, the current working directory is not a place the computer looks for executable scripts, so we have to prepend ./ to its path:\n\n\nType this on the command line and hit RETURN.\n\n./sam-stats.sh\n\nThat runs our script.\n\n\n4.2.3 Scripts are more useful if you can specify the inputs\nSo, that runs our script and produces results, but that is not so useful. We already had those results, in a sense.\nShell scripts become much more useful when you can change the inputs that go to them.\nOne way to do so involves using positional parameters\n\n\n4.2.4 Arguments following a script can be accessed within the script\nIf you put arguments after a script on the command line, for example like:\n\n\nDon't paste this in anywhere\n\nscript.sh arg1 some_other_arg  And_another_arg\n\nthen in the script itself:\n\nthe value of the first argument (arg1) is accessible as $1\nthe value of the second argument (some_other_arg1) is accessible as $2\nthe value of the third argument (And_another_arg) is accessible as $3\n\n…and so forth for as many arguments as you want.\nSo, we can rewrite our script as:\n\n\nJust give this a read\n\n\n#!/bin/bash\n\nfor FILE in $1; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nSee that we have replaced data/samtools_stats/*.gz with $1.\nEdit sam-stats.sh to look like the above. Now we can use it like this:\n\n\nPaste this into the shell\n\n./sam-stats.sh \"data/samtools_stats/s0*.gz\"\n\nHere, we have passed in \"data/samtools_stats/s0*.gz\" as the first argument to the script, and, since we have a $1 in the script where that goes, it does what it did before.\n\n\n4.2.5 Now we can use it on other samtools stats files\nIt is not very exciting to see it just run again on the same set of files.\nBut, now we could direct the script to operate on a different set of files, just by changing the argument that we pass to the script.\nI have put a much larger set of samtools stats files within subdirectory of the /share directory on Sedna. You should be able to list them all with:\n\n\nPaste this into your shell. This should work...\n\n ls /share/all/eriq/big_sam_stats/s*.gz\n\nWhoa! 275 files. (They are here on Sedna, in the shared folder, because I didn’t want to put them all on GitHub.)\nBut, now, we can summarize them all just by pointing our script to them:\n\n\nThis should work on Sedna, Paste it into your shell\n\n./scripts/sam-stat-pp.sh \"/share/all/eriq/big_sam_stats/s*.gz\"\n\nThat is fast.\nIf we wanted to redirect that into a file we could do so\n\n\n\n\n\n\nFun Tip! – Record the positional parameters passed to a script.\n\n\n\n\n\nSometimes it is nice to know and record the values of all the arguments passed to a script that you have written. This can be done by adding something like this to your script:\necho \"Script $0 started at $(date) with arguments $* \" &gt; /dev/stderr\nIn that:\n\n$0 is the path to the script\n$(date) puts the current date and time in the line\n$* expands to a single string with all the arguments passed to the script\n&gt; /dev/stderr redirects stdout from echo to stderr\n\nSo, for the last script, that might look like:\n#!/bin/bash\n\necho \"Script $0 started at $(date) with arguments $* \" &gt; /dev/stderr\n\nfor FILE in $1; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone"
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#bash-functions",
    "href": "nmfs-bioinf/scripts-and-functions.html#bash-functions",
    "title": "4  Bash scripts and functions",
    "section": "4.3 Bash functions",
    "text": "4.3 Bash functions\nLike most programming languages, you can define functions in bash.\nThe syntax for defining a function named MyFunc is:\n\n\nJust read this\n\nfunction MyFunc {\n  ...code that MyFunc executes...\n}\n\nIn this regard, it has a similar syntax to R.\nOnce defined, you can use the name of the function (in the above case, MyFunc) as if it were just another Unix command.\n\n4.3.1 Example: congratulate yourself on making it through your day\nHere is a gratuitous example: we make a bash function called congrats that tells us what time it is and encourages us to keep getting through our day:\n\n\nPaste this into your terminal\n\nfunction congrats { echo \"It is now $(date).  Congrats on making it this far in your day.\"; }\n\n\n\n\n\n\n\nWarning\n\n\n\nCurly braces in bash are extremely finicky. They don’t like to be near other characters. In the above, the space after the { is critical, as is the ; before the }. (The last } needs to have a ; before it if it does not have a line ending before it).\n\n\nNow, you can just type congrats at the command line:\n\n\nType this on the command line\n\ncongrats\n\n\n\n4.3.2 Bash functions take positional parameters too\nWe can rewrite our function as congrats2 so that it can use two arguments, the first a name, and the second an adjective:\n\n\nPaste this into your terminal\n\nfunction congrats2 { \n  echo \"It is now $(date).  Congrats, $1, you are $2.\"\n}\n\nNow, you can use that function and supply it with whatever names and adjectives you would like:\n\n\nPaste these in and see what it does.\n\ncongrats2 Fred splendid\ncongrats2 Eric tired\ncongrats2 Amy amazing\n\nWe will write a few functions, later, to simplify our life in SLURM."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#why-do-we-need-slurm",
    "href": "nmfs-bioinf/slurm.html#why-do-we-need-slurm",
    "title": "5  Sedna and SLURM intro",
    "section": "5.1 Why do we need SLURM?",
    "text": "5.1 Why do we need SLURM?\n\nThe fundamental problem of cluster computing.\nA cluster does not operate like your laptop.\nMost compute-intensive jobs run most efficiently on a dedicated processor or processors."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#hpcc-architecture-in-a-nutshell",
    "href": "nmfs-bioinf/slurm.html#hpcc-architecture-in-a-nutshell",
    "title": "5  Sedna and SLURM intro",
    "section": "5.2 HPCC Architecture in a Nutshell",
    "text": "5.2 HPCC Architecture in a Nutshell\n\n\nNodes: the closest thing to what you think of as a computer. (“Pizza-boxes” with no displays).\n\nEach node is attached via a fast connection to centralized attached storage (A big set of hard drives attached via “Infiniband.”)\nWithin each node are some numbers of Cores or CPUs\n\nCores/CPUs are the actual processing units within a node. (Usually 20 to 24)\n\n\nSedna, like almost all other HPCCs has a login node\n\nThe login node is dedicated to allowing people to communicate with the HPCC.\nDO NOT do computationally intensive, or input/output-intensive jobs on the login node\nNot surprisingly, when you login to Sedna you are on the login node.\n\n\n\n\n\n\n\n\nHot tip!\n\n\n\nIn the default configuration on Sedna, your command prompt at the shell tells you which node you are logged into. The default command prompt looks like:\n[username@computer directory]\nSo, for example, mine at the moment is:\n[eanderson@sedna playground]$\nWhich tells me that I am user eanderson and I am logged in to sedna in the directory whose basename is playground.\nThe login node for Sedna is named sedna. So, this is telling me that I am logged into the login node of Sedna.\nIf you don’t have such an informative prompt…\nOn Alpine, I am not sure what the default command prompt is, but you can always change yours by setting the PS1 variable. Try doing:\nexport PS1='[\\h: \\W]--% '\nThat will give you a command prompt with the hostname (\\h) and the working directory basename (\\W), which is helpful. Mine, right now, looks like:\n[login11: con-gen-csu]--%\nOf course, you can put the export PS1='[\\h: \\W]--% ' into your ~/.bashrc file and then always have an informative prompt.\n\n\n\n\n\n\n\n\nSelf study\n\n\n\nInvestigate your own command prompt and make sure you understand what the different parts in it mean."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#say-it-again-dont-do-heavy-computing-on-the-login-nodes",
    "href": "nmfs-bioinf/slurm.html#say-it-again-dont-do-heavy-computing-on-the-login-nodes",
    "title": "5  Sedna and SLURM intro",
    "section": "5.3 Say it again: Don’t do heavy computing on the login nodes!",
    "text": "5.3 Say it again: Don’t do heavy computing on the login nodes!\nIf you run a big job that is computationally intensive on the login node, it can interfere with other people being able to access the cluster.\nDon’t do it!!\nThat is all fine and well, but how do we avoid computing on the login nodes?"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#do-your-computation-on-the-compute-nodes",
    "href": "nmfs-bioinf/slurm.html#do-your-computation-on-the-compute-nodes",
    "title": "5  Sedna and SLURM intro",
    "section": "5.4 Do your computation on the compute nodes",
    "text": "5.4 Do your computation on the compute nodes\n\nTo run jobs on the compute node you need to ask SLURM to give you “compute resources” to run the job.\nThe basic axes of “compute resources” are:\n\nThe number of cores to use.\nThe maximum amount of RAM memory you might need.\nThe amount of time that you will need those resources for.\n\nTo prepare for the next part of the course on SEDNA, each one of us is going to “check out” 2 cores for 3 hours for interactive work.\n\nInteractive here means that we will have a Unix shell that has access to compute power on the cores that we checked out.\nHere is the command to checkout 2 cores for 3 hours for interactive use:\n\n\n\nPaste this into your shell on SEDNA\n\nsrun -c 2 -t 03:00:00 --pty /bin/bash\n\nIf you are working on ALPINE then you can get on a compute node differently:\n\nLoad the slurm/alpine module\nRequest a shell on the oversubscribed atesting partition.\nThe commands for that look like:\n\n\n\nPaste this into your shell on ALPINE\n\nmodule load slurm/alpine\nsrun --partition atesting --pty /bin/bash\n\n\n\n\n\n\n\n\nSelf study:\n\n\n\nHave a look at your command prompt now. If you are on SEDNA, it should show that you are logged in to nodeXX where XX is a number like 04 or 33. If you are on Alpine, then the node you are on will be named something more cryptic, like c3cpu-a5-u34-3."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#what-does-sedna-have-under-the-hood",
    "href": "nmfs-bioinf/slurm.html#what-does-sedna-have-under-the-hood",
    "title": "5  Sedna and SLURM intro",
    "section": "5.5 What does Sedna have under the hood?",
    "text": "5.5 What does Sedna have under the hood?\nSedna is not an outrageously large cluster, but it is well-maintained and provides a lot of computing power.\nThe nodes in Sedna are broken into three different partitions.\nA partition is a collection of nodes that tend to be similar.\n\nnode partition: This is a collection of 36 nodes:\n\nnode01 – node28: “standard compute nodes”\nnode29 – node36: “standard compute nodes with twice as much memory”\n\nhimem partition: Four nodes with a boatload of memory for high-memory jobs\n\nhimem01 – himem04\n\nbionode partition: legacy machines from an older NWFSC cluster\n\nI’m not sure if all of us have access to this.\nWhen I check, it seems like a lot of the nodes are down, occasionally.\nBut I have been able to checkout resources on it.\nMight be an option, but it doesn’t perform at the same level as the node partition"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#what-does-alpine-have-under-the-hood",
    "href": "nmfs-bioinf/slurm.html#what-does-alpine-have-under-the-hood",
    "title": "5  Sedna and SLURM intro",
    "section": "5.6 What does ALPINE have under the hood?",
    "text": "5.6 What does ALPINE have under the hood?\nAlpine is a pretty darn large cluster, with a boatload of different machines, many of them with 64 cores. It also has some GPU machines, etc.\nWe will have a look at it with the commands, like sinfo described next."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#ask-slurm-about-what-nodes-are-available-and-how-busy-they-are",
    "href": "nmfs-bioinf/slurm.html#ask-slurm-about-what-nodes-are-available-and-how-busy-they-are",
    "title": "5  Sedna and SLURM intro",
    "section": "5.7 Ask SLURM about what nodes are available, and how busy they are",
    "text": "5.7 Ask SLURM about what nodes are available, and how busy they are\nWhen I login to Sedna, before launching and jobs or working on anything, I always like to get a summary of how hard Sedna is currently working.\nFor this we have the sinfo command:\n\n\nType this at your command prompt\n\nsinfo\n\nThe answer will look somethign like this:\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnode*        up   infinite      5    mix node[01,29-31,33]\nnode*        up   infinite      1  alloc node32\nnode*        up   infinite     30   idle node[02-28,34-36]\nhimem        up   infinite      1    mix himem01\nhimem        up   infinite      3   idle himem[02-04]\nbionode      up   infinite      8  down* bionode[10-11,13-18]\nbionode      up   infinite      1  drain bionode12\nbionode      up   infinite     10   idle bionode[01-09,19]\nThis is a hugely informative summary for being so compact.\n\nThe output is sorted by partition.\nWithin each partition it tells us how many nodes are in different states:\nThe main states:\n\nidle: just sitting there with all cores available for checkout,\nmix: some, but not all, cores are checkout out and in use,\nalloc: all the cores on the node are checkout out and in use.\n\nOther states you might see:\n\ndown: node is unavailable because it is not working properly,\ndrain: node is not available because the sys-admins are not letting any new jobs to start because they are going to be working on the system soon, etc.\n\nThe node partition is starred node* because it is the default partition.\nNotation like node[01,29-31,33] gives the specific node numbers, compactly.\n\nIf you type sinfo on ALPINE you get a similar output, but it is much larger and more complex because there are more partitions and more, different nodes in each partition.\nThe CURC lists the partitions on a web page here.\nIf you are going to be working on Alpine, it is worth reading over the entire CURC documentation for it, starting from here.\n\n\n\n\n\n\nSelf-study\n\n\n\nReview how many nodes are currently in use."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#more-detailed-sinfo-output",
    "href": "nmfs-bioinf/slurm.html#more-detailed-sinfo-output",
    "title": "5  Sedna and SLURM intro",
    "section": "5.8 More detailed sinfo output",
    "text": "5.8 More detailed sinfo output\nSometimes you want more information about what is going on with the cluster. sinfo can do that, too, but the syntax is hard to remember and even harder to type.\nHere is a bash function we will define called slinfo for (s-long-info) that uses sinfo to provide more detailed output.\n\n\nPaste this into your shell\n\nfunction slinfo {\n  sinfo -N -O nodelist,cpusstate,memory,allocmem\n}\n\nOnce you have done that:\n\n\nType this at your command prompt\n\nslinfo\n\nThis gives information about each node.\n\nIn CPUS(A/I/O/T):\n\nA = allocated\nI = idle\nO = offline\nT = total\n\nMEMORY is total RAM available on the node (in Megabytes)\nALLOCMEM is the total RAM allocated to jobs.\n\nThis view shows each node just once. But it might be useful to also see what partition(s) each of those nodes are in (especially on ALPINE). So, here is a quick function to do that:\n\n\nPaste this into your shell\n\nfunction slinfop {\n  sinfo -N -O nodelist,partition,cpusstate,memory,allocmem\n}\n\nOnce you have done that:\n\n\nType this at your command prompt\n\nslinfop\n\nNote that many nodes are included in multiple partitions.\nSince the main partition on Alpine for general computing is amilan you can look specifically at nodes in that partition with:\n\n\nCopy this into your command prompt\n\nslinfop | awk '$2~/amilan/'\n\n\n\n\n\n\n\nALPINE Self-study\n\n\n\nQuickly scan the output of the above command and estimate the number of idle (i.e. available) cores in the amilan partition.\nThen, read about the amilan partition (here)[https://curc.readthedocs.io/en/stable/clusters/alpine/alpine-hardware.html#partitions]. In particular, note how much memory is available per core, and the default and max wall times.\nWe will discuss this."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#summarize-what-the-cluster-is-doing-by-job",
    "href": "nmfs-bioinf/slurm.html#summarize-what-the-cluster-is-doing-by-job",
    "title": "5  Sedna and SLURM intro",
    "section": "5.9 Summarize what the cluster is doing by job",
    "text": "5.9 Summarize what the cluster is doing by job\nSLURM keeps track of individual jobs that get submitted.\nFor SLURM, each job is basically a request for resources.\nIf SLURM finds the requested resources are available, it provides the resources and starts the job.\nIf resources are not available because other jobs are running, the reqested job enters the “queue” in a WAITING state.\nWe can see how many jobs are running and how many are waiting, by using the SLURM squeue command:\n(By the way, notice the pattern? All SLURM command start with an s).\n\n\nType this at your command prompt\n\nsqueue\n\nThis tells us a little about all the jobs that are currently allocated resources or are waiting for resources.\nOne thing to note: - Every job is assigned a SLURM_JOB_ID. Which is a unique integer (that gets assigned successively). - And jobs might have a job NAME, that is assigned by the user, which is the name of the script that the job runs, or is defined in the SBATCH directives (we will talk about that later!)"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#i-find-the-standard-default-squeue-output-sorely-lacking",
    "href": "nmfs-bioinf/slurm.html#i-find-the-standard-default-squeue-output-sorely-lacking",
    "title": "5  Sedna and SLURM intro",
    "section": "5.10 I find the standard, default, squeue output sorely lacking",
    "text": "5.10 I find the standard, default, squeue output sorely lacking\nThe standard way that squeue presents information is not super informative, especially if the job names are not very short, or if you are interested in how many cores (not just nodes) each job involves.\nOnce again, we make a bash function, alljobs, that runs squeue but provides more information.\n\n\nPaste this at your command prompt\n\nfunction alljobs {\n  if [ $# -ne 1 ]; then\n    JL=10;\n  else\n    JL=$1;\n  fi;\n  squeue -o  \"%.12i %.9P %.${JL}j %.15u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L\";\n}\n\nAlso, if you follow the command with an integer like 20, that is how many spaces the output will allow for the job NAME.\nFor example, try:\n\n\nType this at the command prompt\n\nalljobs 30\n\nThis is kind of fun to see the full names of the jobs.\n\n\n\n\n\n\nHot Tip!\n\n\n\nIt is worth taking special note of the columns, NODELIST(REASON) and CPUS:\n\nNODELIST(REASON): If the job is running, then this gives the name(s) of the node(s) upon which it is running. If it is not running, it says why it is not yet running. The most typical reason it:\n\nPriority: Too many other users are already using the cluster. Your job is waiting for resources to become available.\n\nCPUS: the number of CPUs the job is using\n\nIt is worth noting that, when I look at alljobs on Alpine, most of the jobs that are waiting due to Priority are jobs that are requesting all or most of the CPUs (like 64 or 32) on a single node. Many of the jobs that are not waiting are those that are requesting only one or a few cores.\nIn general, if you can break your workflows down into a small chunks that run on just one or a few cores, you have a better chance of getting compute time, than if you request all the cores on a node."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#just-tell-me-about-my-jobs",
    "href": "nmfs-bioinf/slurm.html#just-tell-me-about-my-jobs",
    "title": "5  Sedna and SLURM intro",
    "section": "5.11 Just tell me about my jobs!",
    "text": "5.11 Just tell me about my jobs!\nWe can make a similar function that only tells us about our own jobs that are running or waiting in the queue:\n\n\nPaste this into your terminal\n\nfunction myjobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -u $(whoami) -o  \"%.12i %.9P %.${JL}j %.10u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L %p\";\n}\n\nYou can run that the same way as alljobs, and you should see the 2-core job you started with srun:\n\n\nType this into your shell\n\nmyjobs"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#finding-software-on-a-cluster",
    "href": "nmfs-bioinf/slurm.html#finding-software-on-a-cluster",
    "title": "5  Sedna and SLURM intro",
    "section": "5.12 Finding Software on a Cluster",
    "text": "5.12 Finding Software on a Cluster\n\nAnalysis software is not automatically available on a cluster\nOn many clusters you have to install it yourself\nOn Sedna, we are very lucky to have a great support crew that will install software for us, and also make it available for everyone else to use.\n\nHowever, software can be complicated:\n\nSome software might conflict with other software.\nSome users might want different software versions\n\nThe solution: maintain software (and its dependencies) in (somewhat) isolated modules.\n\n\n\n\n\n\nBioinformatic modules on Sedna are only available if the MODULEPATH is specified\n\n\n\nHere we check to see if the bioinformatics paths are in the MODULEPATH:\n\n\nPaste this in your shell\n\necho $MODULEPATH | grep bioinformatics\n\nIf that command did not return anything to stdout, then you need to add a command to your ~/.bashrc that will add /opt/bioinformatics/modulefiles to your MODULEPATH. You can do that (as detailed in the Sedna Google Doc) like this:\n\n\nIf you need to, paste this into the shell\n\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' &gt;&gt; ~/.bashrc\n\nAnd when you are done with that, reload your .bashrc to get the change to take effect:\n\n\nIf you need to, paste this into the shell\n\nsource ~/.bashrc\n\n\n\nMost modules on Alpine are only available if you are on a compute node."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#example-lets-try-running-samtools",
    "href": "nmfs-bioinf/slurm.html#example-lets-try-running-samtools",
    "title": "5  Sedna and SLURM intro",
    "section": "5.13 Example: Let’s try running samtools",
    "text": "5.13 Example: Let’s try running samtools\nsamtools is a common bioinformatic utility.\nTypically, if you run it with no arguments, it gives you some usage information.\nHowever, on Sedna, unless you have installed samtools yourself, or have set your environment up to always have it available, when you type samtools at the shell, you will get a response saying that it is not available:\n\n\nType this at the command prompt\n\nsamtools\n\nThe response I get from the computer is:\nbash: samtools: command not found...\nYou will likely get that response, too.\nOn Alpine, you could install samtools via mamba or you could see if it is available in the system modules."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#check-which-software-programs-are-available-in-the-modules",
    "href": "nmfs-bioinf/slurm.html#check-which-software-programs-are-available-in-the-modules",
    "title": "5  Sedna and SLURM intro",
    "section": "5.14 Check which software programs are available in the modules",
    "text": "5.14 Check which software programs are available in the modules\nThe command module and its subcommands let you interact with the software modules.\nmodule avail tells you what software is available in the modules.\n\n\nType this into your shell\n\nmodule avail\n\nOn SEDNA, there is a lot of tasty bioinformatics software there. On Alpine, there is now a dedicated “Bioinformatics” section in the output of module avail, which is a nice change from Alpine’s predecessor, SUMMIT, which was mostly loaded up with software for geoscientists.\nThe numbers are the version numbers. The (D) after some of them tells us that version is the default."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#load-the-samtools-module",
    "href": "nmfs-bioinf/slurm.html#load-the-samtools-module",
    "title": "5  Sedna and SLURM intro",
    "section": "5.15 Load the samtools module",
    "text": "5.15 Load the samtools module\nWe will load the module for version 1.15.1 of samtools. That can be done like this:\nSEDNA\n\n\nOn SEDNA, Paste this into your shell\n\nmodule load bio/samtools/1.15.1\n\nAlpine\n\n\nOn Alpine, Paste this into your shell\n\nmodule load samtools\n\n(using the load subcommand of module)\n\n\n\n\n\n\nAbout the default versions\n\n\n\nThe same would have been achieved with\nmodule load bio/samtools\nbecause 1.15.1 is the default samtools version.\nNonetheless, it is typically best from a reproducibility standpoint to be explicit about version numbers of software you are using.\n\n\nNow, that it is loaded, you can run the samtools command and get the usage message.\n\n\nType this at your shell\n\nsamtools\n\n\n\n\n\n\n\nInventory and Unload modules\n\n\n\nTo know which modules you have loaded:\nmodule list\nTo unload all the modules you have loaded\nmodule purge\nThere are many more module commands, but those are the only ones that you really need to know."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#two-notes-about-modules",
    "href": "nmfs-bioinf/slurm.html#two-notes-about-modules",
    "title": "5  Sedna and SLURM intro",
    "section": "5.16 Two notes about modules",
    "text": "5.16 Two notes about modules\n\n\n\n\n\n\nLoaded modules are only active in the current session\n\n\n\nIf you open a new shell, or allocate an interactive session on a new core, or logout and log back in again…\n…You will have to reload the modules that you need.\nSo: Any time you write a script that needs some software, you should load the module for that software in the initial lines of the script.\n\n\n\n\n\n\n\n\nModules are not always portable\n\n\n\nNot every cluster that you use will have the same modules set up in the same way. (Though it does seem that Giles has set the modules up on Sedna according to the accepted best practices!).\nSo, if you use code you developed for Sedna (or Alpine) on another cluster, you might have to modify things to ensure that software is available.\nThe conda/mamba package manager is an option if you are going to be running your scripts on different clusters."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-sedna",
    "href": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-sedna",
    "title": "5  Sedna and SLURM intro",
    "section": "5.17 If you need more software or newer versions on Sedna",
    "text": "5.17 If you need more software or newer versions on Sedna\nYou can request software installations by using the Sedna work request form"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-alpine",
    "href": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-alpine",
    "title": "5  Sedna and SLURM intro",
    "section": "5.18 If you need more software or newer versions on Alpine",
    "text": "5.18 If you need more software or newer versions on Alpine\nYou probably can request it from the sys admins, but you will almost indubitably have your required software up and running faster if you install it with mamba."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#before-we-go-make-our-convenience-functions-available-whenever-we-are-on-our-cluster",
    "href": "nmfs-bioinf/slurm.html#before-we-go-make-our-convenience-functions-available-whenever-we-are-on-our-cluster",
    "title": "5  Sedna and SLURM intro",
    "section": "5.19 Before we go, make our convenience functions available whenever we are on our cluster",
    "text": "5.19 Before we go, make our convenience functions available whenever we are on our cluster\nWe defined four convenience functions in bash: slinfo, slinfop, alljobs, and myjobs.\nOnce we log out of the current session, or login to a different session, those convenience functions will be lost.\nTo make sure that they are accessible the next time we login, we will put them into our own .bashrc files.\nThe .bashrc file is a file in your home directory in which you can put function definitions and other configurations that will be applied whenever you open a new bash shell.\nTo add our four convenience functions to your .bashrc file, first start editing your ~/.bashrc file with nano.\n\n\nPaste this into your shell\n\nnano ~/.bashrc\n\nThen come back to this web page and copy the following text:\n\n\nCopy this onto your clipboard\n\nfunction slinfo {\n  sinfo -N -O nodelist,cpusstate,memory,allocmem\n}\n\nfunction alljobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -o  \"%.12i %.9P %.${JL}j %.15u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L\";\n}\n\nfunction myjobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -u $(whoami) -o  \"%.12i %.9P %.${JL}j %.10u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L %p\";\n}\n\nThen paste those function definitions into the editor in your .bashrc file (before your conda/mamba block if you have one), and then do cntrl-X, Y, RETURN to save the file and get out of nano.\nNow, the next time you login you will have all those functions."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#bonus-thoughts-what-are-other-people-doing",
    "href": "nmfs-bioinf/slurm.html#bonus-thoughts-what-are-other-people-doing",
    "title": "5  Sedna and SLURM intro",
    "section": "5.20 Bonus Thoughts — What are other people doing?",
    "text": "5.20 Bonus Thoughts — What are other people doing?\nEspecially when you are working on a smaller workgroup cluster and you are wondering if other users are using it efficiently, there are a few ways you can check up on what they are doing.\nUsing alljobs you can see what other people are doing. Then you can ssh to the node that they are running jobs on, and then do ps -auxww | grep user (where user is their username) and that will show the currently running command lines. This can be useful, for example, to see if their jobs are multithreaded while they are using many cores or not.\nYou can also get information about failures on different nodes with dmesg -T."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#prepare-for-this-session",
    "href": "nmfs-bioinf/sbatch.html#prepare-for-this-session",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.1 Prepare for this session",
    "text": "6.1 Prepare for this session\nGet on your cluster and navigate to the top level (con-gen-csu) of your fork of the class repository. Then, to make sure that you have all the latest updates from the repository, sync the main branch of your fork with the main branch of eriqande/con-gen-csu on GitHub, then in your shell, use git switch main to make sure you are on the main branch of your fork, and then use git pull origin main to pull down those changes."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#the-sbatch-command-and-its-options",
    "href": "nmfs-bioinf/sbatch.html#the-sbatch-command-and-its-options",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.2 The sbatch command and its options",
    "text": "6.2 The sbatch command and its options\nIf you do man sbatch you will see an insanely large number of options and all sorts of complicated stuff.\nYou can get by with a fairly minimal set of options for almost everything you need to do. They are:\n\n--cpus-per-task=&lt;n&gt;: the number of cores to use for the job. The syntax has you using it like this --cpus-per-task=2\n\nOn Sedna, the default is 1.\n\n--mem=&lt;size[units]&gt;: How much total memory for the job. Example: --mem=4G\n\nOn Sedna, the default is about 4.7 Gb for each requested core.\nOn Alpine’s amilan partition, the machines have 3.74 Gb per core.\n\n--time=[D-]HH:MM:SS: How much time are you requesting? You don’t have to specify days, so you could say, --time=1-12:00:00 for one day and twelve hours, or you could say --time=36:00:00 for 36 hours.\n\nOn Sedna, the default is 8 hours.\n\n--output=&lt;filename pattern&gt;: Where should anything on stdout that is not otherwise redirected be written to?\n--error=&lt;filename pattern&gt;: Where should anything on stderr that is not otherwise redirected be written to?\n\nOn top of the options, sbatch takes a single required argument, which must be the path to a shell script (we know about those!) that the job will run.\n\n\n\n\n\n\nFun fact:\n\n\n\nIf you pass any arguments after the name of the shell script that you want sbatch to execute, those are interpreted as arguments to the shell script itself."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#what-an-invocation-of-sbatch-could-look-like",
    "href": "nmfs-bioinf/sbatch.html#what-an-invocation-of-sbatch-could-look-like",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.3 What an invocation of sbatch could look like",
    "text": "6.3 What an invocation of sbatch could look like\nSo, if we wanted to schedule a script called MyScript.sh to run with 4 cores and memory of 80Gb, with an allowance of 16 hours, and we wanted to tell SLURM where to capture any otherwise un-redirected stdout and stderr, we would type something like this:\n\n\nDon't bother copying or pasting this.\n\nsbatch --cpus-per-task=4 --mem=80G --time=16:00:00 --output=myscript_stdout --error=myscript_error MyScript.sh\n\nSome points about that:\n\nTyping all of that is a huge hassle.\nMost of the options will be specific to the actual job in MyScript.sh\n\nSo…sbatch allows you to store the options in your shell script on lines after the shebang line that are preceded by #SBATCH."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#storing-slurm-options-in-your-shell-script",
    "href": "nmfs-bioinf/sbatch.html#storing-slurm-options-in-your-shell-script",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.4 Storing SLURM options in your shell script",
    "text": "6.4 Storing SLURM options in your shell script\nLet’s look at an example like one might do in an sbatch script:\n\n\nContents of scripts/bwa_index.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=2:00:00\n#SBATCH --mem=3G\n#SBATCH --output=bwa_index-%j.out\n#SBATCH --error=bwa_index-%j.err\n\n# this is a little script to index the genome given\n# by the first positional parameter ($1)\n\n# load the module that gives us the bwa software\nmodule load bwa\n\n# make a directory for log output if it does not already exist\nDIR=results/log/bwa_index\nmkdir -p $DIR\n\n# run bwa index on the input\nbwa index $1 &gt; $DIR/log.txt 2&gt;&1\n\n\n\n\n\n\n\nWhat’s that %j in the output and error options?\n\n\n\nIn the above script, you will see\n#SBATCH --output=bwa_index-%j.out\n#SBATCH --error=bwa_index-%j.err\nIn this context, the %j gets replaced by sbatch with the SLURM_JOB_ID.\n\n\n\n\n\n\n\n\nGet email from SLURM.\n\n\n\n\n\nOne useful feature of SLURM—especially if you are running just a few long jobs—is that you can tell it to send you email whenever some event occurs related to a job (like it Starts, or Finishes, or Fails).\nThe SBATCH directives for that look like:\n#SBATCH --mail-user=myemail@emailserver.com\n#SBATCH --mail-type=ALL\nwhere you would replace myemail@emailserver.com with your own email address.\nNote that option --mail-type can be tailored to modulate how much email you will get from SLURM.\nTBH, though, I don’t have SLURM email any news about my jobs anymore. Workflows that are optimized into a lot of smaller jobs would fill your inbox pretty quickly!"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#let-us-submit-the-bwa_index.sh-job-to-slurm",
    "href": "nmfs-bioinf/sbatch.html#let-us-submit-the-bwa_index.sh-job-to-slurm",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.5 Let us submit the bwa_index.sh job to SLURM",
    "text": "6.5 Let us submit the bwa_index.sh job to SLURM\nSince all of the sbatch options are imbedded within the shell script it is easy to write the command to launch the script.\nLet’s prepare for that first. If you are on Alpine, make sure the slurm module is loaded.\n\n\nIf you are working on Alpine, be sure to paste this into your shell\n\nmodule load slurm/alpine\n\nNote that you do not need to be on a compute node to submit a job via sbatch—you can do that from a login node.\nTo launch the script using sbatch, all you do is:\n\n\nPaste this into your shell at the top level of the con-gen-csu repo\n\nsbatch scripts/bwa_index.sh data/genome/genome.fasta\n\nThe first argument is the script scripts/bwa_index.sh and the second, resources/genome.fasta, is the path to the genome that we want to index with bwa.\nWhen this command executes, it returns the SLURM_JOB_ID. Make a note of it.\nOnce you have launched the job, try using myjobs to see your job running. (You don’t have much time, because it doesn’t take very long).\n\n\n\n\n\n\nHey! That job ran in the current working directory\n\n\n\n\n\nNote that our script ran bwa index by passing it the path of a reference genome specified as a relative path: the path was relative to our current working directory.\nOne of the wonderful features of SLURM is that, when sbatch runs your script, it does so from the current working directory of the shell in which you ran sbatch.\n(I mention this because the first cluster I used was set up differently, and you had to explicitly tell it to run from the current working directory—which was the source of endless gnashing of teeth)\n\n\n\nOnce that job is done, use ls -lrt data/genome to see all the files that were newly created by the bwa-index.sh script."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#how-many-resources-did-that-job-use-seff",
    "href": "nmfs-bioinf/sbatch.html#how-many-resources-did-that-job-use-seff",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.6 How many resources did that job use? — seff",
    "text": "6.6 How many resources did that job use? — seff\nWhen you first start doing bioinformatics, you will not be very familiar with how long each job will run, or how much memory it will need.\nThat takes some time, but one helpful utility, seff, will tell you the effective usage of the allocated resources by any completed job.\nIt’s simple to use:\n\n\nHere is the sytnax\n\nseff slurm-jobid\n\n\n\n\n\n\n\nSelf-study\n\n\n\nTry that command, seff slurm-jobid, replacing slurm-jobid with the actual SLURM_JOB_ID of the job that you just ran.\n\n\n\n\n\n\n\n\nMore tips on learning about past job resource use\n\n\n\nYou can also use the sacct command. Check it out with man sacct.\nYou can get information much like seff for your recent jobs with sacct and it is somewhat easier to look at all the jobs that have run (or are running) in the last 12 to 48 hours with it. So, here we define a function to use it easily, and also we can give it more space to print the job names:\n\n\nPaste this into your shell to get a myacct function\n\nfunction myacct {\n  if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n  sacct  --format JobID,JobName%${JL},User,Group,State%20,Cluster,AllocCPUS,REQMEM,TotalCPU,Elapsed,MaxRSS,ExitCode,NNodes,NTasks -u $(whoami)\n}\n\nIf you like being able to use this, you ought to add it to your ~/.bashrc file."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-simple-job-with-default-sbatch-settings",
    "href": "nmfs-bioinf/sbatch.html#a-simple-job-with-default-sbatch-settings",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.7 A simple job with default sbatch settings",
    "text": "6.7 A simple job with default sbatch settings\nWe jumped right into talking about all the most useful options for sbatch.\nHowever, on Sedna (and, indeed, on most clusters), SLURM defines reasonable defaults for an sbatch job.\nIt even sends the output and error to reasonably named files, as we shall see.\nThe following lists the contents of a script called simple-15.sh that doesn’t do much:\n\nIt writes out the SLURM_JOB_ID to stdout\nIt writes a message to stderr\nThen it just sits there for 15 minutes.\n\n\n\nContents of scripts/simple-15.sh\n\n#!/bin/bash\n\n\n(\n  echo \"(stdout) Running with Slurm Job ID: $SLURM_JOB_ID\"\n  echo \"(stdout) Note that we are running this thing through\"\n  echo \"(stdout) the tee utility to try to unbuffer it.\"\n  echo\n) | tee simple-15.stdout\n\necho \"(stderr) This line is being written to stderr\" &gt; /dev/stderr\necho \"(stderr) Interestingly, stderr does not seem to get buffered.\" &gt;&gt; /dev/stderr\n\n\nsleep 900\n\n\n\n\n\n\n\nScripts running under SLURM have access to SLURM_* environment variables\n\n\n\nWhen a job is run by SLURM, it is done so in a shell environment that has a number of extra shell variables defined. In the above, we print the value of one of those: SLURM_JOB_ID.\n\n\nNow we will submit that script to run as a SLURM job with:\n\n\nPaste this into your shell\n\nsbatch scripts/simple-15.sh\n\nNow, use myjobs to see how many cores this job is using (1) and how much memory (4700 Mb on Sedna, 3840 on Alpine). Those are the default values.\nBy default, on some systems both stdout and stderr get written to slurm-%j.out (where %j% is replaced with the SLURM_JOB_ID).\nYou can see that is in that by doing:\n\n\nType this, but replace 331979 with whathever your actual SLURM job id is\n\ncat slurm-331979.out\n\nWe see that stderr gets written to slurm-%j.out immediately. The stdout stream is supposed to get written there, as well, but it seems that there is some hard-core buffering that goes on with slurm: we can see the output in simple-15.stdout, but not in slurm-%j.out.\n\n\n\n\n\n\nDon’t leave stdout and stderr un-redirected in your scripts\n\n\n\nSeeing the buffering issues above hardens my own convictions that you should not rely on SLURM to capture any output to stdout or stderr that is not otherwise redirected to a file. You should always be explicit about redirecting stdout or stderr within your own scripts to files where we want it to go.\nThat way the slurm output logs are left to mostly capture messages from SLURM itself (for example, telling you that you ran out of memory or the job was cancelled.)"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#oh-no-i-need-to-stop-my-jobs-scancel",
    "href": "nmfs-bioinf/sbatch.html#oh-no-i-need-to-stop-my-jobs-scancel",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.8 Oh no! I need to stop my job(s): scancel",
    "text": "6.8 Oh no! I need to stop my job(s): scancel\nNow, we have a job that is sitting around doing nothing for the next 15 minutes or so.\nThis is a great time to talk about how to abort jobs that are running under sbatch:\n\n\n\n\n\n\nCancelling jobs started with sbatch\n\n\n\nIf you have the job number (which is returned when sbatch launched the job or which you can see on the corresponding line of myjobs) you can use scancel followed by the job number.\nFor example:\nscancel 329656\n…but you have to replace the number above with your job’s job number.\nPlease do that now!"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-note-about-memory",
    "href": "nmfs-bioinf/sbatch.html#a-note-about-memory",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.9 A note about memory",
    "text": "6.9 A note about memory\n\n\n\n\n\n\nMemory gets allocated with cores or via --mem\n\n\n\nIt is quite clear that the --mem option to sbatch is intended to increase the memory allocated to the job; however adding cores with --cpus-per-task, without adding any options to dictate memory use, also increases the memory available.\nOn Sedna’s standard compute nodes, which each have 20 cores, if you ask for \\(x\\) cores, the total amount of memory your job will get is about \\(\\frac{x}{20} T\\), where \\(T\\) is a little less than the total memory on the machine.\nOn the standard memory compute nodes, that means your job gets roughly 4700 Mb (4.7 Gb) of RAM for each core that it is allocated. (9400 Mb or 9.4 Gb for each core on the “higher-memory” standard compute nodes, node[29-36])\nNote, however, that if the programs running in your job are not multithreaded, then you might not be able to use all those cores. In which case, it might be better to specify additional memory allocation with --mem, and leave the other cores to other users.\nHowever, on Alpine, if you ask for 3.74 Gb * 10 = 37.4 Gb of memory and only one core for computing, you will still be “charged” (i.e., you will run through your quota) as if you were using 10 cores! (i.e., billing is by the core, and you are charged for the number of cores that would give you a certain amount of memory)."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#what-happens-if-we-exceed-our-resources",
    "href": "nmfs-bioinf/sbatch.html#what-happens-if-we-exceed-our-resources",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.10 What happens if we exceed our resources?",
    "text": "6.10 What happens if we exceed our resources?\nR uses 8 bytes for each numeric value it stores in a vector. But it also seems to do some fancy stuff with delayed evaluation, etc. So it is hard to know exactly how much system RAM R’s process will use.\nNonetheless, I have found that the following R commands will exceed 4700 Mb of RAM:\nx &lt;- runif(1e9); y &lt;- x + 1; y[1:10]\nSomewhere in the y &lt;- x + 1 command, memory usage will exceed the default 4700 Mb (4.7 Gb) of RAM that SLURM allows by default (on SEDNA) or the 3.74 Gb of RAM allowed on Alpine.\nSo, what we are going to do here is run those commands in an sbatch script and see what happens. (You will probably exceed your allocated memory while doing bioinformatics, so you might as well get used to what that looks like.)\nHere is a listing of a shell script to run under sbatch to exceed our memory usage:\n\n\nContents of scripts/r-too-much-mem.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=2:00:00\n#SBATCH --output=r-too-much-mem-%j.out\n#SBATCH --error=r-too-much-mem-%j.err\n\n\nmkdir -p outputs\nEDIR=results/log/r-too-much-mem\nmkdir -p $EDIR\n\nmodule load R\nRscript --vanilla -e \"x &lt;- runif(1e9); y &lt;- x + 1; y[1:10]\" &gt; outputs/r-too-much.out 2&gt; $EDIR/log.txt\n\nRun it like this:\n\n\nPaste this into your shell\n\nsbatch scripts/r-too-much-mem.sh\n\nIt takes about a minute before it runs out of memory. During the time, check that it is still running using myjobs.\nOnce you see it is no longer running, check on the status of that job using myacct 15. The bottom line of output should show the results for that last job:\n\nThe State column will show OUT_OF_MEMORY\nThe MaxRSS column shows how much memory it used before failing. In my case, that was 4805260K.\n\n\n\n\n\n\n\nSelf study\n\n\n\nHow would you modify the scripts/r-too-much-mem.sh so that it did not run out of memory?\n\n\n\n\n\n\n\n\nSelf study answer\n\n\n\n\n\nYou need to allocate more memory to the job. You can do this by adding an sbatch directive line like:\n#SBATCH --mem=12G\namongst all the other sbatch directives. I don’t know if 12G of ram will be enough, but it might be. You could try it.\nOR as we will see below, you could add that memory option on the command line."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#sbatch-options-on-the-command-line-will-override-the-sbatch-directives-in-the-file",
    "href": "nmfs-bioinf/sbatch.html#sbatch-options-on-the-command-line-will-override-the-sbatch-directives-in-the-file",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.11 sbatch options on the command line will override the #SBATCH directives in the file",
    "text": "6.11 sbatch options on the command line will override the #SBATCH directives in the file\nYou can mix and match sbatch options on the command line with sbatch options in the #SBATCH directives in the script.\nThe option on the command line takes precedence over the same option in the file.\nSo, we could provide sufficient memory to scripts/r-too-much-mem.sh and at the same time, override the time it is allotted with this command:\n\n\nDon't paste this into your shell---it uses too much resources\n\nsbatch --mem=16G --time=01:00:00 scripts/r-too-much-mem.sh\n\nIf you launched that job, you could use your myjobs function to see that the MIN_MEMORY is 16G and the TIME_LIMIT is 1 hour.\nWhen it finished you could run myacct to see that it successfully completed, and look in the output file outputs/r-too-much.out to see that it printed the first 10 of one billion random numbers with 1 added to them."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#lets-run-out-of-time",
    "href": "nmfs-bioinf/sbatch.html#lets-run-out-of-time",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.12 Let’s run out of time",
    "text": "6.12 Let’s run out of time\nWe are going to rerun our simple-15.sh script, but only allocate one minute for it. So we should see it fail after only 1 minute.\n\n\n\n\n\n\nEric, why are making us run so many jobs that fail?\n\n\n\n\n\nLet’s be honest, as you embark on your bioinformatic career with SLURM, you are going to spend a significant amount of your time dealing with jobs that fail.\nYou might as well see jobs failing in several different ways so that you can recognize them when you see them later.\n\n\n\nSo, try this:\n\n\nPaste this into your shell\n\nsbatch --time=00:01:00 scripts/simple-15.sh\n\nThat will fail in a minute, after which, use myacct to see what it says about how it failed.\nIt will tell you that the main script hit a TIMEOUT and the batch job running in that script was CANCELLED"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-major-gotcha-do-not-direct-sbatchs-output-or-error-to-a-path-that-does-not-exist",
    "href": "nmfs-bioinf/sbatch.html#a-major-gotcha-do-not-direct-sbatchs-output-or-error-to-a-path-that-does-not-exist",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.13 A major gotcha: Do not direct sbatch’s output or error to a path that does not exist",
    "text": "6.13 A major gotcha: Do not direct sbatch’s output or error to a path that does not exist\n\n\n\n\n\n\nBeware!\n\n\n\nIf you try something like:\n#SBATCH output=outputs/mydir/outfile.txt\n#SBATCH error=outputs/mydir/errfile.txt\nbut do so without having actually made the directory outputs/mydir, then sbatch will fail, and it won’t be able to write out why it failed.\nThis is dynamite. If you ever find that your sbatch jobs are failing immediately but with no reasonable error messages anywhere, check to make sure that you aren’t sending SLURM’s output or error to a directory that does not exist.\nNote that you cannot make outputs/mydir within your script, because SLURM needs those directories before your script even gets executed."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#schedule-multiple-jobs-with-sbatch-using-a-for-loop",
    "href": "nmfs-bioinf/sbatch.html#schedule-multiple-jobs-with-sbatch-using-a-for-loop",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.14 Schedule multiple jobs with sbatch using a for loop",
    "text": "6.14 Schedule multiple jobs with sbatch using a for loop\nWe now consider a small job of mapping some paired end reads to the genome that we indexed a few steps ago.\nLet’s review the shell code in scripts/bwa-map.sh:\n\n\nContents of file scripts/bwa_map.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1:00:00\n#SBATCH --output=bwa_map-%j.out\n#SBATCH --error=bwa_map-%j.err\n\n# this is a little script to map a pair of fastq files\n# that are in data/fastqs.\n#\n# The first positional parameter ($1)\n# should be something like: DPCh_plate1_B10_S22\n\n# load the module that gives us the bwa software\nmodule load bwa\nmodule load samtools\n\n# make a directory for log output if it does not already exist\nLDIR=results/log/bwa_map\nODIR=results/mapped\n\nmkdir -p $LDIR $ODIR\n\nSAMP=$1\n\n# run bwa mem on the input and pipe it to samtools to sort it\nbwa mem data/genome/genome.fasta \\\n  data/fastqs/${SAMP}_R1.fq.gz \\\n  data/fastqs/${SAMP}_R2.fq.gz  2&gt; $LDIR/bwa_mem_${SAMP}.log | \\\n(\n  samtools view -u - | \\\n  samtools sort -o $ODIR/$SAMP.bam\n) 2&gt; $LDIR/samtools_$SAMP.log\n\n\nNote that we are using backslashes (\\) at the ends of the lines so that we can break a long line up onto separate lines of text.\nAlso note that grouping of commands by parentheses.\nThis script is run by passing it DPCh_plate1_B10_S22, DPCh_plate1_B11_S23, or DPCh_plate1_B12_S24, etc., as the first postitional parameter.\nSo, if we wanted to schedule three of those mapping jobs, we could do:\n\n\nPaste this code into your shell\n\nfor S in DPCh_plate1_B10_S22 DPCh_plate1_B11_S23 DPCh_plate1_B12_S24; do sbatch scripts/bwa_map.sh $S; done\n\nWhen that is done, check what is running with myjobs and alljobs. (But do it fast! Because these jobs finish very quickly)."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#final-thought",
    "href": "nmfs-bioinf/sbatch.html#final-thought",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.15 Final thought",
    "text": "6.15 Final thought\nThough multiple jobs can be submitted via sbatch easily using this sort of for loop construct, there is another way of launching multiple repetitions of the same job in SLURM: using job arrays\nWe will discuss that in the next section."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#prepare-for-this-session",
    "href": "nmfs-bioinf/slurm-arrays.html#prepare-for-this-session",
    "title": "7  Slurm Job Arrays",
    "section": "7.1 Prepare for this session",
    "text": "7.1 Prepare for this session\nGet on your cluster and navigate to the top level (con-gen-csu) of your fork of the class repository. Then, to make sure that you have all the latest updates from the repository, sync the main branch of your fork with the main branch of eriqande/con-gen-csu on GitHub, then in your shell, use git switch main to make sure you are on the main branch of your fork, and then use git pull origin main to pull down those changes."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#sbatchs---array-option",
    "href": "nmfs-bioinf/slurm-arrays.html#sbatchs---array-option",
    "title": "7  Slurm Job Arrays",
    "section": "7.2 sbatch’s --array option",
    "text": "7.2 sbatch’s --array option\nWhen you give sbatch the --array=1-10 option, say, then it runs your job 10 separate times, each time with the environment variable SLURM_ARRAY_TASK_ID set to a different number between 1 and 10.\nLet’s see a quick example, using the following script:\n\n\nContents of scripts/array_example.sh\n\n#!/bin/bash\n#SBATCH --time=00:05:00\n#SBATCH --output=my_output_%A_%a\n#SBATCH --error=my_error_%A_%a\n#SBATCH --array=1-10\n\nODIR=results/array_example\nmkdir -p $ODIR\n\n(\n  echo \"The SLURM_ARRAY_JOB_ID is : $SLURM_ARRAY_JOB_ID\"\n  echo \"The SLURM_ARRAY_TASK_ID is: $SLURM_ARRAY_TASK_ID\"\n  echo \"The SLURM_JOB_ID is: $SLURM_JOB_ID\"\n  echo\n  echo \"You can refer to this individual SLURM array task as: ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}\"\n  echo\n) &gt; $ODIR/output_$SLURM_ARRAY_TASK_ID.txt\n\nsleep 20\n\n\nThis is helpful to see a few of the variables that are defined in the environment of an array job:\n\nSLURM_ARRAY_JOB_ID: the overall JOB_ID for the whole array\nSLURM_ARRAY_TASK_ID: the index that runs from 1 to 10 in this case\nSLURM_JOB_ID: The underlying job_id\n\nOne can run this script, but we don’t want a whole classroom full of people throwing all these jobs on the cluster. So, get into groups of three or four, talk amongst yourselves about what you think this script will do, and then have just one person from each group launch the job with the following command:\n\n\nOne person from each group, paste this into your shell\n\nsbatch scripts/array_example.sh\n\nAnd then use myjobs and alljobs to watch what is happening on the cluster.\nWhen that job is done, or when it is running, look at the values written to the first output files:\n\n\nPaste this into you shell\n\nhead results/array_example/output_{1..10}.txt\n\n\n\n\n\n\n\nCool syntax interlude: {1..10}\n\n\n\n\n\nOn the shell, if you do something like:\n\n{2..7}: that will expand to 2 3 4 5 6 7\n{a..g}: that will expand to a b c d e f g\n{0001..0015}: that will expand to 0001 0002 0003 0004 0005 0006 0007 0008 0009 0010 0011 0012 0013 0014 0015\n{F..M}: that will expand to F G H I J K L M\n\n\n\n\nFrom looking at the array_example output files, we can infer that:\n\nSLURM_ARRAY_JOB_ID: is a unique number that refers to the entire set of jobs in the array\nSLURM_ARRAY_TASK_ID: is the integer that is being cycled over in the array job\nSLURM_JOB_ID: is a unique SLURM_JOB_ID of the specific array task.\n\nAlso, in alljobs and myjobs you see that jobs can be referred to like 331989_4. That is SLURM_ARRAY_JOB_ID + underscore + SLURM_ARRAY_TASK_ID. For example: scancel 376578_12.\nIn the slurm output files specified like:\nSBATCH --output=my_output_%A_%a\n\n%A expands to SLURM_ARRAY_JOB_ID\n%a expands to SLURM_ARRAY_TASK_ID\n\nIf you need to cancel a particular array task using scancel you would typically use SLURM_ARRAY_JOB_ID + underscore + SLURM_ARRAY_TASK_ID, unless you wanted to cancel all remaining instances of an array task, in which case you would use\nscancel SLURM_ARRAY_JOB_ID"
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#variations-on-the-array_spec",
    "href": "nmfs-bioinf/slurm-arrays.html#variations-on-the-array_spec",
    "title": "7  Slurm Job Arrays",
    "section": "7.3 Variations on the <array_spec>",
    "text": "7.3 Variations on the &lt;array_spec&gt;\nThere are some important variations to how you can specify those array numbers:\n\n--array=1-50: simple, consecutive numbers\n--array=1-10:3: 1 through 10 by threes, (so 1,4,7,10)\n--array=1,2,3,6,9,15 non-consecutive numbers\n--array=1-21:10,100-200:50: non-consecutive ranges. It becomes (1,11,21,100,150,200)\n--array=1-10,4: WARNING, this becomes 1,2,3,4,5,6,7,8,9,10,4. SLURM does not check that the array numbers are unique, so task array 4 would be run twice (possibly concurrently overwriting the output.)\n--array=1-20%5 VERY IMPORTANT SYNTAX: Run the jobs, but don’t ever have more the 5 running at a time. This is useful for making sure your jobs don’t consume every last CPU on Sedna.\n\nLet’s try putting all these together. Read the following command and, talking among the members of your group, figure out what the array spec is doing. Then have one person from each group submit the job with the following command:\n\n\nOne person from each group, aste this into your shell\n\nsbatch --array=100,200,300-400:10%5 scripts/array_example.sh\n\nThen use myjobs and alljobs to see what is going on in the cluster.\nDo you ever have more than 5 jobs running?"
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#translating-array-indexes-to-job-instances",
    "href": "nmfs-bioinf/slurm-arrays.html#translating-array-indexes-to-job-instances",
    "title": "7  Slurm Job Arrays",
    "section": "7.4 Translating Array Indexes to Job Instances",
    "text": "7.4 Translating Array Indexes to Job Instances\n\nThe user is left to translate what a job array index of, say, 7, means in terms of what actions that array task should take.\nQuite often you will want to map an array index to a different file to analyze, or perhaps a different region of a chromosome to do variant calling on, etc.\nA flexible and generic way of doing this mapping from array indexes to job specifics is to first define the variables (things like filenames, etc.) required by each array task in a simple TAB-delimited text file in which the first row holds the names of the variables in different TAB-separated columns, and each row below that holds the values that those variables should take for different values of the array index.\nThe array index itself should be listed in the first column, whose name is index.\n\n\n7.4.1 An example TAB-delimited file\nLet’s explore the following file, which is at data/sample-array-info.tsv:\n\n\nContents of inputs/fq-samples.tsv\n\nindex   sample  library flowcell    platform    lane    barcode fq1 fq2\n1   DPCh_plate1_B10_S22 plate1  HTYYCBBXX   ILLUMINA    1   GACGTCAT+TCAACTGG   fastqs/DPCh_plate1_B10_S22_R1.fq.gz fastqs/DPCh_plate1_B10_S22_R2.fq.gz\n2   DPCh_plate1_B11_S23 plate1  HTYYCBBXX   ILLUMINA    1   CCGCTTAA+ACGATGAC   fastqs/DPCh_plate1_B11_S23_R1.fq.gz fastqs/DPCh_plate1_B11_S23_R2.fq.gz\n3   DPCh_plate1_B12_S24 plate1  HTYYCBBXX   ILLUMINA    1   GACGAACT+TCGCATTG   fastqs/DPCh_plate1_B12_S24_R1.fq.gz fastqs/DPCh_plate1_B12_S24_R2.fq.gz\n4   DPCh_plate1_C10_S34 plate1  HTYYCBBXX   ILLUMINA    1   AGTCACAT+CCATAATG   fastqs/DPCh_plate1_C10_S34_R1.fq.gz fastqs/DPCh_plate1_C10_S34_R2.fq.gz\n5   DPCh_plate1_C11_S35 plate1  HTYYCBBXX   ILLUMINA    1   CCTTTCAC+AAACAAGA   fastqs/DPCh_plate1_C11_S35_R1.fq.gz fastqs/DPCh_plate1_C11_S35_R2.fq.gz\n6   DPCh_plate1_C12_S36 plate1  HTYYCBBXX   ILLUMINA    1   GCACACAA+TCGTCAAG   fastqs/DPCh_plate1_C12_S36_R1.fq.gz fastqs/DPCh_plate1_C12_S36_R2.fq.gz\n7   DPCh_plate1_D09_S45 plate1  HTYYCBBXX   ILLUMINA    1   CTTTCCCT+AGTAAGCC   fastqs/DPCh_plate1_D09_S45_R1.fq.gz fastqs/DPCh_plate1_D09_S45_R2.fq.gz\n8   DPCh_plate1_D11_S47 plate1  HTYYCBBXX   ILLUMINA    1   GACAATTC+CATATCGT   fastqs/DPCh_plate1_D11_S47_R1.fq.gz fastqs/DPCh_plate1_D11_S47_R2.fq.gz\n9   DPCh_plate1_F10_S70 plate1  HTYYCBBXX   ILLUMINA    1   ACACGACT+CTGCGGAT   fastqs/DPCh_plate1_F10_S70_R1.fq.gz fastqs/DPCh_plate1_F10_S70_R2.fq.gz\n10  DPCh_plate1_F11_S71 plate1  HTYYCBBXX   ILLUMINA    1   TCCACGTT+GTTCAACC   fastqs/DPCh_plate1_F11_S71_R1.fq.gz fastqs/DPCh_plate1_F11_S71_R2.fq.gz\n11  DPCh_plate1_F12_S72 plate1  HTYYCBBXX   ILLUMINA    1   AACCAGAG+AACCGAAG   fastqs/DPCh_plate1_F12_S72_R1.fq.gz fastqs/DPCh_plate1_F12_S72_R2.fq.gz\n12  DPCh_plate1_G09_S81 plate1  HTYYCBBXX   ILLUMINA    1   CGAATACG+GTCGGTAA   fastqs/DPCh_plate1_G09_S81_R1.fq.gz fastqs/DPCh_plate1_G09_S81_R2.fq.gz\n13  DPCh_plate1_G10_S82 plate1  HTYYCBBXX   ILLUMINA    1   CAGTGCTT+ACCTGGAA   fastqs/DPCh_plate1_G10_S82_R1.fq.gz fastqs/DPCh_plate1_G10_S82_R2.fq.gz\n14  DPCh_plate1_G12_S84 plate1  HTYYCBBXX   ILLUMINA    1   TCCATTGC+AGACCGTA   fastqs/DPCh_plate1_G12_S84_R1.fq.gz fastqs/DPCh_plate1_G12_S84_R2.fq.gz\n15  DPCh_plate1_H09_S93 plate1  HTYYCBBXX   ILLUMINA    1   GTCGATTG+ACGGTCTT   fastqs/DPCh_plate1_H09_S93_R1.fq.gz fastqs/DPCh_plate1_H09_S93_R2.fq.gz\n16  DPCh_plate1_H10_S94 plate1  HTYYCBBXX   ILLUMINA    1   ATAACGCC+TGAACCTG   fastqs/DPCh_plate1_H10_S94_R1.fq.gz fastqs/DPCh_plate1_H10_S94_R2.fq.gz\n\nAn easier way to view this might be to look at in on GitHub at: here\nSomething that would be really handy would be a little shell script that would pick out a particular line of that file that corresponds to the value in the index column and then define some shell variables according to the columns names: index, sample, library, flowcell, platform, lane, barcode, fq1, fq2 so that they could be used in an array script.\nWe have already seen some of the bash and awk machinery that would make that possible, and we have wrapped it up in the line-assign.sh script.\n\n\n7.4.2 The line-assign.sh script\nWithin the repository is a script called scripts/line-assign.sh, that looks like this:\n\n\nContents of scripts/line-assign.sh\n\n#!/bin/bash\n# simple script.  Arguments are:\n#  1. The index to pick out (like 1, or 2, or 7, etc)\n#  2. Path to a TAB delimited file with the first column named index\n#     and subsequent columns named valid shell variable names.\n#\n# This script will pick out the line that matches $1, and return a command\n# line assigning the column header names as shell variables whose values are the\n# values in each cell in the file $2.\n#\n# Note: this is not written with a whole lot of error checking or catching.\n#\n\n\nif [ $# -ne 2 ]; then\n  echo \"Wrong number of arguments in $0 \" &gt; /dev/stderr\nfi\n\nawk -F\"\\t\" -v LINE=$1 '\n  $1 == \"index\" {for(i=1; i&lt;=NF; i++) vars[i]=$i; next}\n  $1 == LINE {for(i=1; i&lt;=NF; i++) printf(\"%s=\\\"%s\\\"; \", vars[i], $i); printf(\"\\n\");}\n' $2\n\nLet’s see what sorts of results this produces:\n\n\nPaste this into your shell\n\n ./scripts/line-assign.sh 3 data/sample-array-info.tsv\n\nWhoa! It returns a command line that assigns values to a lot of shell variables.\nSo, if we wanted to run that command line, we would have to precede it with the eval keyword (because the command line itself includes special characters like =). Let’s do that like this:\n\n\nPaste this into your shell\n\nCOMM=$(./scripts/line-assign.sh 3 data/sample-array-info.tsv)\neval $COMM\n\nNow, that you have done that, you can see that a lot of variables have been assigned the values on the index == 3 line of our TSV file:\n\n\nPaste this into your shell\n\n(\necho \"index:      $index\"\necho \"sample:     $sample\"\necho \"library:    $library\"\necho \"platform:   $platform\"\necho \"lane:       $lane\"\necho \"barcode:    $barcode\"\necho \"fq1:        $fq1\"\necho \"fq2:        $fq2\"\n)\n\nHoly Smokes! These are variables that we could use in a job array script.\nIn this case, we can assign values to the pesky Read Group string for bwa mem."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#putting-it-all-together-a-read-mapping-job-array",
    "href": "nmfs-bioinf/slurm-arrays.html#putting-it-all-together-a-read-mapping-job-array",
    "title": "7  Slurm Job Arrays",
    "section": "7.5 Putting it all together: a read-mapping job array",
    "text": "7.5 Putting it all together: a read-mapping job array\nWe can now elaborate on our simple bwa_map.sh shell script and turn that into a SLURM job array script. The key here is that when SLURM runs the script as a slurm array, there will be an environment variable called SLURM_ARRAY_TASK_ID that we can use to pick out the desired sample from the data/sample-array-info.tsv file.\nHere is what it looks like:\n\n\nContents of scripts/bwa_map_array.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1:00:00\n#SBATCH --output=bwa_map_array-%A_%a.out\n#SBATCH --error=bwa_map_array-%A_%a.err\n#SBATCH --array=1-16\n\n# load the module that gives us the bwa software\nmodule load bwa\nmodule load samtools\n\n# make a directory for log output if it does not already exist\nLDIR=results/log/bwa_map_array\nODIR=results/mapped\n\nmkdir -p $LDIR $ODIR\n\n# get shell variables for this array task:\nCOMM=$(./scripts/line-assign.sh $SLURM_ARRAY_TASK_ID data/sample-array-info.tsv)\neval $COMM\n\n\n# run bwa mem on the input and pipe it to samtools to sort it\nbwa mem \\\n  -R \"@RG\\tID:${sample}.${library}.${flowcell}.${lane}\\tSM:${sample}\\tLB:${library}\\tBC:${barcode}\\tPU:\\tID:${sample}.${library}.${flowcell}.${lane}.${barcode}\\tPL:ILLUMINA\" \\\n  data/genome/genome.fasta \\\n  $fq1 $fq2  2&gt; $LDIR/bwa_mem_$sample.log | \\\n(\n  samtools view -u - | \\\n  samtools sort -o $ODIR/$sample.bam\n) 2&gt; $LDIR/samtools_$sample.log\n\nThe main differences from our simple bwa_map.sh script are:\n\nThe SLURM_ARRAY_TASK_ID is used to pick out the right line from our TSV file of information\nThe slurm output and error have been changed to use %A_%a notation.\nThere is a #SBATCH --array=1-16 directive, to run it over the 16 different files.\nThe shell code that runs bwa and samtools uses the variables that were defined by eval-ing the results of the call to the line-assign.sh script.\n\nYou can launch that job array and see how it goes:\n\n\nPaste this into your shell\n\nsbatch scripts/bwa_map_array.sh\n\nThen check with myjobs and alljobs to see what is happening on Sedna.\nThat runs really fast.\nAfterward you can check that the results exist:\n\n\nPaste this into your shell\n\nls -l results/mapped\n\nAnd you can look at the logs for bwa mem:\n\n\nPaste this into your shell\n\ntail  results/log/bwa_map_array/bwa_mem_*\n\nTo see what one of the output files looks like, you can use samtools. On Alpine, you have to get onto a compute node to do that, so you would first do:\n\n\nDo this if you are on the Alpine cluster\n\nmodule load slurm/alpine\nsrun --partition=atesting --pty /bin/bash\n\nThen do like this:\n\n\nPaste this into your shell\n\nmodule load samtools\nsamtools view results/mapped/DPCh_plate1_F10_S70.bam | less -S\n\nUse the left and right arrows, and space bar and backspace to see all parts of the file.\nRemember to hit q to get out of the less viewer."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#wrap-up",
    "href": "nmfs-bioinf/slurm-arrays.html#wrap-up",
    "title": "7  Slurm Job Arrays",
    "section": "7.6 Wrap Up",
    "text": "7.6 Wrap Up\nSo, that was a quick tour of the capabilities of sbatch.\nOddly enough, I haven’t directly launched any jobs using sbatch (apart from the example jobs for this course) in many months, because I drive all my bioinformatics projects using Snakemake, which sends my jobs to SLURM for me.\nEventually we will have an introduction to Snakemake!"
  },
  {
    "objectID": "nmfs-bioinf/snake.html",
    "href": "nmfs-bioinf/snake.html",
    "title": "8  Snakemake Tutorial Introduction",
    "section": "",
    "text": "Snakemake is a Python-based system for orchestrating bioinformatic workflows.\nIt is loosely based on the GNU utility make that was designed for coordinating the compilation of large software projects.\nBut, it is tailored to bioinformatic problems and it is much easier to use the make (in my opinion).\nNonetheless, it has a bit of a learning curve.\n\nOur goal today is to let y’all interact with Snakemake on a small example project, to get a sense for some of its features.\nThis part of the presentation is structured as a set of slides, rather than as a book.\nPlease clink on this following link:\nSnakemake Slides"
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#sequences",
    "href": "nmfs-bioinf/bioinf-formats.html#sequences",
    "title": "9  Bioinformatic file formats",
    "section": "9.1 Sequences",
    "text": "9.1 Sequences"
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#fastq",
    "href": "nmfs-bioinf/bioinf-formats.html#fastq",
    "title": "9  Bioinformatic file formats",
    "section": "9.2 FASTQ",
    "text": "9.2 FASTQ\nThe FASTQ format is the standard format for lightly-processed data coming off an Illumina machine. If you are doing whole genome sequencing, it is typical for the Illumina processing pipeline to have separated all the reads with different barcodes into different, separate files. Typically this means that all the reads from one library prep for one sample will be in a single file. The barcodes and any associated additional adapter sequence is typically also removed from the reads. If you are processing RAD data, the reads may not be separated by barcode, because, due to the vagaries of the RAD library prep, the barcodes might appear on different ends of some reads than expected.\nA typical file extension for FASTQ files is .fq. Almost all FASTQ files you get from a sequencer should be gzipped to save space. Thus, in order to view the file you will have to uncompress it. Since you would, in most circumstances, want to visually inspect just a few lines, it is best to do that with gzcat and pipe the output to head.\nAs we have seen, paired-end sequencing produces two separate reads of a DNA fragment. Those two different reads are usually stored in two separate files named in such a manner as to transparently denote whether it contains sequences obtained from read 1 or read 2. For example bird_08_B03_R1.fq.gz and bird_08_B03_R2.fq.gz. Read 1 and Read 2 from a paired read must occupy the sames lines in their respective files, i.e., lines 10001-10004 in bird_08_B03_R1.fq.gz and lines 10001-10004 in bird_08_B03_R2.fq.gz should both pertain to the same DNA fragment that was sequenced. That is what is meant by “paired-end” sequencing: the sequences come in pairs from different ends of the same fragment.\nThe FASTQ format is very simple: information about each read occupies just four lines. This means that the number of lines in a proper FASTQ file must always be a multiple of four. Briefly, the four lines of information about each read are always in the same order as follows:\n\nAn Identifier line\nThe DNA sequence as A’s, C’s, G’s and T’s.\nA line that is almost always simply a + sign, but can optionally be followed by a repeat of the ID line.\nAn ASCII-encoded, Phred-scaled base quality score. This gives an estimated measure of certainty about each base call in the sequence.\n\nThe code block below shows three reads worth (twelve lines) of information from a FASTQ file. Take a moment to identify the four different lines for each read.\n@K00364:64:HTYYCBBXX:1:1108:4635:14133/1\nTAGAATACGCCAGGTGTAAGAATAGTAGAATACGCCAGGTGTAAGAATAGTAGAACACGCCAGGTGTAAGAATAGTAGAA\n+\nAAAFFJJJJJFFJFJJJFJJFFJFJFJJJJJJJJFFJJJFJJJFJJAJJFJJFJJFJJJ7JJJF-&lt;JAFJJ&lt;F&lt;AJAJJF\n@K00364:64:HTYYCBBXX:1:1108:5081:25527/1\nAAAACACCAAAAGAAAGATGCCCAGGGTCCCTGCTCATCTGCGTGAAAGTGACTTAGGCATGCTGCAAGGAGGCATGAGG\n+\nAAFFFJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ\n@K00364:64:HTYYCBBXX:1:1108:5852:47295/1\nAGGTGGCTCTAGAAGGTTCTCGGACCGAGAAACAGCCTCGTACATTTGCAATGATTTCAATTCATTTTGACCATTACGAA\n+\nAAFFFJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ\nLines 2 and 3 are self-explanatory, but we will expound upon lines 1 and 4 below.\n\n9.2.1 Line 1: Illumina identifier lines\nThe identifier line can be just about any string that starts with an @, but, from Illumina data you might see something like this:\n@K00364:64:HTYYCBBXX:1:1108:4635:14133/1\nThe colons (and the /) are field separators. The separate parts of the line above are interpreted something along the lines as follows (keeping in mind that Illumina occasionally changes the format and that there may be additional optional fields):\n@           : mandatory character that starts the ID line\nK00364      : Unique sequencing machine ID \n64          : Run number on instrument\nHTYYCBBXX   : Unique flow cell identifier\n1           : Lane number\n1108        : Tile number (section of the lane)\n4635        : x-coordinate of the cluster within the tile\n14133       : y-coordinate of the cluster within the tile\n1           : Whether the sequence is from read 1 or read 2 \nFor Illumina data processed with versions 1.8+ of Casava, the identifier lines are a little different, like this:\n@A00600:191:HY75HDSX2:1:2624:27480:35744 2:N:0:AAGACCGT+CAATCGAC\nin which we have:\n@           : mandatory character that starts the ID line\nA00600      : Unique sequencing machine ID \n191         : Run number on instrument\nHY75HDSX2   : Unique flow cell identifier\n1           : Lane number\n2624        : Tile number (section of the lane)\n27480       : x-coordinate of the cluster within the tile\n35744       : y-coordinate of the cluster within the tile\n\n2                 : Whether the sequence is from read 1 or read 2 \nN                 : N if the sequence was not flagged as crappy by the maching, Y otherwise\n0                 : 0 when no control bits are turned on\nAAGACCGT+CAATCGAC : Index/barcode of the sample\nQuestion: For paired reads, do you expect the x- and y-coordinates for read 1 and read 2 to be the same?\n\n\n9.2.2 Line 4: Base quality scores\nThe base quality scores give an estimate of the probability that the base call is incorrect. This comes from data the sequencer collects on the brightness and compactness of the cluster radiance, other factors, and Illumina’s own models for base call accuracy. If we let \\(p\\) be the probability that a base is called incorrectly, then \\(Q\\), the Phred-scaled base quality score, is:\n\\[\nQ = \\lfloor-10\\log_{10}p\\rfloor,\n\\] where \\(\\lfloor x \\rfloor\\) means “the largest integer smaller than \\(x\\).\nTo get the estimate of the probability that the base is called incorrectly from the Phred scaled score, you invert the above equation:\n\\[\np = \\frac{1}{10^{Q/10}}\n\\] Base quality scores from Illumina range from 0 to 40. The easiest values to use as guideposts are \\(Q = 0, 10, 20, 30, 40\\), which correspond to error probabilites of 1, 1 in 10, 1 in 100, 1 in 1,000, and 1 in 10,000, respectively.\nAll this is fine and well, but when we look at the quality score line above we see something like 7JJJF-&lt;JAFJJ&lt;F&lt;AJAJJF. What gives? Well, from a file storage and parsing perspective, it makes sense to only use a single character to store the quality score for every base. So, that is what has been done: each of those single characters represents a quality score—a number between 0 and 40, inclusive.\nThe values that have been used are the decimal representations of ASCII text characters minus 33.\nThe decimal representation or each character can be found in Figure 9.1.\n\n\n\n\n\nFigure 9.1: This lovely ASCII table shows the binary, hexadecimal, octal and decimal representations of ASCII characters (in the corners of each square; see the legend rectangle at bottom. Table produced from TeX code written and developed by Victor Eijkhout available at https://ctan.math.illinois.edu/info/ascii-chart/ascii.tex\n\n\n\n\nThe decimal representation is in the upper left of each character’s rectangle.\nFind the characters corresponding to base quality scores of 0, 10, 20, 30, and 40. Remember that the base quality score is the character’s decimal representation minus 33.\nHere is another question: why do you think the scale starts with ASCII character 33?"
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#fasta",
    "href": "nmfs-bioinf/bioinf-formats.html#fasta",
    "title": "9  Bioinformatic file formats",
    "section": "9.3 FASTA",
    "text": "9.3 FASTA\nThe FASTQ format, described above, is tailored for representing short DNA sequences—and their associated quality scores—that have been generated from high-throughput sequencing machines. A simpler, leaner format is used to represent longer DNA sequences that have typically been established from a lot of sequencing, and which no longer travel with their quality scores. This is the FASTA format, which you will typically see storing the DNA sequence from reference genomes. FASTA files typically use the file extensions .fa, .fasta, or .fna, the latter denoting it as a FASTA file of nucleotides.\nIn an ideal world, a reference genome would contain a single, uninterrupted sequence of DNA for every chromosome. While the resources for some well-studied species include “chromosomal-level assemblies” which have much sequence organized into chromosomes in a FASTA file, even these genome assemblies often include a large number of short fragments of sequence that are known to belong to the species, but whose location and orientation in the genome remain unknown.\nMore often, in conservation genetics, the reference genome for an organism you are working on might be the product of a recent, small-scale, assembly of a low-coverage genome. In this case, the genome may be represented by thousands, or tens of thousands, of scaffolds, only a few of which might be longer than one or a few megabases. All of these scaffolds go into the FASTA file of the reference genome.\nHere are the first 10 lines of the FASTA holding a reference genome for Chinook salmon:\n&gt;CM008994.1 Oncorhynchus tshawytscha isolate JC-2011-M1\nAGTGTAGTAGTATCTTACCTATATAGGGGACAGTGTAGTAGTATCTTACTTATTTGGGGGACAATGCTCTAGTGTAGTAG\nAATCTTACCTTTATAGGGGACAGTGCTGGAGTGCACTGGTATCTTACCTATATAGGGGACAGTGCTGGAGTGTAGTAGTG\nTCTCGGCCCACAGCCGGCAGGCCTCAGTCTTAGTTAGACTCTCCACTCCATAAGAAAGCTGGTACTCCATCTTGGACAGG\nACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA\nTGTGTGGTACATGTTTACAGAGAAGGAGtatattaaaaacagaaaactgTTTTGGttgaaatatttgtttttgtctgaAG\nCCCGAAAAACACATGAAATTCAAAAGATAATTTGACCTACGCACTAACTAGGCTTTTCAGCAGCTCAACTACTGTCCGTT\nTATTGATCTACTGTACTGCAACAACATATGTACTCACACAACAGACTATATTGGATTCAGACAGGACCTATAGGTTACCA\nTGCTTCCTCTCTACAGGACCTATAGGTTACCATGCTTCCTCTCTACAAGGTCTATAGGTTACCATGCGTCCTCTCTACAG\nGACCTATAGGTTACCATGCTTCCTCTCTACAGGGCCTATAGGTTACCATGCTTCCTCTCTACAGGACCTGTAGGTTACCA\nThe format is straightforward: any line starting with &gt; is interpreted as a line holding the identifier of the following sequence. In this case, the identifier is CM008994.1. The remainder of the line (following the first white space) are further comments about the sequence, but will not typically be carried through downstream analysis (such as aligment) pipelines. In this case CM008994.1 is the name of an assembled chromosome in this reference genome. The remaining lines give the DNA sequence of that assembled chromosome.\nIt is convention with FASTA files that lines of DNA sequence should be less than 80 characters, but this is inconsistently enforced by different analysis programs. However, most of the FASTA files you will see will have lines that are 80 characters long.\nIn the above fragment, we see DNA sequence that is either upper or lower case. A common convention is that the lowercase bases are segments of DNA that have been inferred (by, for example RepeatMasker) to include repetitive DNA. It is worth noting this if you are trying to design assays from sequence data! However, not all reference genomes have repeat-sequences denoted in this fashion.\nMost reference genomes contain gaps. Sometimes the length of these gaps can be accurately known, in which case each missing base pair is replaced by an N. Sometimes gaps of unknown length are represented by a string of N’s of some fixed length (like 100).\nFinally, it is worth reiterating that the sequence in a reference-genome FASTA file represents the sequence only one strand of a double-stranded molecule. In chromosomal-scale assemblies there is a convention to use the strand that has its 5’ end at the telomere of the short arm of the chromosome [@cartwrightMultiplePersonalitiesWatson2011]. Obviously, such a convention cannot be enforced in a low-coverage genome in thousands of pieces. In such a genome, different scaffolds will represent sequence on different strands; however the sequence in the FASTA file, whichever strand it is upon, is taken to be the reference, and that sequence is referred to as the forward strand of the reference genome.\n\n9.3.1 Genomic ranges\nAlmost every aspect of genomics or bioinformatics involves talking about the “address” or “location” of a piece of DNA in the reference genome. These locations within a reference genome can almost universally be described in terms of a “genomic range” with a format that looks like:\nSegmentName:start-stop\nFor example,\nCM008994.1:1000001-1001000\ndenotes the 1 Kb chunk of DNA starting from position 1000001 and proceeding to (and including!) position 1001000. Such nomenclature is often referred to as the genomic coordinates of a segment.\nIn most applications we will encounter, the first position in a chromosome is labeled 1. This is called a base 1 coordinate system. In some genomic applications, a base 0 coordinate system is employed; however, for the most part such a system is only employed internally in the guts of code of software that we will use, while the user interface of the software consistently uses a base 1 coordinate system.\nSometimes you will have to convert between base 0 and base1 coordinate systems. Thinking about how to do this is easy if you keep a simple picture in your head—I like to think of it as, the base-1 coordinate system counts “beads” that are the actual base pairs, while the base-0 system counts “fences” that separate the beads. In the following picture, the fences and their corresponding numbers are red, while the beads and their corresponding numbers are black.\n\n\n\n\n\nFrom this picture, it is pretty clear that if you have a base-1 range from \\(x\\) to \\(y\\) (with \\(x \\leq y\\)), then you could refer to that by the base-0 range from \\(x-1\\) to \\(y\\) (those are the “fences” that contain the beads). Likewise, if you have a base-0 range from \\(v\\) to \\(w\\) (with \\(v&lt;w\\)), then the corresponding base-1 range would be from \\(v+1\\) to \\(w\\), as those are the beads that are contained by the fences at \\(v\\) and \\(w\\).\n\n\n9.3.2 Extracting genomic ranges from a FASTA file\nCommonly (for example, when designing primers for assays) it is necessary to pick out a precise genomic range from a reference genome. This is something that you should never try to do by hand. That is too slow and too error prone. Rather the software package samtools (which will be discussed in detail later) provides the faidx utility to index a FASTA file. It then uses that index to provide lightning fast access to specific genomic coordinates, returning them in a new FASTA file with identifiers giving the genomic ranges. Here is an example using samtools faidx to extract four DNA sequences of length 150 from within the Chinook salmon genome excerpted above:\n# assume the working directory is where the fasta file resides\n\n# create the index\nsamtools faidx GCA_002831465.1_CHI06_genomic.fna\n\n# that created the file: GCA_002831465.1_CHI06_genomic.fna.fai\n# which holds four columns that constitute the index\n\n# now we extract the four sequences:\nsamtools faidx \\\n    GCA_002831465.1_CHI06_genomic.fna \\\n    CM009007.1:3913989-3914138 \\\n    CM009011.1:2392339-2392488 \\\n    CM009013.1:11855194-11855343 \\\n    CM009019.1:1760297-1760446\n    \n# the output is like so:\n&gt;CM009007.1:3913989-3914138\nTTACCGAtggaacattttgaaaaacacaaCAATAAAGCCTTGTGTCCTATTGTTTGTATT\nTGCTTCGTGCTGTTAATGGTAgttgcacttgattcagcagccgtAGCGCCGGGAAggcag\ntgttcccattttgaaaaaTGTCATGTCTGA\n&gt;CM009011.1:2392339-2392488\ngatgcctctagcactgaggatgccttagaccgctgtgccactcgggaggccttcaGCCTA\nACTCTAACTGTAAGTAAATTGTGTGTATTTTTGGGTACATTTCGCTGGTCCCCACAAGGG\nGAAAGggctattttaggtttagggttaagg\n&gt;CM009013.1:11855194-11855343\nTGAGGTTTCTGACTTCATTTTCATTCACAGCAGTTACTGTATGCCTCGGTCAAATTGAAA\nGGAAAGTAAAGTAACCATGTGGAGCTGtatggtgtactgtactgtactgtattgtactgt\nattgtgtgGGACGTGAGGCAGGTCCAGATA\n&gt;CM009019.1:1760297-1760446\nttcccagaatctctatgttaaccaaggtgtttgcaaatgtaacatcagtaggggagagag\naggaaataaagggggaagaggtatttatgactgtcataaacctacccctcaggccaacgt\ncatgacactcccgttaatcacacagactGG\n\n\n9.3.3 Downloading reference genomes from NCBI\nI might want to write blurb here about how much nicer I find it is to use the ftp link: http://ftp.ncbi.nlm.nih.gov/genomes/genbank/ to find genomes on Genbank. Although now that the number of genomes is growing so quickly, it can take quite a while for the pages to load. But I still find it easier to navigate to exactly the files I want using this system than the actual user interface that they have."
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#sambamfiles",
    "href": "nmfs-bioinf/bioinf-formats.html#sambamfiles",
    "title": "9  Bioinformatic file formats",
    "section": "9.4 Alignments",
    "text": "9.4 Alignments\nA major task in bioinformatics is aligning reads from a sequencing machine to a reference genome. We will discuss the operational features of that task in a later chapter, but here we treat the topic of the SAM, or Sequence Alignment Map, file format which is widely used to represent the results of sequence alignment. We attempt to motivate this topic by first considering a handful of the intricacies that arise during sequence alignment, before proceeding to a discussion of the various parts of the SAM file that are employed to handle the many and complex ways in which DNA alignments can occur and be represented. This will necessarily be an incomplete and relatively humane introduction to SAM files. For the adventurous a more complete—albeit astonishingly terse—description of the SAM format specification is maintained and regularly updated.\n\n9.4.1 How might I align to thee? Let me count the ways…\nWe are going to consider the alignment of very short (10 bp) paired-end reads from the ends of a short (50 bp) fragment from the fourth line of the FASTA file printed above. In other words, those 80 bp of the reference genome are:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\nAnd we will be considering double-stranded DNA occupying the middle 50 base pairs of that piece of reference genome. That piece of double stranded DNA looks like:\n5'  ACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGA  3'\n    ||||||||||||||||||||||||||||||||||||||||||||||||||\n3'  TGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACT  5'\nIf we print it alongside (underneath, really) our reference genome, we can see where it lines up:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGA  3'\n                   ||||||||||||||||||||||||||||||||||||||||||||||||||\n               3'  TGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACT  5'\nNow, remember that any template being sequenced on an Illumina machine is going to be single-stranded, and we have no control over which strand, from a double-stranded fragment of DNA, will get sequenced. Futhermore, recall that for this tiny example, we are assuming that the reads are only 10 bp long. Ergo, if everything has gone according to plan, we can expect to see two different possible templates, where I have denoted the base pairs that do not get sequenced with -’s\neither:\n               5'  ACCTGCAGGA------------------------------AACACAGTGA  3'\nor:\n               3'  TGGACGTCCT------------------------------TTGTGTCACT  5'\nIf we see the top situation, we have a situation in which the template that reached the lawn on the Illumina machine comes from the strand that is represented in the reference genome. This is called the forward strand. On the other hand, the bottom situation is one in which the template is from the reverse complement of the strand represented by the reference. This is called the reverse strand.\nNow, things start to get a little more interesting, because we don’t get to look at the the entire template as one contiguous piece of DNA in the 5’ to 3’ direction. Rather, we get to “see” one end of it by reading Read 1 in the 5’ to 3’ direction, and then we “see” the other end of it by reading Read 2, also in the 5’ to 3’ direction, but Read 2 is read off the complementary strand.\nSo, if we take the template from the top situation:\nthe original template is:\n               5'  ACCTGCAGGA------------------------------AACACAGTGA  3'\n               \nSo the resulting reads are:\n\nRead 1:        5'  ACCTGCAGGA  3'   --&gt; from 5' to 3' on the template\nRead 2:        5'  TCACTGTGTT  3'   --&gt; the reverse complement of the\n                                        read on the 3' end of the template\nAnd if we take the template from the bottom scenario:\nthe original template is:\n               3'  TGGACGTCCT------------------------------TTGTGTCACT  5'\n               \nSo the resulting reads are:\n\nRead 1:        5'  TCACTGTGTT  3'   --&gt; from 5' to 3' on the template\nRead 2:        5'  ACCTGCAGGA  3'   --&gt; the reverse complement of the\n                                        read on the 3' end of the template\nAha! Regardless of which strand of DNA the original template comes from, sequences must be read off of it in a 5’ to 3’ direction (as that is how the biochemistry works). So, there are only two possible sequences you will see, and these correspond to reads from 5’ to 3’ off of each strand. So, the only difference that happens when the template is from the forward or the reverse strand (relative to the reference), is whether Read 1 is from the forward strand and Read 2 is from the reverse strand, or whether Read 1 is from the reverse strand and Read 2 is from the forward strand. The actual pair of sequences you will end up seeing is still the same.\nSo, to repeat, with a segment of DNA that is a faithful copy of the reference genome, there are only two read sequences that you might see, and as we will show below Read 1 and Read 2 must align to opposite strands of the reference.\nWhat does a faithful segment from the reference genome look like in alignment? Well, in the top case we have:\n      Read 1:  5'  ACCTGCAGGA  3' \n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\nforward-strand\n    ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'  \n                                              Read 2:  3'  TTGTGTCACT  5'\nAnd in the bottom case we have:\n      Read 2:  5'  ACCTGCAGGA  3' \n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'  \nforward-strand\n    ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'\n                                              Read 1:  3'  TTGTGTCACT  5'\nNote that, although one of the reads always aligns to the reverse strand, the position at which it is deemed to align is still read off of the position on the forward strand. (Thank goodness for that! Just think how atrocious it would be if we counted positions of fragments mapping to the reverse strand by starting from the reverse strands’s 5’ end, on the other end of the chromosome, and counting forward from there!!)\nNote that the alignment position is one of the most important pieces of information about an alignment. It gets recorded in the POS column of an alignment. It is recorded as the first position (counting from 1 on the 5’ end of the forward reference strand) at which the alignment starts. Of course, the name of the reference sequence the read maps to is essential. In a SAM file this is called the RNAME or reference name.\nBoth of the last two alignments illustrated above involve paired end reads that align “properly,” because one read in the pair aligns to the forward strand and one read aligns to the reverse strand of the reference genome. As we saw above, that is just what we would expect if the template we were sequencing is a faithful copy (apart from a few SNPs or indels) of either the forward or the reverse strand of the reference sequence. In alignment parlance we say that each of the reads is “mapped in a proper pair.” This is obviously an important piece of information about an alignment and it is recorded in a SAM file in the so-called FLAG column for each alignment. (More on these flags later…)\nHow can a read pair not be properly mapped? There are a few possibilities:\n\nOne read of the pair gets aligned, but the other does not. For example something that in our schematic would look like this:\n\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGA  3'\n\nBoth reads of the pair map to the same strand. If our paired end reads looked like:\n\nRead 1:   5'  ACCTGCAGGA  3'\nRead 2:   5'  AACACAGTGA  5'\nthen they would both align nicely to just the forward strand of the reference genome:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGA------------------------------AACACAGTGA  3'\nAnd, as we saw above, this would indicate that the template must not conform to the reference genome in some way. This may occur if there is a rearrangement (like an inversion, etc.) in the genome being sequenced, relative to the reference genome.\n\nThe two different reads of a pair get aligned to different chromosomes/scaffolds or they get aligned so far apart on the same chromosome/scaffold that the alignment program determines the pair to be aberrant. This evaluation requires that the program have a lot of other paired end reads from which to estimate the distribution, in the sequencing library, of the template length—the length of the original template. The template length for each read pair is calculated from the mapping positions of the two reads and is stored in the TLEN column of the SAM file.\n\nWhat is another way I might align to thee? Well, one possbility is that a read pair might align to many different places in the genome (this can happen if the reads are from a repetitive element in the genome, for example). In such cases, there is typically a “best” or “most likely” alignment, which is called the primary alignment. The SAM file output might record other “less good” alignments, which are called secondary alignments and whose status as such is recorded in the FLAG column. The aligner bwa mem has an option to allow you to output all secondary alignments. Since you don’t typically output and inspect all secondary alignments (something that would be an unbearable task), most aligners provide some measure of confidence about the alignment of a read pair. The aligner, bwa, for example, looks at all possible alignments and computes a score for each. Then it evaluates confidence in the primary alignment by comparing its score to the sum of the scores of all the alignments. This provides the mapping quality score found in the MAPQ column of a SAM file. It can be interpreted, roughly, as the probability that the given alignment of the read pair is incorrect. These can be small probabilities, and are represented as Phred scaled values (using integers, not characters!) in the SAM file.\nThe last way that a read might align to a reference is by not perfectly matching every base pair in the reference. Perhaps only the first part of the read matches base pairs in the reference, or maybe the read contains an insertion or a deletion. For example, if instead of appearing like 5'  ACCTGCAGGA  3', one of our reads had an insertion of AGA, giving: 5'  ACCAGAGTGCAGGA  3', this fragment would still align to the reference, at 10 bp, and we might record that alignment, but would still want a compact way of denoting the position and length of the insertion—a task handled by the CIGAR column.\nTo express all these different ways in which an alignment can occur, each read occupies a single line in a SAM file. This row holds information about the read’s alignment to the reference genome in a number of TAB-delimited columns. There are 11 required columns in each alignment row, after which different aligners may provide additional columns of information. Table 9.1 gives a brief description of the 11 required columns (intimations of most of which occurred in ALL CAPS BOLDFACE in the preceding paragraphs. Some, like POS are relatively self-explanatory. Others, like FLAG and CIGAR benefit from further explanation as given in the subsections below.\n\n\n\n\nTable 9.1: Brief description of the 11 required columns in a SAM file.\n\n\n\n\n\n\n\n\nColumn\nField\nData Type\nDescription\n\n\n\n\n1\nQNAME\nString\nName/ID of the read (from FASTQ file)\n\n\n2\nFLAG\nInteger\nThe SAM flag\n\n\n3\nRNAME\nString\nName of scaffold/chromosome the read aligns to\n\n\n4\nPOS\nInteger\n1-based 5’-most alignment position on reference forward strand\n\n\n5\nMAPQ\nInteger\nPhred-scaled mapping quality score\n\n\n6\nCIGAR\nString\nString indicating matches, indels and clipping in alignment\n\n\n7\nRNEXT\nString\nScaffold/chromosome that the read’s mate aligns to\n\n\n8\nPNEXT\nInteger\nAlignment position of the read’s mate\n\n\n9\nTLEN\nInteger\nLength of DNA template whose ends were read (in paired-end sequencing)\n\n\n10\nSEQ\nString\nThe sequence of the read, represented in 5’ to 3’ on the reference forward strand\n\n\n11\nQUAL\nString\nBase quality scores, ordered from 5’ to 3’ on the reference forward strand\n\n\n\n\n\n\n\n\n9.4.2 Play with simple alignments\nTeam up with someone who has a Mac. They can clone the RStudio project repository on GitHub at https://github.com/eriqande/alignment-play (by opening a new RStudio project with the “From Version Control” –&gt; GitHub option, for example).\nThis has a notebook that will let us do simple alignments and familiarize ourselves with the output in SAM format. This is only available on Mac because bwa and samtools are not available on conda compiled for windows (and because I figured there wouldn’t be many folks running Linux on their laptops).\nRead through the R notebook and run the code. See how the SAM flags and or the CIGAR strings change when you make read1 and read2 different sequences, as shown in the notebook.\n\n\n9.4.3 SAM Flags\nThe FLAG column expresses the status of the alignment associated with a given read (and its mate in paired-end sequencing) in terms of a combination of 12 yes-or-no statements. The combination of all of these “yesses” and “nos” for a given aligned read is called its SAM flag. The yes-or-no status of any single one of the twelve statements is called a “bit” because it can be thought of as a single binary digit whose value can be 0 (No/False) or 1 (Yes/True). Sometimes a picture can be helpful: we can represent each statement as a circle which is shaded if it is true and open if it is false. Thus, if all 12 statements are false you would see \\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\). However, if statements 1, 2, 5, and 7 are true then you would see \\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\bullet}{\\circ}{\\bullet}~{\\circ}{\\circ}{\\bullet}{\\bullet}\\). In computer parlance we would say that bits 1, 3, 5, and 7 are “set” if they indicate Yes/True. As these are bits in a binary number, each bit is associated with a power of 2 as shown in Table 9.2, which also lists the meaning of each bit.\n\n\n\n\nTable 9.2: SAM flag bits in a nutshell. The description of these in the SAM specification is more general, but if we restrict ourselves to paired-end Illumina data, each bit can be interpreted by the meanings shown here. The “bit-grams” show a visual representation of each bit with open circles meaning 0 or False and filled circles denoting 1 or True. The bit grams are broken into three groups of four, which show the values that correspond to different place-columns in the hexadecimal representation of the bit masks.\n\n\n\n\n\n\n\n\n\n\nbit-#\nbit-gram\n\\(2^x\\)\ndec\nhex\nMeaning\n\n\n\n\n1\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\bullet}\\)\n\\(2^0\\)\n1\n0x1\nthe read is paired (i.e. comes from paired-end sequencing.)\n\n\n2\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\bullet}{\\circ}\\)\n\\(2^1\\)\n2\n0x2\nthe read is mapped in a proper pair\n\n\n3\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\bullet}{\\circ}{\\circ}\\)\n\\(2^2\\)\n4\n0x4\nthe read is not mapped/aligned\n\n\n4\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\bullet}{\\circ}{\\circ}{\\circ}\\)\n\\(2^3\\)\n8\n0x8\nthe read’s mate is not mapped/aligned\n\n\n5\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\bullet}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^4\\)\n16\n0x10\nthe read maps to the reverse strand\n\n\n6\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\bullet}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^5\\)\n32\n0x20\nthe read’s mate maps to the reverse strand\n\n\n7\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\bullet}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^6\\)\n64\n0x40\nthe read is read 1\n\n\n8\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\bullet}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^7\\)\n128\n0x80\nthe read is read 2\n\n\n9\n\\({\\circ}{\\circ}{\\circ}{\\bullet}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^8\\)\n256\n0x100\nthe alignment is not primary (don’t use it!)\n\n\n10\n\\({\\circ}{\\circ}{\\bullet}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^9\\)\n512\n0x200\nthe read did not pass platform quality checks\n\n\n11\n\\({\\circ}{\\bullet}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^{10}\\)\n1024\n0x400\nthe read is a PCR (or optical) duplicate\n\n\n12\n\\({\\bullet}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^{11}\\)\n2048\n0x800\nthe alignment is part of a chimeric alignment\n\n\n\n\n\n\nIf we think of the 12 bits as coming in three groups of four we can easily represent them as hexadecimal numbers. Hexadecimal numbers are numbers in base-16. They are expressed with a leading “0x” but otherwise behave like decimal numbers, except that instead of a 1’s place, 10’s place, and 100’s place, and so on, we have a 1’s place, a 16’s place, and 256’s place, and so forth. In the first group of four bits (reading from right to left) the bits correspond to 0x1, 0x2, 0x4, and 0x8, in hexadecimal. The next set of four bits correspond to 0x10, 0x20, 0x40, and 0x80, and the last set of four bits correspond to 0x100, 0x200, 0x400, 0x800. It can be worthwhile becoming comfortable with these hexadecimal names of each bit.\nIn the SAM format, the FLAG field records the decimal (integer) equivalent of the binary number that represents the yes-or-no answers to the 12 different statements. It is relatively easy to do arithmetic with the hexadecimal flags to find the decimal equivalent: add up the numbers in each of the three hexadecimal value places (the 1’s, 16’s, and 256’s places) and multiply the result by 16 raised to the number of zeros right of the “x” in the hexadecimal number. For example if the bits set on an alignment are 0x1 & 0x2 & 0x10 & 0x40, then they sum column-wise to 0x3, and 0x50, so the value listed in the FLAG field of a SAM file would be \\(3 + 5 \\cdot 16 = 83\\).\nWhile it is probably possible to get good at computing these 12-bit combinations from hexadecimal in your head, it is also quite convenient to use the Broad Institute’s wonderful SAM flag calculator.\nWe will leave our discussion of the various SAM flag values by noting that the large SAM-flag bits (0x100, 0x200, 0x400, and 0x800) all signify something “not good” about the alignment. The same goes for 0x4 and 0x8. On the other hand, when you are dealing with paired-end data, one of the reads has to be read 1 and the other read 2, and that is known from their read names and the FASTQ file that they are in. So, we expect that 0x40 and 0x80 should always be set, trivially. With paired-end data, we are always comforted to see bits 0x1 and 0x2 set, as departures from that condition indicate that the pairing of the read alignments does not make sense given the sequence in the reference genome. As we saw in our discussion of how a template can properly map to a reference, you should be able to convince yourself that, in a properly mapped alignment, exactly one of the two bits 0x10 and 0x20 should be set for one read in the pair, and the other should be set for the other. Therefore, in good, happy, properly paired reads, from a typical whole genome sequencing library preparation, we should find either:\nread 1 : 0x1 & 0x2 & 0x10 & 0x40 = 83\nread 2 : 0x1 & 0x2 & 0x20 & 0x80 = 163\nor\nread 1 : 0x1 & 0x2 & 0x20 & 0x40 = 99\nread 2 : 0x1 & 0x2 & 0x10 & 0x80 = 147\nSo, now that we know all about SAM flags and the values that they take, what should we do with them? First, investigating the distribution of SAM flags is an important way of assessing the nature and reliability of the alignments you have made (this is what samtools flagstat is for, as discussed in a later chapter). Second, you might wonder if you should do some sort of filtering of your alignments before you do variant calling. With most modern variant callers, the answer to that is, “No.” Modern variant callers take account of the information in the SAM flags to weight information from different alignments, so, leaving bad alignments in your SAM file should not have a large effect on the final results. Furthermore, filtering out your data might make it hard to follow up on interesting patterns in your data, for example, the occurrence of improperly aligning reads can be used to infer the presence of inversions. If all those improperly paired reads had been discarded, they could not be used in such an endeavor.\nNonetheless, you will want to mark (rather than remove) some aberrations, like PCR duplicates. We discuss that in a later section.\n\n\n9.4.4 The CIGAR string\nCIGAR is an acronym for Compressed Idiosyncratic Gapped Alignment Report. It provides a space-economical way of describing the manner in which a single read aligns to a reference genome. It is particularly important for recording the presence of insertions or deletions within the read, relative to the reference genome. This is done by counting up, along the alignment, the number of base pairs that: match (M) the reference; that are inserted (I) into the read and absent from the reference; and that are deleted (D) from the read, but present in the reference. To arrive at the syntax of the CIGAR string you catenate a series of Number-Letter pairs that describe the sequence of matches, insertions and deletions that describe an alignment.\nSome examples are in order. We return to our 80 base-pair reference from above and consider the alignment to it of a 10 bp read that looks like 5'  ACCTGCAGGA  3':\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGA  3'\nSuch an alignment has no insertions or deletions, or other weird things, going on. So its CIGAR string would be 10M, signifying 10 matching base pairs. A very important thing to note about this is that the M refers to bases that match in position in the alignment even though they might not match the specific nucleotide types. For example, even if bases 3 and 5 in the read don’t match the exact base nucleotides in the alignment, like this:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACTTACAGGA  3'\nits CIGAR string will typically still be 10M. (The SAM format allows for an X to denote mismatches in the base nucleotides between a reference and a read, but I have never seen it used in practice.)\nNow, on the other hand, if our read carried a deletion of bases 3 and 4. It would look like 5'  ACGCAGGA  3' and we might represent it in an alignment like:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  AC--GCAGGA  3'\nwhere the -’s have replaced the two deleted bases. The CIGAR string for this alignment would be 2M2D6M.\nContinuing to add onto this example, suppose that not only have bases 3 and 4 been deleted, but also a four-base insertion of ACGT occurs in the read between positions 8 and 9 (of the original read). That would appear like:\n5'  ACATAGACAGGGACCACCTGCAG----GACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  AC--GCAGACGTGA  3'\nwhere -’s have been added to the reference at the position of the insertion in the read. The CIGAR string for this arrangement would be 2M2D4M4I2M which can be hard to parse, visually, if your eyes are getting as old as mine, but it translates to:\n2 bp Match  \n2 bp Deletion  \n4 bp Match  \n4 bp Insert   \n2 bp Match   \nIn addition to M, D and I (and X) there are also S and H, which are typically seen with longer sequences. They refer to soft- and hard-clipping, respectively, which are situations in which a terminal piece of the read, from either near the 3’ or 5’ end, does not align to the reference, but the central part, or the other end of the read does. Hard clipping removes the clipped sequence from the read as represented in the SEQ column, while soft clipping does not remove the clipped sequence from the SEQ column representation.\nOne important thing to understand about CIGAR strings is that they always represent the alignment as it appears in the 5’ to 3’ direction. As a consequence, it is the same whether you are reading it off the read in the 5’ to 3’ direction or if you are reading it off from how the reverse complement of the read would align to the opposite strand of the reference. Another picture is in order: if we saw a situation like the following, with a deletion in Read 1 (which aligns to the reverse strand), the CIGAR string would be, from 5’ to 3’ on Read 1, 6M2D2M, which is just what we would have if we were to align the reverse complement of Read 1, called Comp R1 below to the forward strand of the reference.\n      Read 2:  5'  ACCTGCAGGA  3'            Comp R1:  5'  AA--CAGTGA  3'\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'  \nforward-strand\n    ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'\n                                              Read 1:  3'  TT--GTCACT  5'\nFor the most part, it is important to have an understanding of CIGAR strings, though you will rarely end up parsing and using them yourself. That job is best left for the specialized tools that process SAM files and their compressed equivalents, BAM files. Nonetheless, it is worth pointing out that if you want to identify the nucleotide values (i.e. alleles) at different variant positions upon single reads, it is necessary to contend with CIGAR strings to do so. This is one of the things that gets taken care of (with some Perl code) in the R package microhaplot for extracting microhaplotypes from short read data (and then visualizing them).\n\n\n9.4.5 The SEQ and QUAL columns\nThese columns hold the actual reads and quality scores that came off the sequencing machine and were in the FASTQ files. (Note, if you thought that after aligning your reads, the SAM or BAM files would end up taking up less space then the ridiculously large, gzipped FASTQ files you just downloaded from the sequencing center, guess again! SAM files actually have all the information present in a FASTQ, along with extra information about the alignments.)\nThe only thing that is tricky about these columns is that, if the read aligns to the reverse strand of the reference genome, the entry in the SEQ column is the reverse complement of the read that actually appeared in the FASTQ file. Of course, when you read off the letters of DNA from a reverse complement, going left to right the way you read a book, the order in which you encounter the complement of each base from the original sequence is reversed from the way you would read the bases in the original sequence. Accordingly, the order of the base quality scores that appear in the QUAL column will be in reverse order is the mapping was to the reverse strand.\n\n\n9.4.6 SAM File Headers\nThis next section uses an example file that is in the repository at example-files/s001---1.sam. To get it, you might need to sync your fork of the repo and then pull down changes into your main branch.\nTo this point, we have talked almost exclusively about the rows in a SAM file which record the alignments of different reads. However, if you look at a SAM file (with cat or less, or in a text editor) the first thing you will see is the SAM file header: a series of lines that all start with the @ symbol followed by two capital letters. Like @SQ.\nTry this:\nless -S example-files/s001---1.sam\nKeep hitting the space bar until you get down to the alignments, then use the up-arrow key to go back up through the file and inspect all the header lines.\nThese file header lines can appear daunting at first, and, when merging or dividing SAM and BAM files can prove to the bane of your existence, so understanding both their purpose and structure is paramount to avoiding some pain down the road.\nIn a nutshell, the lines in the header provide information to programs (and people) that will be processing the data in a SAM (or BAM, see below) file. Some of the header lines give information about the reference genome that the reads were aligned to, others provide an overview of the various samples (and other information) included in the file, while still others give information about the program and data that produced the SAM file.\nIf you have ever looked at the SAM header for an alignment to a reference genome that is in thousands of pieces, you were likely overwhelmed by thousands of lines that started with @SQ. Each of these lines gives information about the names and lengths (and optionally, some other information) of sequences that were used as the reference for the alignment.\nEach header line begins with an @ followed by two capital letters, for example, @RG or @SQ. The two-letter code indicates what kind of header line it is. There are only five kinds of header lines:\n\n@HD: the “main” header line, which, if present, will be the first line of the file. It’s purpose is to reveal the version of the SAM format in use, and also information about how the alignments in the file are sorted. (You won’t find this line in results/sam/s001---1sam. It’s not there! But we will see it later after converting it to a BAM file and sorting it!)\n@SQ: typically the most abundant SAM header lines, each of these gives information about a sequence in the reference genome aligned to.\n@RG: these indicate information about read groups, which are collections of reads that, for the purposes of downstream analysis can all be assumed to be from the same individual and to have been treated the same way during the processes of library preparation and sequencing. We will discuss read groups more fully in the section on alignment.\n@PG: a line that tells about the program that was used to produced the SAM file.\n@CO: a line in which a comment can be placed.\n\nLook though the header for example-files/s001---1.sam and find the @RG and @PG lines.\nThose first three characters (i.e. @ followed to two uppercase letters) signify that a line is a header line, but, within each header line, how is information conveyed? In all cases (except for the comment lines using @CO), information within a header line is provided in TAB-delimited, colon-separated key-value pairs. This means that the type or meaning of each piece of information is tagged with a key, like ID, and its value follows that key after a colon. For example, the line\n@PG     ID:bwa  PN:bwa  VN:0.7.17-r1188 CL:bwa mem -R @RG\\tID:s001_T199967_Lib-1_HY75HDSX2_1_AAGACCGT+CAATCGAC\\tSM:T199967\\tPL:ILLUMINA\\tL\nB:Lib-1\\tPU:HY75HDSX2.1.AAGACCGT+CAATCGAC resources/genome.fasta results/trimmed/s001---1_R1.fq.gz results/trimmed/s001---1_R2.fq.gz\ntells us that the ID of the program that produced the SAM file was bwa, and its version number (VN) was 0.7.17-r1188. And it also shows the complete command line (CL) that was used to produce it.\nThe keys in the key-value pairs are always two uppercase letters. And different keys are allowed (and in some case required) in the context of each of the five different kinds of SAM header lines.\nThe most important keys for the different kinds of header lines are as follows:\n\nFor @HD\n\nVN: the version of the SAM specification in use. This is required.\nSO: the sort order of the file. The default value is unknown. More commonly, when your SAM/BAM file is prepared for variant calling, it will have been sorted in the order of the reference genome, which is denoted as SO:coordinate. For other purposes, it is important to sort the file in the order of read names, or SO:queryname. It is important to note that setting these values in a SAM file does not sort it. Rather, when you have asked a program to sort a SAM/BAM file, that program will make a note in the header about how it is sorted.\n\nFor @SQ\n\nSN is the key for the sequence name and is required.\n\nLN is the key for the length (in nucleotide bases) of the sequence, and is also required.\n\nFor @RG\n\nID is the only required key for @RG lines. HUGE NOTE: if multiple @RG lines occur in the file, each of their ID values must be different.\nSM is the key for the name of the sample from which the reads came from. This is the name used when recording genotypes of individuals when doing variant calling.\nLB denotes the particularl library in which the sample was prepared for sequencing.\nPU denotes the “Platform Unit,” of sequencing. Typically interpreted to mean the flow-cell and the lane upon which the sample was sequenced. We will talk much more about the contents of read-group header lines, and how to fill them.\nPL denotes the sequencing technology (PL is short for “platform”) used. If the reads are from an Illumina sequencer, the value would be ILLUMINA.\n\nFor @PG\n\nID is required and provides information about the program that produced the SAM output.\nVN as we saw before, this key lets you record the version of the program used to produce the SAM output.\n\n\nA complete accounting of the different possible SAM header lines and their contents is given in the SAM specification. It is given in a terse table that is quite informative and is not terribly tough sledding. It is recommended that you read the section on header lines.\n\n\n9.4.7 The BAM format\nAs you might have inferred from the foregoing, SAM files can end up being enormous text files. For our example file, try this: du -h example-files/s001---1.sam to see how big it is:\nThe two big problems with having such large files are:\n\nThey could take up a lot of hard drive space.\nIt would take you (or some program that was processing a SAM file) a lot of time to “scroll” through the file to find any particular alignment you (it) might be interested in.\n\nThe originators of the SAM format dealt with this by also specifying and creating a compressed binary (meaning “not composed of text”) format to store all the information in a SAM file. This is called a BAM (Binary Alignment Map) file. In a BAM file, each column of information is stored in its native data type (i.e., the way a computer would represent it internally if it were working on it) and then the file holding all of these “rows” is compressed into a series of small blocks in such a way that the file can be indexed, allowing rapid access (without “scrolling” through the whole file) to the alignments within a desired genomic range. As we will see in a later chapter, in order to index such a file for rapid access to alignments in a particular genomic range, the alignments must be sorted in the order of genomic coordinates in the reference sequence.\nSince BAM files are smaller than SAM files, and access into them is faster than for SAM files, you will almost always convert your SAM files to BAM files to prepare for futher bioinformatic processing. The main tool used for this purpose is the program samtools (written by the creators of the SAM and BAM formats) for that purpose. We will encounter samtools in a later chapter.\nFinally, humans cannot directly read BAM files, or even decompress them with standard Unix tools. If you want to view a BAM file, you can use samtools view to read it in SAM format.\n\n\n9.4.8 Quick self study\n\nSuppose a read is Read 2 from the FASTQ, it aligns to the reverse strand, and its mate does too. The alignment is a primary alignment, but it has been flagged as a PCR duplicate. Write down, in hexadecimal form, all the bits that will be set for this read’s alignment, then combine them to compute the SAM FLAG for it.\nGiven the alignments shown below, what do you think the CIGAR strings for Read 1 and Read 2 might look like?\n\n      Read 2:  5'  ACCT--AGGAGGACACACAC   3' \n5'  ACATAGACAGGGACCACCTGCAGGA---CACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'  \nforward-strand\n    |||||||||||||||||||||||||---|||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCT---GTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'\n                                      Read 1:  3' AAAAAAAAAAAATTGTGTC-CT  5'"
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#variants",
    "href": "nmfs-bioinf/bioinf-formats.html#variants",
    "title": "9  Bioinformatic file formats",
    "section": "9.5 Variants",
    "text": "9.5 Variants\nMost conservation geneticists who started working in the field before the genomic revolution will recognize variant data as being most similar to what they have traditionally thought of as “genotype data” or, simply, “the data.” As the name implies, variant data consist of data only at those places in the genome where there is variation within a population or a species. This variation is the informative portion of the data for many analyses. In other words, you needn’t always carry with you information about every single base in the genome, because at most positions in the genome, every individual you are studying will carry the same base, and such fixed sites are not informative for many genetic analyses.\nOn the one hand, you can think of sites with no variation as being a little like monomorphic microsatellite loci (if that analogy is helpful for our older, more experienced, readers, who have been doing conservation genetics for a while…) Typically, such monomorphic loci were dropped for many analyses. On the other hand, when you are sampling genetic variation from an entire genome, it is also very useful to be able to keep track of how many sites in the genome are variable versus not variable. Accordingly,with genomic variant data it is important to keep track of how many variants exists within a given portion of the genome, so, we must keep track of where, within our reference genome, the variant sites are.\nFinally, decades ago, many of the different types of genetic variant data were a reflection of the methodology used to assay them. So, for example, restriction-fragment length polymorphisms and amplified fragment length polymorphisms were somewhat crude summaries that were responsive to differences in the underlying sequences of bases, but were only telling us about sequence at a very small (like 6 bp) section of the genome). Meanwhile minisatellites and microsatellites were expressed in terms of the number of repeated short motifs in a location of the genome, without telling us what the sequence there actually was. Today, however, with the resolution provided by genome sequencing, many such polymorphisms that were defined by gross or summarized features of the sequence can now be categorized and described in terms of the actual genome sequence carried by the two copies of a chromosome in a diploid individual. The variants in this context are define according to how those sequences differ from the corresponding sequence in the reference genome. The latter point bears repeating: currently, variant data are defined in terms of differences between the sequence carried by an individual and the genome sequence as it is listed in the reference genome.\n(We note that there is exciting work on developing a more “de-centralized” notion of variation data: one in which variants are not defined as departures from a “canonical” reference genome, but rather one in which all the known variation is recorded in a single “genomic variation graph.” This has exciting possibilities for reducing biases that might arise from the use of a single reference genome; however its use in conservation genetics is still likely a few (or more) years distant.)\nThe resolution of genetic data now allows us to define many different kinds of polymorphisms in terms of the sequence data itself. At the same time, just as the instruments used to gather sequence data can attach a variety of additional information (like base-quality and mapping scores) to sequence data, the methods used to infer the variants carried by an individual also may provide an abundance of additional information that should remain attached to the genotypes of individuals. Further, as is a constant theme in genomics, the scale of variant data one can obtain is huge and we will often want to access information about only certain defined subsets of the variants carried by any particular individual. And finally, as more data gets collected, we will want to be able to combine data from different sources and studies and use the combined data in a unified fashion.\nFor all the above reasons, the genomics community has recognized the need for a flexible and extensible format for storing variant data. Developing such a format and helping it to evolve as new types of genomic data allow new types of variants to be recorded, has been a major focus, driven largely by research in human genetics. For the most part, this community has converged on a format known as Variant Call Format or VCF. This format makes it possible to do all of the following:\n\nRecord variants such as SNPs, and short insertions and deletions, in a single framework, in terms of the sequences carried by individuals. (There is also now support for reording large structural variations and copy number variations; however, those have not, as yet, become central in conservation genetics).\nAttach to each particular locus or variant an arbitrary quantity of extra information, which might include confidence/quality scores or annotation information (which genes is the variant in), etc.\nAttach to each particular inferred genotype of an individual a variety of information such as confidence scores, read depth, haplotypic phase, etc.\nCompress all of the data in an efficient fashion and still allow the data to be processed (by some utilities) without uncompressing the data.\nIndex the data so that fast access to information about specific genomic regions can be extracted quickly.\n\nThe flexibility and extensibility of the VCF format comes with a small cost to conservation genomicists in that the format might look like nothing you have ever seen before! On top of that, if you want to work with VCF files, you really ought to learn to use specialized tools that have been developed for handling VCF files. This turns out to be essential in conservation genomics for the simple reason that even if you don’t end up doing most of your own bioinformatics (i.e. you leave the alignment and variant calling to a genotyping/bioinformatics service (or to a graduate student or postdoc)), the chances are good that whomever does that for you will (or, at least, should) send results to you in a VCF file. So, while you might be able to do work in conservation genomics without ever having to crack open a BAM file, or parse the lines of a FASTQ file, you will almost invariably have to bang your head against a VCF file at some point.\nSo, what I am saying, is don’t be a complete noob like I was in 2011 when I received my first data set in VCF format. I didn’t have any idea what it was, and after wracking my brain over it finally wrote back to the genotyping service that had sent it to me:\nI have looked over the file and everything makes sense save for a few \nthings.   I was hoping you could answer just a few (should be easy) \nquestions.  You went over this when we videoconferenced, but just a \nfew things are hazy. Sorry.\n\nQUAL column:  How is this scaled? There are 13990 distinct values \nranging from 6.2 to 250.64.  Is it just sort of relative?  What is \n\"good\" quality.\n\nThe INFO column:  what does this mean: \"NS=22:AN=1:DP=541\"\n\nIn other words what are NS, AN, and DP?\n\nThe FORMAT column.  Looking at the file it appears that \n\"GT:DP:GQ:EC:SG\" means that the first field is a genotype (0,1, or \n0/1, for homozygous reference, homozygous non-reference, and \nheterozygous).  And SG must be the IUPAC code for the individual's \ngenotype, but what are DP, GQ, and EC?  If I had to hazard a guess I \nwould say:\nDP = # of reads with reference base\nGQ = # of reads total whether or not they overlapped the position of \nthe base in question\nEC = # of reads with alternate base\nHah! I just put that out there to remind myself that VCF files will seem super mysterious to everyone when they first encounter them. My goal in the next section is to help you be better informed than I was when I saw my first VCF file. So, with that in mind, let’s jump into learning about the format…\n\n9.5.1 VCF Format – The Body\nVCF files, just like SAM files, have a header section and a main “body” section. We will start by describing the body section, in which each row holds information about a variant and the genotypes carried by individuals in a sample. We will discuss the header section after that.\nTo download a VCF file that you might view in a text editor you can use this link:\n\nhttps://www.dropbox.com/s/dzwixabvy3wwfdr/chinook-32-3Mb.vcf.gz?dl=1\n\nIf you want to, you can decompress that file and open it with an able text editor (like TextWrangler or BBedit on a Mac or Notepad++ on Windows). Don’t open it with MS Word! Also, on you laptop, don’t try to just double click it! The .vcf extension is often interpreted as a “Virtual Contact File”, meaning that your computer will try to open it—and possibly try to import it—using whatever program you handle your contacts’ phone numbers and addresses with.\nTo get download the “index” for that file, use this:\n\nhttps://www.dropbox.com/s/d303fi7en71p8ug/chinook-32-3Mb.vcf.gz.csi?dl=1\n\nIf you want to download these onto a remote Unix server you can use:\nwget https://www.dropbox.com/s/dzwixabvy3wwfdr/chinook-32-3Mb.vcf.gz?dl=1\nwget https://www.dropbox.com/s/d303fi7en71p8ug/chinook-32-3Mb.vcf.gz.csi?dl=1\n\n# after which you might need to rename them, if wget retained the `?dl=1`\n# in the resulting file names:\nmv chinook-32-3Mb.vcf.gz\\?dl\\=1 chinook-32-3Mb.vcf.gz\nmv chinook-32-3Mb.vcf.gz.csi\\?dl\\=1 chinook-32-3Mb.vcf.gz.csi\nAt their core, VCF files are TAB-delimited text files. Below is a fragment of a VCF file body (and the final header line) holding information about 9 variants (in 9 rows below the final header line) and the genotypes at those variants carried in four individuals (the four right-most columns).\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  DPCh_plate1_A05_S5  DPCh_plate1_A06_S6  DPCh_plate1_A11_S11 DPCh_plate1_A12_S12\nNC_037124.1 4001417 .   T   G   1528.4  .   AC=14;AF=0.245;AN=44;BaseQRankSum=2.296;ClippingRankSum=-1.168;DP=206;FS=0;MLEAC=51;MLEAF=0.236;MQ=60;MQRankSum=1.501;QD=23.88;ReadPosRankSum=1.685;SOR=0.608   GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  ./.:.:.:.:.\nNC_037124.1 4001912 .   A   G   4886.98 .   AC=29;AF=0.652;AN=48;BaseQRankSum=4.315;ClippingRankSum=-0.984;DP=220;FS=0.753;MLEAC=139;MLEAF=0.662;MQ=59.92;MQRankSum=0.311;QD=29.62;ReadPosRankSum=-1.071;SOR=0.737  GT:AD:DP:GQ:PL  0/0:1,0:1:3:0,3,46  ./.:.:.:.:. 0/1:3,1:4:33:33,0,120   1/1:0,2:2:6:87,6,0\nNC_037124.1 4004574 .   AAAGG   A   957.91  .   AC=12;AF=0.176;AN=48;BaseQRankSum=1.052;ClippingRankSum=0.887;DP=201;FS=0;MLEAC=28;MLEAF=0.149;MQ=60;MQRankSum=0.73;QD=26.61;ReadPosRankSum=-1.135;SOR=0.591    GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:2,0:2:6:0,6,119 0/0:2,0:2:6:0,6,109\nNC_037124.1 4006558 .   T   TG  3442.71 .   AC=24;AF=0.56;AN=28;BaseQRankSum=-1.068;ClippingRankSum=-1.509;DP=191;FS=5.394;MLEAC=102;MLEAF=0.554;MQ=60;MQRankSum=-0.621;QD=29.42;ReadPosRankSum=0.552;SOR=0.464 GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,2:2:6:75,6,0  1/1:0,2:2:6:82,6,0  ./.:.:.:.:.\nNC_037124.1 4006887 .   G   T   109.29  .   AC=2;AF=0.048;AN=40;BaseQRankSum=-0.411;ClippingRankSum=-0.145;DP=175;FS=0;MLEAC=6;MLEAF=0.032;MQ=60;MQRankSum=-0.718;QD=15.61;ReadPosRankSum=-0.502;SOR=1.323  GT:AD:DP:GQ:PL  0/0:2,0:2:6:0,6,90  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  0/0:1,0:1:3:0,3,42\nNC_037124.1 4007343 .   T   TGGAAGGAAGGAAACG    7696.07 .   AC=42;AF=0.985;AN=42;BaseQRankSum=2.278;ClippingRankSum=1.604;DP=213;FS=0;MLEAC=192;MLEAF=0.99;MQ=60;MQRankSum=0.139;QD=31.22;ReadPosRankSum=0.973;SOR=0.7  GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,1:1:3:34,3,0  ./.:.:.:.:. 1/1:0,1:1:9:135,9,0\nNC_037124.1 4007526 .   C   T   1040.07 .   AC=13;AF=0.17;AN=54;BaseQRankSum=1.507;ClippingRankSum=0.333;DP=227;FS=2.374;MLEAC=35;MLEAF=0.165;MQ=60;MQRankSum=2.562;QD=17.05;ReadPosRankSum=1.567;SOR=0.679 GT:AD:DP:GQ:PL  0/1:1,1:2:19:19,0,39    0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,49  0/0:2,0:2:6:0,6,90\nNC_037124.1 4007583 .   C   CAA 596.58  .   AC=3;AF=0.084;AN=54;BaseQRankSum=-0.197;ClippingRankSum=-0.445;DP=224;FS=0;MLEAC=19;MLEAF=0.089;MQ=60;MQRankSum=0.096;QD=20.57;ReadPosRankSum=-1.754;SOR=1.127  GT:AD:DP:GQ:PL  ./.:.:.:.:. 0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,90  0/0:1,0:1:3:0,3,45\nNC_037124.1 4007584 .   GCC G,CCC   3246.17 .   AC=3,25;AF=0.083,0.324;AN=54;BaseQRankSum=1.157;ClippingRankSum=-0.459;DP=227;FS=2.342;MLEAC=16,56;MLEAF=0.074,0.259;MQ=60;MQRankSum=0.412;QD=28.98;ReadPosRankSum=-0.656;SOR=0.527 GT:AD:DP:GQ:PL  ./.:.:.:.:. 2/2:0,0,1:1:3:37,39,45,3,3,0    2/2:0,0,2:2:6:82,84,90,6,6,0    2/2:0,0,1:1:3:45,45,45,3,3,0\nWow! That is really hard to look at. The column corresponding to the INFO field is quite long and obtrusive. Below we turn that column into a . in every row:\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  DPCh_plate1_A05_S5  DPCh_plate1_A06_S6  DPCh_plate1_A11_S11 DPCh_plate1_A12_S12\nNC_037124.1 4001417 .   T   G   1528.4  .   .   GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  ./.:.:.:.:.\nNC_037124.1 4001912 .   A   G   4886.98 .   .   GT:AD:DP:GQ:PL  0/0:1,0:1:3:0,3,46  ./.:.:.:.:. 0/1:3,1:4:33:33,0,120   1/1:0,2:2:6:87,6,0\nNC_037124.1 4004574 .   AAAGG   A   957.91  .   .   GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:2,0:2:6:0,6,119 0/0:2,0:2:6:0,6,109\nNC_037124.1 4006558 .   T   TG  3442.71 .   .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,2:2:6:75,6,0  1/1:0,2:2:6:82,6,0  ./.:.:.:.:.\nNC_037124.1 4006887 .   G   T   109.29  .   .   GT:AD:DP:GQ:PL  0/0:2,0:2:6:0,6,90  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  0/0:1,0:1:3:0,3,42\nNC_037124.1 4007343 .   T   TGGAAGGAAGGAAACG    7696.07 .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,1:1:3:34,3,0  ./.:.:.:.:. 1/1:0,1:1:9:135,9,0\nNC_037124.1 4007526 .   C   T   1040.07 .   .   GT:AD:DP:GQ:PL  0/1:1,1:2:19:19,0,39    0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,49  0/0:2,0:2:6:0,6,90\nNC_037124.1 4007583 .   C   CAA 596.58  .   .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,90  0/0:1,0:1:3:0,3,45\nNC_037124.1 4007584 .   GCC G,CCC   3246.17 .   .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 2/2:0,0,1:1:3:37,39,45,3,3,0    2/2:0,0,2:2:6:82,84,90,6,6,0    2/2:0,0,1:1:3:45,45,45,3,3,0\nPhew! That is easier on the eyes at this point. Looking at the above, notice that the first nine columns carry information that pertains to the variant itself (i.e., the SNP or the indel that is being described in this row). These nine columns are followed by four columns, each one pertaining to one of the four samples whose names are given in the top row: DPCh_plate1_A05_S5    DPCh_plate1_A06_S6  DPCh_plate1_A11_S11 DPCh_plate1_A12_S12.\nWe will start by describing the 9 standard variant-specific columns:\n\nCHROM: the name of the reference sequence upon which the variant was found. This comes from the FASTA file that was used as a reference to align the sequencing data to, orinally.\nPOS: The 1-based position within CHROM that the variant is found at. For SNPs, this is straightforward. For insertions and deletions it is also straightforward, but you must remember that the value in the POS columns give the first position in the entry in the REF column. More on that below…\nID: An identifier for a particular variant. These identifiers are used to indicate the name of the variant in different data bases, like the human dbSNP data base. For non-model organisms of conservation concern, there are typically no such data bases, so the values in this column will be “missing” which is always denoted by a single period, ., in a VCF file.\nREF the sequence of the reference—i.e. the “reference allele” at the variant. See below for more description about how this is designated.\nALT the sequence of the alternate allele(s) at the variant. If there is more than one alternate allele at the variant, the sequences for each are separated by a comma. (See more below).\nQUAL: a Phred-scaled assessment of whether the variant truly represents variation in the sample that was sequenced and genotyped, or whether it might be a spurious variant—for example, not a true polymorphism, but rather due to a sequencing error. If an alternate allele is given in ALT, then the QUAL is \\(-10\\log_{10} P(\\mathrm{there~truly~is~NOT~such~an~alternate~allele(s)})\\). A larger number means more confidence that the alternate allele is real. If there is no alternate allele given in ALT (i.e. the ALT field holds a .), then the value in QUAL is \\(-10\\log_{10} P(\\mathrm{there~actually~IS~a~variant~here})\\).\nFILTER: a semicolon-delimited list of filters that this variant tests positive for. This provides a way of creating different possible sets of variants that can be tagged in different ways, but not removed from the data set. Not used extensively in conservation genomics.\nINFO: a semicolon-delimited list of “key=value” pairs that describe properties of the variant. GATK attaches a large number of these. The meaning of different keys is given in the header (see below).\nFORMAT: this is a column that says, “In the remaining columns, information about the genotypes of different individuals will be given in the following fields separated by colons.” It is basically a key to deciphering the following columns of the genotype data of the individuals, in that row. Specifying the genotype column formats for each row, in this fashion, makes it possible to attach different types of information to different variants, as might be appopriate.\n\n\n9.5.1.1 Coding of REF and ALT\nWhen coding for insertions and deletions—or for more complex variants in which some of the insertions might carry SNPs, etc.—the convention is followed that the REF column always shows the sequence in the reference sequence starting at POS. For understanding how insertions and deletions are encoded in this system, it is helpful to view them in columnar format with positions running down the first column, and different alleles in different columns. Thus, a simple SNP, such as the first row in the above VCF example would appear as:\n# meaning of: NC_037124.1 4001417 .   T   G \n\npos         Ref  Alt\n4001417     T      G\nBy contrast, an insertion, relative to the reference genome, creates bases that don’t have a standard coordinate that they align to. Thus, if we take the second-to-last variant in the above example (at position 4007583), and we assume that positions 4007582 and 4007584 in the reference sequence are a G and a T, respectively, then the first five columns of the row indicate that the variation appears as follows:\n# meaning of: NC_037124.1 4007583 .   C   CAA\n\nPos         Ref  Alt\n4007582     G    G\n4007583     C    C\n.           -    A\n.           -    A\n4007584     T    T\nwhere the - represents an insertion relative to the reference genome.\nA deletion relative to the reference genome, like that in the third row of the above example: would appear in columnar format as:\n# meaning of: NC_037124.1 4004574 .   AAAGG   A\n\nPos         Ref  Alt\n4004573     C    C\n4004574     A    A\n4004575     A    -\n4004576     A    -\n4004577     G    -\n4004578     G    -\n4004579     T    T\nwhere we are imagining that the reference holds a C at position 4004573 and a T at position 4004579, and where the -’s indicate deletions relative to the reference.\nFinally, the multiallelic and complex example on the last row, in columnar format, if the reference held an A at position 4007583 and a T at position 4007587, would show the two possible alternate alleles like:\n# meaning of: NC_037124.1 4007584 .   GCC G,CCC\n\nPos         Ref   Alt1  Alt2\n4007583     A     A     A\n4007584     G     G     C\n4007585     C     -     C\n4007586     C     -     C\n4007587     T     T     T\n\n\n9.5.1.2 The Genotype Columns\nEach of the columns after the 9th column correspond to genotype information at a single one of the samples. The FORMAT string for each in the example is: GT:AD:DP:GQ:PL. There could be more fields than these five (and, if so, information about those fields will be stored in the header section of the VCF file); however, these five are pretty standard, and will be described here. For a concrete example, we will focus in the following on the genotype column for the first sample on the third line from the bottom (POS = 4007526):\n0/1:1,1:2:19:19,0,39\nBroken into the five fields we would see:\n\nGT = 0/1 — The GT stands for GenoType. This field gives the “called” genotype of the individual as two alleles (0 and 1, here) separated by either a / or a |. The 0 always refers to the reference allele (REF), and a number greater than 0 refers to an alternate allele (from the ALT column). If there are only two alleles, (a reference allele and one alternate allele) then the alternate allele will always be denoted by a 1. If there are two alternate alleles, the two alternate alleles will be denoted as 1 and 2, respectively.\n\nA / means that the two gene copies are not phased—in other words, it is not known which haplotype they occur on.\nA | as the separator means that the two gene copies in the individual are recorded as being phased. If an indiduals GT field in a series of variants looks like:\n::: {.cell}\n0|1\n1|0\n1|0\n1|1\n0|0\n1|0\n:::\nThen this is recording that on one of the (diploid) individual’s two chromosomes, the alleles would be strung along like: 0-1-1-1-0-1, and the other chromosome would carry the alleles strung along like: 1-0-0-1-0-1.\n\n\nAD = 1,1 — The AD stands for Allele Depth. This field gives the comma-separated depths of reads from the individual that carried the different alleles at this position. In this case there are only two alleles (a REF and an ALT), so that there is one comma separating two values. The 1 before the comma says that we saw one read with the REF allele, and the 1 after the comma says that we saw one read that had the ALT allele. If there is more than one ALT allele, then there will be more commas, each one separating a different allele read depth (i.e., REF,ALT1,ALT2,etc). These values can sometimes be extracted and used in place of the actual genotypes for making inferences (for example, doing PCAs or computing Fst values using single read sampling).\nDP = 2 — The DP denotes DePth. This field gives the total number of reads from this individual mapped on top of this position. It is typically the sum of the allele depths, but not always, owing to some reads indicating an allele at the position that the variant caller might have deemed a sequening error, etc. In general, the greater the read depth, the more confidence you can have in the genotype calls. Though it might be worthwhile to focus on the read depth itself to identify variants in repetitive regions or copy-number variation.\nGQ = 19 — The GQ stands for Genotype Quality. This field gives a Phred scaled measure of confidence in the genotype call. This value is given conditional on there actually being a variant (in the population) at this position. Thus, it says, “If we set aside the question of whether or not the variant we see here is real, its genotype quality is given by \\(-10\\log_{10} P(\\mathrm{the~genotype~call~is~wrong})\\).” So, colloquially, the probability that the genotype recorded for the individual is incorrect is \\(10^{-GQ/10}\\). So, if GQ = 19, we have (ostensibly) the probability that the genotype call is incorrect equalling \\(10^{-19/10} = 0.0126\\). These probabilities must be taken with a grain of salt. Often, the raw genotype calls from low-ish coverage sequencing data can be somewhat more error prone.\n\nPL = 19,0,39 — PL stands for\n\n\n\n\n9.5.2 VCF Format – The Header\nLet’s just get in there and take a look at it.\n\n\n9.5.3 Boneyard\nVCF. I’ve mostly used vcftools until now, but I’ve gotta admit that the interface is awful with all the –recode BS. Also, it is viciously slow. So, let’s just skip it all together and learn how to use bcftools. One nice thing about bcftools is that it works a whole lot like samtools, syntactically.\nNote that for a lot of the commands you need to have an indexed vcf.gz."
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#segments",
    "href": "nmfs-bioinf/bioinf-formats.html#segments",
    "title": "9  Bioinformatic file formats",
    "section": "9.6 Segments",
    "text": "9.6 Segments\nBED"
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#conversionextractions-between-different-formats",
    "href": "nmfs-bioinf/bioinf-formats.html#conversionextractions-between-different-formats",
    "title": "9  Bioinformatic file formats",
    "section": "9.7 Conversion/Extractions between different formats",
    "text": "9.7 Conversion/Extractions between different formats\n\nvcflib’s vcf2fasta takes a phased VCF file and a fasta file and spits out sequence."
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#visualization-of-genomic-data",
    "href": "nmfs-bioinf/bioinf-formats.html#visualization-of-genomic-data",
    "title": "9  Bioinformatic file formats",
    "section": "9.8 Visualization of Genomic Data",
    "text": "9.8 Visualization of Genomic Data\nMany humans, by dint of our evolution, are exceptionally visual creatures. Being able to create visual maps for different concepts is also central facilitating understanding of those concepts and extending their utility to other realms. The same is true for bioinformatic data: being able to visualize bioinformatic data can help us understand it better and to see connections between parts of our data sets that we did not, before.\nThe text-based bioinformatic formats we have discussed so far do not, standing by themselves, offer a rich visual experience (have you ever watched a million lines of a SAM file traverse your terminal, and gotten much understanding from that?). However, sequence data, once it has been aligned to a reference genome has an “address” or “genomic coordinates” that, by analogy to street addresses and geographic coordinates suggests that aligned sequence data might be visualized like geographic data in beautiful and/or thought-provoking “maps.”\nThere are a handful of programs that do just that: they make compelling, interactive pictures of bioinformatic data. One of those programs (that I am partial to), called IGV (for Integative Genomics Viewer), was developed in the cross-platform language, Java, by researchers at the Broad Institute. It is available for free download from https://software.broadinstitute.org/software/igv/, and it should run on almost any operating system. It has been well-optimized to portray genomics data at various scales and to render an incredible amount of information in visual displays as well as text-based “tool-tip” reports that may be activated by mousing over different parts of the display.\nThere is extensive documentation that goes with IGV, but it is valuable (and more fun!) to just crack open some genomic data and start playing with it. To do so, there are just a few things that you need to know:\n\nThe placement of all data relies on the reference genome as a sort of “base map.” The reference genome serves the purpose of a latitude-longitude coordinate system that lets you make sense of spatial data on maps. Therefore, it is required for all forays with IGV. You must specify a reference genome by choosing one of the options from the “Genomes” menu. If you are working on a non-model organism of conservation concern it is likely that you will have the reference genome for that critter on your local computer, so you would “Genomes-&gt;Load Genome From File…”, to show IGV where the FASTA file (cannot be compressed) of your genome is on your hard drive. Let’s repeat that: if you are loading the FASTA for a reference genome into IGV, you must use the “Genomes” menu option. If you use “File-&gt;Load From File…” to try to load the reference genome, it won’t let you. Don’t do it! Use “Genomes-&gt;Load Genome From File”\nOnce your reference genome is known to IGV, you can add data from the bioinformatic formats described in this chapter that include positions from a reference genome. These include SAM or BAM files and VCF files (but note FASTQ files). To include data from these formats, choose “File-&gt;Load From File…”. (Note, BAMs are best sorted and indexed). The data from each file that you “Load” in this manner appears in a separate track of data in a horizontally tiled window that is keyed to the reference genome coordinates.\nYou can zoom in and out as appropriate (and in several different ways).\nRight clicking within any track gives a set of options appropriate for the type of track it is. For example, if you are viewing a BAM file, you can choose whether the reads joined together with their mates (“View as pairs”) or not, or whether the alignments should be viewed at “full scale” (“Expanded”), somewhat mashed down (“Collapsed”) or completely squashed down and small (“Squished”).\n\n\n9.8.1 Sample Data\nAbout 0.5 Gb of sample data can be downloaded from https://drive.google.com/file/d/1TMug-PjuL7FYrXRTpNikAgZ-ElrvVneH/view?usp=sharing.\nThis download includes a zip-compressed folder called tiny-genomic-data. Within that folder are two more folders:\n\nchinook-wgs-3-Mb-on-chr-32: FASTQs, BAMs, and VCFs from whole genome sequencing data along a 3 Mb span of Chinook salmon chromosome 32 (which in the reference is named NC_037124.1). The genomic region from which the data comes from is NC_037124.1:4,000,000-7,000,000. The BAM and VCF files can be viewed against the reference genome in IGV.\nmykiss-rad-and-wgs-3-Mb-chr-omy28 includes BAMs from RAD-seq data (sequenced in the study by [@princeEvolutionaryBasisPremature2017]) from multiple steelhead trout individuals merged together into two BAM files. The genomic region included in those BAM files is omy28:11,200,000-12,200,000. Also included is a VCF file from whole genome resequencing data from 125 steelhead and rainbow trout in the 3 Mb region from `omy28:10,150,000-13,150,000.\n\nBoth of the above directories include a genome directory that holds the FASTA that you must point IGV to. Note that in neither case does the FASTA hold the complete genome of the organism. I have merely included three chromosomes in each—the chromosome upon which the BAM and VCF data are located, and the chromosome on either side of that chromosome.\nExplore these data and have fun. Some things to play with (remember to right-click [cntrl-click on a Mac] each track for a menu) :\nStart with the Chinook data:\n\nLoad the FASTA for each data set first\nLoad a BAM file after that\nThen load a VCF file.\nYou will likely have to zoom pretty far into a genomic region with data (see above!) before you see anything interesting.\nTry zooming in as far as you can.\nToggle “View as Pairs” and see the result.\nPlay with “Collapsed/Expanded/Squished”\nExperiment with grouping/sorting/coloring alignments by different properties\nUse coloring to quickly find alignments with\n\nF1R2 of F2R1 orientation\nInsert size &gt; 1000 bp\n\nSort by mapping quality and find some reads with MAPQ &lt; 60\nZoom out and use the VCF to find a region with a high variant density, then zoom back in and view the alignments there? What do you notice about the number of reads aligning to those areas?\n\nFor the steelhead data additionally:\n\nDo you see where the RAD cutsite must have been and how paired-end sequencing works from either side of the cutsite?\nWhat do you notice about the orientation of Read 1 and Read 2 on either side of the cutsite?\nWhy do we see the read depth patterns we see on either side of the cutsite? (i.e., in many cases it goes up as you move away from the cutsite, and then drops off again.)\nDo you appreciate this visual representation of how sparse RAD data is compared to whole genome resequencing data?"
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#read-journey",
    "href": "nmfs-bioinf/sequence-alignment.html#read-journey",
    "title": "10  Sequence alignment and handling BAM files",
    "section": "10.1 The Journey of each DNA Fragment from Organism to Sequencing Read",
    "text": "10.1 The Journey of each DNA Fragment from Organism to Sequencing Read\nIn Section @ref(get-seqs) we discussed one solution for downloading gzipped FASTQ files from a sequencing center. This turns out to be the last step in a very long process of acquiring sequencing data from a collection of organisms that you are studying. We could say that the DNA of each organism that you are studying took an extraordinary journey to get to you. This journey included, 1) you (or a colleague) sampling tissue or blood from that organism, 2) DNA being extracted from that tissue or blood, 3) that DNA getting Illumina adapters (with sample-specific barcodes) attached to it and then getting PCR amplified some number of times in the library preparation stage, 4) the products of the library preparations are sequenced, sometimes on multiple flow cells and often across multiple lanes, during the sequencing stage, 5) the sequencer processes the read data into FASTQ files, 6) finally, you download the data.\nAnyone who has ever been involved in research knows that at every stage of the above process there are opportunities for errors or biases to occur that could affect the final data (and inferences) produced. The effects of some of the errors and biases that occur during the library prep stage and the sequencing stage can be eliminated or lessened. Doing so requires that we keep careful track of the route that each read took during those those steps in its journey. This is the role of read groups, which we discuss below. For now, however, we will offer that the main errors/effects that can be controlled for by careful accounting of reads into read groups are: 1) controlling for effects on the reported base quality scores of the particular flowcell or the particular lane on a flowcell that a read was sequenced on; 2) The identification of sequencing reads which are PCR duplicates of another read in the data set. Thus, most of the focus on defining read groups is concerned with identifying features of a read’s journey that can be used to ameliorate those two types of errors/biases.\nThe process of controlling for batch effects of flowcells and lanes on base quality scores is called, unsurprisingly, Base Quality Score recalibration. This might be discussed in a later section. It is a very important process when one is searching for de novo mutations, or otherwise attempting to very accurately identify variants that occur at very low frequency. Doing base quality score recalibration is considered a “best practice” when using GATK (Section XX), but I have my doubts as to whether it will make appreciable differences in analyses where the importance of rare variants is downweighted. Nonetheless, whether you are going to do base quality score recalibration or not, you should certainly prepare your data so that you can do it should you decide to. In order to do it, you must know which flowcell and upon which lane each read was sequenced. That information can be found in the FASTQ file as part of the name for each sequence, however, methods for doing base quality score recalibration do not typically use the names of the reads to access that information. Rather it must be stored in the read group information.\nA PCR duplicate is a template sequence that happens to be a copy (made by PCR) of another sequence that has also been sequenced. They are considered “errors” in bioinformatics, because many of the models for inferring genotypes from next generation sequencing data assume a process of independent sampling of gene copies (i.e. the maternal and paternal copies of a choromosome) within a diploid individual, and that model and the data are used to estimate how likely an individual is a homozygote versus a heterozygote at each position. For example, if Individual X produces one read covering position Y—a position known to be a variable SNP with alleles A and G in the species—and the read carries an A at position Y, then you would conclude that Individual X likely has genotype AA or AG at position Y, and is less likely to have genotype GG, because our observed read with an A at that position would have only occurred because of a sequencing error in that case. If we saw 10 reads from individual X, and they all had the A base at position Y, however, we could be quite confident that the genotype is AA rather than AG, because if the indvidual had genotype AG, then each read should have a 50% chance of carrying an A and a 50% chance of carrying a G, and it is very unlikely, in that case, that all 10 would carry the A allele. However, imagine how your conclusions would change if someone were to tell you that 95% of the reads were PCR duplicates of another one of the reads. In that case, even if the individual has genotype AG, there is high probability that only a single, true template carrying an A came from the individual, and the remaining 9 reads that carry an A are just PCR copies of that first original template.\nPCR duplicates are copies of the same template fragments. Therefore in paired-end data if a pair of reads maps to exactly the same location as another pair of reads, it is likely that one of them is a PCR duplicate of the other. Figure 10.1 provides a simplified schematic (with merely single-end data) describing a situation with high and low numbers of PCR duplicates. The PCR duplicate fragments are denoted with gray-colored flanking sequence. In the figure, note that the length of the fragments gives information about which fragments are duplicated and which are not—if two fragments of the same length are identified, then one of them is considered a PCR duplicate. It is worth noting that it isn’t (to my knowledge) really known which fragment is the duplicate and which is the “original,” but, in order to correct the situtation, one (or more of them) will be identified as duplicates, and one of them will be identified as an “original.”\n\n\n\n\n\nFigure 10.1: An example of PCR duplicates. Note that in paired end data, duplicates are identified by the lengths of both reads as well as the insert length. This figure represents a simplified, toy, situation, showing just single-end data.\n\n\n\n\nPCR duplicates occur during the library prep stage, and also are only possible for fragments originating from the same individual sample. In other words, if you found two different paired-end reads that mapped to the same location in the genome, but they were from different individuals (i.e. had different sample-specific DNA barcodes), you would not suspect that they were PCR duplicates. Likewise, DNA fragments from two different library preps, even if they came from the same individual—in the case that DNA from that individual had been prepared in two separate libraries—would not be considered to be PCR duplicates. (We should pause here to note what is considered a “library prep.” Basically, if a batch of samples were all processed together in the same day with reagents from the same kit, that is considered a single “library prep.”) Accordingly, when keeping track of read groups for the purposes of identifying PCR duplicates, the most important information is which individuals the DNA came from and which library preparation it was prepared in.\nWhat we can conclude from this section is that, for downstream analyses, we need a way to quickly (and efficiently, in terms of data storage space) identify, for each read the:\n\nSample it originated from\nThe library in which it was prepared for sequencing.\nThe flowcell that it was sequenced on.\nThe lane on that flowcell that it was sequenced on\n\nThis information can be strored in the alignment file using read group information as described below."
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#read-groups",
    "href": "nmfs-bioinf/sequence-alignment.html#read-groups",
    "title": "10  Sequence alignment and handling BAM files",
    "section": "10.2 Read Groups",
    "text": "10.2 Read Groups\nWe previously touched briefly on read groups, noting that the SAM file header can contain lines like this one:\n@RG ID:HTYYCBBXX.1.CH_plate1_A01    SM:CH_plate1_A01    LB:Lib-1    PU:HTYYCBBXX.1.TCAATCCG+TTCGCAGT    PL:ILLUMINA\nThe purpose of this line is to identify a read group that is named via the ID tag. In the above case the read group is named HTYYCBBXX.1.CH_plate1_A01. The attributes of this read group are stored using the remaining tags in the @RG line. Happily, those attributes mirror exactly what we might need to know to identify PCR duplicates and to perform base quality score recalibration; namely:\n\nSM : give the name of the sample, CH_plate1_A01, the read is from,\nLB : gives the name of the library prep, Lib-1, the read is from,\nPU : gives the name of the flowcell, HTYYCBBXX, and the lane, 1, the read is from.\n\nIt is important to note that this line in the SAM header is just a header line that gives information about a particular read group that is found in the file. It is not, by itself giving you information about any particular read found in the alignment file. Read group information about a particular read in the alignment file is given on the alignment line in a column (way off near the end of the line typically) tagged with RG:Z. That RG:Z column in an alignment row simply holds the ID of the read group that the read belongs to. In this way, all the information that applies to the read—i.e., the sample it came from, (SM), the library prep it was in (LB), and information about the flowcell and lane (PU)—can be assigned to the read, without writing all of that information on every line in the alignment file. Accordingly, it should be obvious why the read group IDs must be unique across all the different read groups (@RG tags) in a SAM/BAM file: the ID is used to identify all the other information that goes along with that read, and is used to differentiate those different read groups!\nYou can download the first 100 lines (without the @SQ lines) of a SAM file that has 8 different read groups in it here. Let’s do that and have a look at it.\nThe way that read group information—particularly the platform unit, or PU information—is recorded is dictated by which software will be used for downstream processing. We describe the conventions that are useful for passing the aligned data to the Genome Analysis Toolkit (GATK), an extensive and well-supported piece of software developed by the Broad Institute. Currently, the GATK conventions for read group tags are these:\n\nID : any string that uniquely identifies a read group. The user must make sure that these uniquely identify the read groups. If you have multiple samples, libraries, flowcells, and lanes, and easy way to ensure this is to just bung those all together in a format like {Sample}.{Library}.{Flowcell}.{Lane}, where the parts in curly braces should be replaced by the actual names of those items. As long as there is a PU field, however, you can make the ID anything you want, as long as it is unique for each read group.\nSM : any string (no spaces please!) that serves as an identifier for the individual the sample was taken from. This is used to assign genotypes to indvididuals, so even if you have different “samples” from the same individual (i.e. some blood and some tissue, prepped in different libraries; or an original extraction prepared in an earlier library and a later extraction prepared in a library for individuals that needed more sequening coverage after an initial sequencing run) you should still assign them all the same SM tag because they are the same individual, and you will’ want all of the DNA sequences from them to apply to calling genotypes for them.\nLB : any string which gives the name of the library prep these reads were prepared in. If you prepped 192 individuals in one day, they are all part of the same library.\nPU : GATK expects this information to be given in a specific format. You can provide it as {Flowcell}.{Lane} (for example, HTYYCBBXX.1), but with later versions of the base quality score recalibration methods in GATK, it appears that it might be useful to also add information about the particular individual/library the read came from. It is suggested that this be done by including the barcode of reads, as {Flowcell}.{Lane}.{Barcode}, like HTYYCBBXX.1.TCAATCCG+TTCGCAGT.\nPL : It is also important for base calibration to indicate what type of sequencer was used. Most of the time, currently, that will be ILLUMINA.\n\nUsually, when you get data back from the sequencing center, it will be in a series of different FASTQ files, each one containing reads from a single DNA sample well (on a plate that you sent to them), that was sequenced on a certain lane on a given flow cell. The files will often be named something like: DPCh_plate1_C07_S31_L7_R2.fq.gz where the first part gives a sample identifier, the part after the L gives the lane, and the part after the R tells whether the file holds read 1 or read 2 of paired end reads. You can get the flowcell name by looking at the sequence names inside the file (See Section @ref(illumina-ids)). You will map the reads in each file separately, and when you do so, you will attach the read group information to them (see below). Accordingly, it is a great idea to maintain a spreadsheet that links the different files to the attributes and the read group information of the reads inside them, like this:\nindex   file_prefix ID  PU  SM  PL  LB  Flowcell    Lane\n1   DPCh_plate1_A01_S1_L1_R HTYYCBBXX.1.CH_plate1_A01   HTYYCBBXX.1.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   1\n2   DPCh_plate1_A01_S1_L2_R HTYYCBBXX.2.CH_plate1_A01   HTYYCBBXX.2.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   2\n3   DPCh_plate1_A01_S1_L3_R HTYYCBBXX.3.CH_plate1_A01   HTYYCBBXX.3.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   3\n4   DPCh_plate1_A01_S1_L4_R HTYYCBBXX.4.CH_plate1_A01   HTYYCBBXX.4.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   4\n5   DPCh_plate1_A01_S1_L5_R HTYYCBBXX.5.CH_plate1_A01   HTYYCBBXX.5.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   5\n6   DPCh_plate1_A01_S1_L6_R HTYYCBBXX.6.CH_plate1_A01   HTYYCBBXX.6.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   6\n7   DPCh_plate1_A01_S1_L7_R HTYYCBBXX.7.CH_plate1_A01   HTYYCBBXX.7.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   7\n8   DPCh_plate1_A01_S1_L8_R HTYYCBBXX.8.CH_plate1_A01   HTYYCBBXX.8.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   8\n9   DPCh_plate1_A02_S2_L1_R HTYYCBBXX.1.CH_plate1_A02   HTYYCBBXX.1.CGCTACAT+CGAGACTA   CH_plate1_A02   ILLUMINA    Lib-1   HTYYCBBXX   1\nIn the exercise we will see how to use such a file to assign read groups when mapping."
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#aligning-reads-with-bwa",
    "href": "nmfs-bioinf/sequence-alignment.html#aligning-reads-with-bwa",
    "title": "10  Sequence alignment and handling BAM files",
    "section": "10.3 Aligning reads with bwa",
    "text": "10.3 Aligning reads with bwa\nThere are several programs for aligning reads to a reference genome. We focus on bwa which is an industry standard aligner written by Heng Li and Richard Durbin [@liFastAccurateShort2009]. The name stands for “Burrows-Wheeler Aligner.” We won’t go deeply into the guts of this alignment algorithm, but we will briefly state that nearly all alignment methods rely on pre-processing the reference genome into a data-structure (like a suffix tree) that provides an index which makes it fast to find the place in a genome where a query sequence matches. Such indexes can take up a large amount of computer memory. The Burrows-Wheeler Transform (BWT) provides a way of decreasing the size of such indexes. The BWT is an operation that takes a sequence of characters (in this case DNA bases) and re-orders them so that similar characters tend to appear together in long runs of the same character. Sequences with long runs of the same character can be easily compressed to take up less space using run length encoding, for example. (We have already seen an example of run length encoding in the CIGAR string (Section @ref(cigar)), in which runs of the same kind of alignment characteristic (Matches, Deletions, etc) where encoded by their type and the length of the run.) The remarkable thing about the BWT is that it is invertible: if someone gives you a sequence and says that it is the result of applying the BWT to an original sequence, you can actually recover the original sequence without any other information! The program bwa uses the BWT to compress the index used for alignment so that it can easily fit into memory—even for very large genomes—on most any computer system, even a low-powered laptop produced in the last half decade.\n\n10.3.1 Indexing the genome for alignment\nAs the foregoing discussion suggests, it is necessary to index a genome before aligning to it with bwa. This is a step that only needs to be done once, since the process creates an index that is stored on the hard drive alongside the genome. So, after you download a reference genome, before you can start aligning sequences to it, you must index it using bwa. The syntax for this is very straightforward:\nbwa index path-to-genome\nFor example:\nbwa index genome/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz\nThe reference genome must be stored as a FASTA file (Section @ref(fasta)), which can be compressed using gzip. (In the above example, the .fna.gz prefix means that the file is a FASTA file of nucleotides (.fna) and has been gzipped (.gz)).\nThis process may take several hours, depending upon the size of the reference genome. When it is complete, several new files with names that are the orginal reference genome file plus several extensions will have been produced and saved in the same directory as the reference genome. In the above example, after indexing, a long listing of the directory with the genome file in it shows these files:\n% ls -hl\ntotal 3.8G\n-rw-rw-r-- 1 eanderson eanderson 704M Mar  5 00:08 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz\n-rw-rw-r-- 1 eanderson eanderson 4.4M Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.amb\n-rw-rw-r-- 1 eanderson eanderson 2.1M Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.ann\n-rw-rw-r-- 1 eanderson eanderson 2.3G Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.bwt\n-rw-rw-r-- 1 eanderson eanderson 579M Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.pac\n-rw-rw-r-- 1 eanderson eanderson 1.2G Mar  5 00:58 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.sa\nThe first file is the original gzipped compressed reference genome, and the remaining files are all part of the index. Note that the index files occupy considerably more space than the original reference file (but not so much as they would if the BWT were not used to reduce the size of the index).\nIf you ever wonder whether you have indexed a certain reference genome with bwa, it is a simple matter to go to the directory holding the reference genome and see what other files are there. If you find files with the same name, but having the suffixes, .amb, .ann, .bwt, .pac and .sa, that means that the reference genome has already been indexed.\n\n\n10.3.2 Mapping reads with bwa mem\nOnce the reference genome has been indexed, you are ready to align reads to it. The program bwa includes several different alignment algorithms. For paired-end Illumina data, the best available bwa algorithm, today, is the mem algorithm. The syntax is very simple. While there are many options available to tune the parameters used for alignment, the default values usually give good performance. Thus, at its simplest, bwa mem simply takes three file arguments:\nbwa mem   path-to-reference-genome   path-to-read1-fastq-file   path-to-read2-fastq-file\nAn example might look like:\nbwa mem genome/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz fastqs/DPCh_plate1_A01_S1_L1_R1.fq.gz fastqs/DPCh_plate1_A01_S1_L1_R2.fq.gz\nbwa mem prints progress messages to stderr and prints its output in SAM format to stdout. The read alignments come spewing out in the order they are listed in the FASTQ files, such that each read and its paired-end mate appear in adjacent rows in the output. It’s all super simple!\n\n\n10.3.3 Hold it Right There, Buddy! What about the Read Groups?\nIndeed! It is during the mapping with bwa mem that we must include read group tags for our reads. This is straightforward if the pair of FASTQ files contains reads that are all from a single read group, we merely have to give the read group information that we want to see as the argument to the -R option of bwa mem. This argument must be a single token on the command line, so, if it does not include spaces or other whitespace it can be supplied unquoted; however, it is usually safer to quote the argument on the command line. The tabs that occur between read group tags are given as \\t in the argument to the -R option.\nThus, in the bwa mem documentation, the example invocation of the -R option shows it single quoted as '@RG\\tID:foo\\tSM:bar'. In other words, you pass it the read group header line, complete with the @RG header tag, encoding the tabs between identifiers with \\t. What this example does not make clear is that you can also use double quotes around that argument. Double quotes, you will recall, allow variable substitution (Section @ref(quotes-and-var-subs)) to occur inside them. So, in your own scripts, especially when cycling over very many different pairs of FASTQ files, it is extremely useful to be able to define the values of the read group tags, and then supply them on the command line. Coarsely this would look like:\nID=HTYYCBBXX.1.CH_plate1_A01\nSM=CH_plate1_A01\nLB=Lib-1\nPU=HTYYCBBXX.1.TCAATCCG+TTCGCAGT\nPL=ILLUMINA\n\nbwa mem -R \"@RG\\tID:$ID\\tSM:$SM\\tLB:$LB\\tPU:$PU\\tPL:$PL\"  genome.fasta  file_R1.fq file_R2.fq\nHowever, you would probably want some way of assigning the values of the shell variables ID, SM, etc., programmatically, perhaps from the spreadsheet that holds all that information. The exercise this week shows one example of that using awk and a TAB-delimited spreadsheet."
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#samtools",
    "href": "nmfs-bioinf/sequence-alignment.html#samtools",
    "title": "10  Sequence alignment and handling BAM files",
    "section": "10.4 Processing alignment output with samtools",
    "text": "10.4 Processing alignment output with samtools\nDoing the alignment with bwa mem is only the first step of getting data ready to do variant calling. Once the SAM format data comes out of bwa mem, a number of things must happen to it before it can be used for variant calling:\n\nIt must be converted to BAM format (the compressed binary companion format to SAM)\nAdditional information about mate pairs might need to be attached to sequences\nYou must sort the BAM files from the original ordering produced by bwa mem into a coordinate-ordered format, in which the reads are sorted by where they appear within the reference genome.\nIf you started with multiple pairs of FASTQ files for a single individual (i.e. from different lanes or from different sequencing runs or libraries) you might need to merge those all into a single BAM file before variant calling.\nYou might want to mark PCR duplicates as such in the BAM file.\nFinally when you have your BAM file all ready for variant calling, you typically will need to index it for rapid access by the variant caller program.\n\nPhew! That is a lot of steps! Fortunately, there is a single “go-to” program (one-stop shopping!) that can manage all of that for you. It is the program samtools, brought to you by the same people who developed the SAM/BAM formats [@liSequenceAlignmentMap2009] and the bwa aligner (and bcftools for that matter). If you want to get anywhere in bioinformatics, it is important to become good friends with samtools.\nThe program samtools includes a large number of different subcommands. We will cover just a few of them, here, that are used in our typical paired-end workflow for whole genome sequencing, and in a somewhat cursory fashion, and we will disscuss only the most commonly used options. All readers are encouraged to completely read the online manual page for samtools for a complete account of its uses.\nOr, you can read the usage guidelines for any subcommand from the command line:\nconda activate bwasam\n\n# This lists all the different subcommands for you, categorized by\n# their uses\nsamtools  \n\n# if you follow samtools by any subcommand with nothing else, it gives you information\n# about all the options of that subcommand.  For example:\nsamtools view\n\nsamtools sort\nThe usage patterns of the subcommands can broadly be categorized into two groups:\n\nSubcommands that explicitly require an argument that gives the name of an output file. These subcommands are usually described like this:\n\n\nsamtools subcommand [options] out.bam in.bam\n\nwhere an output file is explicitly called for. These commands do not write output to stdout! They must be given an output file name. It is not possible to pipe output from one of these subcommands to another program or utility.\n\nSubcommands that don’t explicitly require an argument giving an output file:\n\n\nsamtools subcommand [options] in.bam\n\nThese subcommands write their output to stdout, and that output can be piped into other programs or can be redirected into a file (some also provide a -o file option to write output to file rather than to stdout.)\nIn short, if the syntax of the subcommand explicitly calls for an output file on the command line (not as part of a -o file option), then that subcommand does not write its output to stdout and you can’t pipe it into the next part of your pipeline.\nOn the other hand, if you can write output from samtools to stdout, you might be interested in knowing how you can pipe that into the input for another samtools subcommand. Rejoice! There is a syntactically economical and lovely way to do that. Any subcommand which expects a single input file will be described like:\nsamtools subcommand [options]  in.bam\nYou would typically put the path to the input file in the place of in.bam there. However, if you put - in place of in.bam then samtools will take input for in.bam from stdin.\nSo, for example, you can do something like this:\nbwa mem genome.fna R1.fq R2.fq |\n  samtools view -u -  |   # convert the SAM output from bwa mem into BAM format\n  samtools sort -l 9  - -o output_file.bam  # take stdin as the input, sort it, and write (with the best\n                                        # compression possible: -l 9) the output to output_file.bam\nThis is HUGE! It means that you don’t even have to save the initial SAM file that bwa mem makes, you can proceed directly to the sorted BAM file you want without eating up disk space on the intermediate files.\n\n10.4.1 samtools subcommands\nWe will go through some of the samtools subcommands that are particularly important in processing whole genome sequencing data in a step-by-step fashion, so you can get some experience with them, rather than just running them as parts of long and complex pipelines.\nTo do these steps, you will need to have a SAM file that is in the location example-files/s001---1.sam relative to the current working directory. To get that, you might need to sync your fork and then pull that into main.\nAlso, you will need samtools. On Alpine you can get that with\nmodule load samtools\nfrom a compute node.\nYou should be doing this on an interactive shell.\n\n10.4.1.1 samtools view\nThe view command is a major samtools workhorse.\nIt is often used to convert between SAM and BAM format, or to view them easily. Look at the options:\nsamtools view\nTo convert from SAM to BAM format specify that the output should be BAM with the -b option:\nsamtools view -b example-files/s001---1.sam &gt; example-files/s001---1.bam\nOnce you have done that, compare the size of the SAM and BAM versions of the same data:\ndu -h example-files/*\nHow much of a reduction in storage is that?\nNote that you can’t read a BAM file as a human. Try it with head and get the universal symbols of “HEY THIS IS A BINARY FILE!!”\nhead example-files/s001---1.bam\nYou can use samtools view to look at the alignments in a bam file. By default it doesn’t show you the SAM header:\n# pipe output to head to look at the first 10 alignments\nsamtools view example-files/s001---1.bam | head\nIf you want the output to inlude the header, you use the -h option:\nsamtools view -h example-files/s001---1.bam | head\nIf all you want to print is the SAM header, then use the -H command:\n# this is a handy way to get the last N (12 in this case) lines of the\n# SAM header:\nsamtools view -H example-files/s001---1.bam | tail -n 12\nCheck out that read group line! Also, note that some PG lines have been added there\nIn general, you will almost exclusively use BAM format instead of SAM format in bioinformatics, because it is much faster to process BAM input that SAM input. But it is worth understanding that BAM format can come in uncompressed or highly compressed versions. The compressed versions take up less disk space, but could require more time to operate on.\nWhen samtools view is used for converting from the text-based SAM format to BAM format, you might not want to spend time compressing the output. This is particularly true if you are piping the output back into another command that would just have to de-compress the BAM output in order to use it. If that is the case, then BAM output using the -u option is preferred over the -b option, since -u makes BAM output, but it saves the time that would be spent compressing and then decompressing the data as it gets piped from one samtools command to the next.\nWe will see an example of that when we do sorting.\n\n\n10.4.1.2 samtools sort\nThis subcommand sorts the alignments in a BAM file in order of their placement in the reference genome. This must be done for many downstream operations, so it is something that you will often do to alignments that come out of bwa mem or any other aligner.\nWhen a BAM file is sorted by genomic coordinates (i.e., by order of the placement of the alignments in the genome), it is said to be “coordinate-sorted.”\nFirst notice that our example-files/s001---1.sam file is not coordinate-sorted. Just look at it with less -S and note that the successive sequences are on different chromosomes/scaffolds and are not in sorted order at all!\nHere we will coordinate sort the BAM file we made above. We will put it into a file called example-files/s001---1.srt.bam. First, look at the samtools sort syntax:\nsamtools sort\nNow, let’s sort the file and make sure it is well compressed:\nsamtools sort -l 9 -o example-files/s001---1.srt.bam example-files/s001---1.bam\nCheck out how big the resulting file is:\ndu -h example-files/*\nCool! Even smaller than the original BAM file!\nFor fun, check out the header for the file and see that it now has an @HD line:\nsamtools view -H example-files/s001---1.srt.bam | head\nAnd, for fun, look at the @PG lines at the bottom of the header:\nsamtools view -H example-files/s001---1.srt.bam | tail -n 10\nNote that every command that has been used to modify the file contents is there!\nsamtools sort produces intermediate files when it is sorting large BAM files. You will sometimes see them if ls-ing the contents of directories while running a sort job. You used to have to be very careful if sorting multiple files at the same time that those intermediate files didn’t overwrite one another, but that is largely fixed/taken care of automatically by versions of samtools sort available today.\nNote that, instead of making the intermediate bam file, we could have piped BAM output into sort. This is very useful:\n# remove the .bam .srt.bam files:\nrm example-files/s001---1.bam example-files/s001---1.srt.bam\n\n# now, directly make a sorted BAM file at example-files/s001---1.srt.bam\n# by piping samtools view output into samtools sort.  Note the use\n# of -u for uncompressed BAM output, and the - at the end of the\n# line, instead of a file name, to mean take\n# input from stdin instead of a file\nsamtools view -u example-files/s001---1.sam | \\\n  samtools sort -l 9 -o example-files/s001---1.srt.bam -\n\n\n10.4.1.3 samtools index\nThe samtools index command operates on a coordinate-sorted BAM file and creates a new file having the name of the original file, but with the extension .bai added. .bai stands for “bam index.” It is an index that makes it very fast to access the alignments within certain genomic regions.\nWe can index our sorted BAM file like this:\nsamtools index example-files/s001---1.srt.bam\nOnce we’ve done that, let’s see how big that .bai file is:\ndu -h example-files/*\nIt’s pretty small! It is also a binary file.\nIf you have indexed a BAM file, you can use samtools view to retrieve and print all the reads in that indexed BAM file covering one or more genomic regions as specified using genomic coordinates. This happens very fast.\nFor example, to see all the read that overlap the 100 base pairs from 1001 to 1100 on chromosome CM031202.1, you would do this.\nsamtools view example-files/s001---1.srt.bam CM031202.1:1001-1100\nIn the full dataset there are three sequences that overlap that region. In the small data set, if you directly downloaded it today, there might not be any.\n\n\n10.4.1.4 samtools merge\nIf you have multiple sorted BAM files that you wish to merge into a single BAM file, you can use samtools merge. This subcommand opens up connections to each of the files and steps through each of them, alignment-by-alignment, copying alignments from each of the original BAMs into the merged BAM file. Thought of this way, it is clear why the input BAM files must be sorted. The merged BAM in outputted in sorted order.\nThere is some arcana with this utility when you have different reference genomes represented in your different BAMs, and older versions of samtools merge didn’t always play nicely with mutliple read groups in different files. For the most part, those issues were resolved in later releases. So, if you have just mapped a bunch of fastqs into BAMs and have sorted them, samtools merge should work and behave appropriately when merging those files.\nWhy should you merge files? The reason we typically to it is to make a single file that holds all the reads that came from one individual in one library prep for finding PCR duplicates. Alternatively, you might want to merge all BAMS from a single individual into a single BAM for variant calling.\nBack in the “old days” people would sometimes merge all the BAMs from multiple individuals. This is definitely not recommended today. It is easier to have a single BAM for each sample. Some utilities, like ANGSD, even require that input for variant discovery be done with one individual per BAM. So, you should almost never take BAM-merging further than merging all the alignments for a single individual (i.e., when you are done you should have one BAM file per individual.)\nThe syntax for samtools merge is:\nsamtools merge [options] output-bam-name.bam  sorted-input-bam-1.bam sorted-input-bam-2.bam ...\nIn other words, you give it the name of the output file you want it to produce, followed by all the input file names. If you get the order of the filenames wrong—for example putting the output file last, then it will usually bark you an error because you are, in effect, asking it to overwrite an existing input file.\nWe don’t have multiple files to merge at this point, but you might see this at a later time.\n\n\n10.4.1.5 samtools flagstats\nProvide summary information about the SAM flags of the alignments. This is a great utility to assess how many reads mapped, how many didn’t, and the nature of their mapping (as relayed by their SAM flags).\nsamtools flagstats example-files/s001---1.srt.bam\n\n\n10.4.1.6 samtools idxstats\nFrom an indexed BAM file, report how many reads map to each sequence in the reference genome.\nsamtools idxstats example-files/s001---1.srt.bam | less\nThe columns that come out of this are:\n\nReference sequence name\nLength of reference sequence\nNumber of reads that properly paired-end-map to the reference sequence\nNumber of reads that align to the reference sequence, but not as part of a properly paired read\n\n\n\n10.4.1.7 samtools stats\nComprehensive summaries of features of the alignments.\nsamtools stats example-files/s001---1.srt.bam | less\nNote that, in the headers for each of the different summaries in the output from samtools stats, instructions are given on how to extract just that summary.\nPage through the output. It isn’t always easy to read in text format. MULTIQC can gobble it up and make pretty graphs of it."
  }
]
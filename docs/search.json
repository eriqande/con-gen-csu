[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "con-gen-csu",
    "section": "",
    "text": "Welcome!\nWelcome to the website for the National Marine Fisheries Service Linux, Slurm, and Bioinformatics training to be held virtually over three days:",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "con-gen-csu",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course will be using the Sedna high-performance computing cluster located at the Northwest Fisheries Science Center. This cluster (and, hence, this course) is only available to NMFS employees and affiliates. If you are a NMFS employee and you are interested in this course, please see here for information about how to get an account on the cluster.\nThis course is intended for people who have already had some exposure to Unix or Linux. You should be reasonably comfortable navigating around the Unix filesystem using the command line. For a refresher, please read this chapter from my online bioinformatics book.\nMy goal is to:\n\nteach the shell programming constructs and the text processing tricks that I find myself using all the time in my day-to-day work\nprovide an introduction to how to use SLURM to do cluster computing\nOn the last day, show how Snakemake works, and how it can be used on the Sedna cluster to simplify your bioinformatics life.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#course-topics-and-sessions",
    "href": "index.html#course-topics-and-sessions",
    "title": "con-gen-csu",
    "section": "Course Topics and Sessions",
    "text": "Course Topics and Sessions\n\nDay 1: Intro, Unix-review, shell programming, awk\n\nIntroduction to the Sedna cluster (15 minutes Krista and Giles)\n\nCluster infrastructure and configuration.\nScientific software and the installation requests\n\nQuick Unix Review (25 minutes)\nShell Programming (50 Minutes)\nA Brief awk Intro Processing text files with awk (30 minutes)\n\nDay 2: A little bash stuff, then Sedna and SLURM\n\nBash scripts and functions (20 minutes)\nSedna and SLURM intro (40 minutes)\nSubmitting jobs with sbatch (40 minutes)\nSlurm Job Arrays (40 minutes)\n\nDay 3: Job Arrays, then an introduction to Snakemake\n\nSlurm Job Arrays (25 minutes)\nSnakemake Tutorial Introduction (90 minutes)",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "nmfs-bioinf/open-on-demand-alpine.html",
    "href": "nmfs-bioinf/open-on-demand-alpine.html",
    "title": "1  OpenOnDemand on Alpine",
    "section": "",
    "text": "1.1 Getting onto RStudio Server on Alpine\nIn order to establish an RStudio session container/image/overlay, the Alpine administrators recommend using 4 cores, because the initial setup can take a few minutes. After that you can re-attach to that image quickly and use just a single core if you would like. Briefy instructions on how to do that are:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>OpenOnDemand on Alpine</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/open-on-demand-alpine.html#getting-onto-rstudio-server-on-alpine",
    "href": "nmfs-bioinf/open-on-demand-alpine.html#getting-onto-rstudio-server-on-alpine",
    "title": "1  OpenOnDemand on Alpine",
    "section": "",
    "text": "In your web browser, go to https://ondemand-rmacc.rc.colorado.edu/.\nThe first time you click that, it will take you to a page where you must choose which institution’s OpenOnDemand infrastructure you are accessing. Choose Colorado State University, from the pop-up menu and be sure to select “Remember this selection” and then click “Log On”.\n\nThat should redirect you to your CSU NetID login page. Authenticate yourself.\nThat should drop you into the CURC OnDemand Dashboard page that looks like this:\n\nChoose “Interactive Apps–&gt;RStudio Server Presets”, then use the bottom toggle box to choose “4 cores, 4 hours”, and hit “Launch”. This will start making you an RStudio Server container. Again, we choose 4 cores just for the initial setup, but you can choose 1 core in the future.\n\nNow, go to your “Interactive Sessions” and you will see messages about your session starting up. Once your session is ready you can connect to it by clicking “Connect to RStudio Server”\n\nVoila! Clicking “Connect to RStudio Server” opens a new tab with an RStudio window that is pretty much what you are used to.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>OpenOnDemand on Alpine</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/open-on-demand-alpine.html#working-in-rstudio",
    "href": "nmfs-bioinf/open-on-demand-alpine.html#working-in-rstudio",
    "title": "1  OpenOnDemand on Alpine",
    "section": "1.2 Working in RStudio",
    "text": "1.2 Working in RStudio\nNote that you can, if you would like, open an RStudio project you have on Alpine by going to the “Project” tab in the upper right (or choosing File -&gt; Open Project... from the main menu) and navigating your way to the .Rproj file. This means that you could interact with your con-gen-csu repo on Alpine as an Rstudio Project. The possibilities are endless. You can open as many files on the cluster as you wish in the RStudio source-code editor window, and edit them with RStudio, making changes directly to them on the cluster. You have a graphical interface to git (but no ssh! see the section Section 1.2.1, below), that is familiar, and you can also get to a shell in the “Terminal” tab. You also have an active R session on the cluster in case you want to analyze any data you have on the cluster with R.\nJust as a reminder, the git interface in RStudio can be found on the “git” tab in the upper right panel (typically). You might have to extend the right panel to the left (make it larger) in order to see all the elements of the interface (like push and pull arrows and commit buttons, etc.)\nNote that if you open up files in Rstudio on Alpine that Rstudio recognizes as shell script files (because they have a .sh extension), then choosing CMD-Return (on a Mac) or cntrl-Return (on a PC) will send the line that your cursor is on to the Terminal shell (in much the same way that you can evaluate R code from an R script in the R console within RStudio). This provides a very handy way to test your shell scripts line by line on the Alpine bash shell.\nAt the time of this writing, I have not been able to load the slurm/alpine module in the RStudio Terminal to allow submission of jobs via sbatch in this environment, but I can keep you posted. In the meantime, you can always get a shell in your browser (see the next section).\nAs always, you should set your RStudio preferences (Tools –&gt; Global Options) so that R does not “Restore .Rdata into workspace at startup” and that it Never “Saves workspace to .Rdata on exit.”:\n\n\n\n\n\nWhen you are done using RStudio Server on Alpine, you should terminate your R session, with “Session –&gt; Terminate R…”:\n\n\n\n\n\nAnd then Delete the session running on the compute node from your OnDemand Dashboard.\nNote that if your time on the compute node runs out before you are done working on RStudio, then unsaved work and variables in your global environment will be lost and RStudio will start saying that it has lost connection (with a bunch of 503 errors).\n\n1.2.1 Using git on Rstudio Server on Alpine OnDemand\nThe program git works just fine in the OnDemand RStudio Server on when you are just dealing with local files. However it does not seem to have access to ssh. There is simply no ssh at /usr/bin/ssh in the shell that RStudio gets to run. This is super weird. We will have to check with the CURC admins about this.\nUpdate! I spoke with the CURC folks. There are two ways you can deal with it. Method 1 is to install ssh via fakeroot on an apptainer, and Method 2 is to just do it all on the command line.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>OpenOnDemand on Alpine</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/open-on-demand-alpine.html#method-1-install-ssh-and-use-rstudio-as-usual",
    "href": "nmfs-bioinf/open-on-demand-alpine.html#method-1-install-ssh-and-use-rstudio-as-usual",
    "title": "1  OpenOnDemand on Alpine",
    "section": "1.3 Method 1: Install ssh and use RStudio as usual",
    "text": "1.3 Method 1: Install ssh and use RStudio as usual\nThe CURC pages have a section on installing software on your OpenDemand overlay here. In synopsis, here is what you need to do:\n\nExport a USER2 variable in your ~/.bashrc. This is necessary because the @ in your username on Alpine flummoxes the system, otherwise. Add this line to your ~/.bashrc, before the conda block:\n\nexport USER2=$(echo $USER |  awk -F@ '{print \".\"$2\"/\"$1}')\n\nOnce you have done that, on your shell on Alpine, source your ~/.bashrc to make sure that USER2 is in effect:\n\nsource ~/.bashrc\n\nGet on a compute node with the following command:\n\nacompile --ntasks=4\n\nOnce you are on the compute node “modify the overlay by launching the overlay using fakeroot” with the following command:\n\napptainer shell --fakeroot \\\n    --bind /projects,/scratch/alpine,$CURC_CONTAINER_DIR_OOD \\\n    --overlay /projects/$USER/.rstudioserver/rstudio-server-4.2.2_overlay.img  \\\n    $CURC_CONTAINER_DIR_OOD/rstudio-server-4.2.2.sif\n\nYou might want to check this in the future to make sure that the rstudio server version numbers in the command correspond to what you are using on Open OnDemand.\nWhen the last command returns, you should get a command prompt of Apptainer&gt;. At this prompt you can give any commands like you would as a root user on a Linux system to install necessary software packages. In this case, to install ssh do:\n\napt-get update\n\nOnce that is done, you can do:\n\napt install ssh\n\nThat takes a while to chug through, but it will finish eventually.\nOnce the above command has finished, you can get out of the Apptainer and off of the acompile node with:\n\nexit\nexit\n\n\nNow, when you restart an RStudio server session, you will have access to SSH and the git push and pull buttons will work as expected for branches that are tracking upstream. So, when you make new branches in RStudio, you should click Sync branch with remote.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>OpenOnDemand on Alpine</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/open-on-demand-alpine.html#method-2-just-use-the-command-line",
    "href": "nmfs-bioinf/open-on-demand-alpine.html#method-2-just-use-the-command-line",
    "title": "1  OpenOnDemand on Alpine",
    "section": "1.4 Method 2: Just use the command line",
    "text": "1.4 Method 2: Just use the command line\nIf you don’t want to hassle with all of Method 1 (although it really isn’t a hassle at all!) you can use RStudio to stage and commit files (and gitignore them), just like you would on Rstudio on your laptop. And you can also switch branches and create new branches. But you can’t do git push and git pull operations, on RStudio Server on Alpine. Instead, you can use the browser-based shell (see the next section) to do your git push and git pull.\nNote that if you create a new branch in RStudio on Alpine, you should not click the “Sync branch with remote” button, because that operation will fail, since it requires ssh. However, once you have made the branch, you can go to your terminal (either in the browser or through your terminal emulator (i.e. tmux) and then cd to the repo. Check with git branch to make sure that you are on the new branch and then do\ngit push -u origin &lt;new-branch&gt;\nwhere you replace &lt;new-branch&gt; with the actual branch name. The -u option tell git to create a new branch named &lt;new-branch&gt; on origin that automatically tracks &lt;new-branch&gt; on your local repository, so that from now on, any time you are on &lt;new-branch&gt;, from now on, you can just say git pull or git push without specifying anything else, and git will automatically know to push or pull to &lt;new-branch&gt; on origin.\nFor example, in RStudio I created a new branch called gratuitous-branch and I told RStudio not to sync the branch with remote:\n\n\n\n\n\nThen I went to my shell and did git branch to confirm I was on that branch:\n(base) [login-ci1: ~]--% cd projects/con-gen-csu/\n(base) [login-ci1: con-gen-csu]--% git branch\n  005-trim-map2\n  fastp-iteration\n* gratuitous-branch\n  main\nThe asterisk tells me that is the branch I am on! So now I do:\ngit push -u origin gratuitous-branch\nNothing got transferred because I had no changes on this new branch, but the output message from git is good to see:\nTotal 0 (delta 0), reused 0 (delta 0), pack-reused 0\nremote: \nremote: Create a pull request for 'gratuitous-branch' on GitHub by visiting:\nremote:      https://github.com/eca-home/con-gen-csu/pull/new/gratuitous-branch\nremote: \nTo github-home:eca-home/con-gen-csu.git\n * [new branch]      gratuitous-branch -&gt; gratuitous-branch\nbranch 'gratuitous-branch' set up to track 'origin/gratuitous-branch'.\nMost importantly it tells me that branch 'gratuitous-branch' set up to track 'origin/gratuitous-branch'. which is what we want.\nWe can also verify branch tracking by using git branch -vv:\n(base) [login-ci1: con-gen-csu]--% git branch -vv\n  005-trim-map2     9901ce8 [origin/005-trim-map2] answer key\n  fastp-iteration   67a106e [origin/fastp-iteration] done\n* gratuitous-branch 034d018 [origin/gratuitous-branch] Merge branch 'eriqande:main' into main\n  main              034d018 [origin/main] Merge branch 'eriqande:main' into main\nHere, we see that all my local branches are set up to track a remote branch of the same name on origin. So, if I commit any changes on a branch, I can just say,\ngit push \nor\ngit pull\nand it will push or pull to/from the correct branch on the remote origin. Cool!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>OpenOnDemand on Alpine</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/open-on-demand-alpine.html#other-places-to-go-from-the-dashboard",
    "href": "nmfs-bioinf/open-on-demand-alpine.html#other-places-to-go-from-the-dashboard",
    "title": "1  OpenOnDemand on Alpine",
    "section": "1.5 Other places to go from the Dashboard",
    "text": "1.5 Other places to go from the Dashboard\nWe should also mention that if you go to the “Files” tab on the original OnDemand Dashboard page, you get a nice web-browser-based file browser of your files on Alpine. This can be a good way to transfer a couple of small files here and there, but don’t transfer too much. For big files, or for transferring things to scratch, you should use globus, as explained here.\nFinally, from you Alpine dashboard in your browser, you can even get a shell (in your browser) on Alpine. Do that by selecting Clusters--&gt; &gt;_Alpine Shell. It is a fully functional shell terminal in your browser. Apparently you can even get to this on your phone or tablet browswer. (Just when you thought cluster computing couldn’t intrude any more on your life than it currently is—it is gonna get you via your devices).\nIn the shell, you can choose different color themes from the selector in the upper right. Also, you can open as many shells as you want—each one will go in a new browser tab.\nEnjoy!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>OpenOnDemand on Alpine</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html",
    "href": "nmfs-bioinf/quick-unix-review.html",
    "title": "2  Quick Unix Review",
    "section": "",
    "text": "2.1 What is bash?\nWe start by acknowledging that there are many different flavors of Unix and Linux. I will refer to them all simply as Unix or unix.\nAlso, there are a number of different shells for Unix. The shell is the part that interprets commands.\nWe will be talking about the bash shell. This is the default shell on Sedna, and it is also the most popular shell for bioinformatics.\nBash stands for “Bourne-again shell”. It is an update to an earlier shell called the Bourne shell.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick Unix Review</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#setting-up-our-workspace",
    "href": "nmfs-bioinf/quick-unix-review.html#setting-up-our-workspace",
    "title": "2  Quick Unix Review",
    "section": "2.2 Setting up our workspace",
    "text": "2.2 Setting up our workspace\n\nI have prepared a repository with a few different example data files that we be using.\nIt also contains all these notes.\nI want everyone to download it to their home directory and then cd into its playground directory, where we will be playing today and tomorrow.\n\nAfter logging onto Sedna:\n\n\nPaste this into your shell\n\ncd ~\ngit clone https://github.com/eriqande/nmfs-bioinf-2022.git\ncd nmfs-bioinf-2022/playground\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen we say “Paste this into your shell” or “Type this at your command prompt” we also implicitly mean “Hit RETURN afterward.”\n\n\n\nThis is where our working directory will be for the next two days.\nUse the tree utility to see the files that we have to play with within this playground:\n\n\n\nType this command at your prompt\n\ntree\n\nThe data directory has a few things that we will be using for examples.\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ncd: change directories\ngit: run git subcommands, like clone with this. In the above case it clones the repository that is found at the GitHub URL.\ntree: Super cool “text-graphical” directory listing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick Unix Review</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#a-motivating-example",
    "href": "nmfs-bioinf/quick-unix-review.html#a-motivating-example",
    "title": "2  Quick Unix Review",
    "section": "2.3 A motivating example",
    "text": "2.3 A motivating example\n\nThe data/samtools_stats directory has gzipped output from running the samtools stats program on 30 different samples.\nThis provides information about reads that have been mapped to a reference genome in a BAM file.\n\nTo see what those files look like:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\n\n\nHit the SPACE-bar to go down a screenful, the b key to go back up\nMost terminal emulators let you use up-arrow and down-arrow to go one line at a time, too.\nHit the q key to quit out of the less viewer.\n\nTo see it without lines wrapping all over the place try this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\n\nNow you can use the left and right arrows to see different parts of lines that are not wrapped on the screen.\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ngzip -cd: decompress gzipped file to stdout (this is what zcat does, but zcat is not portable).\nless: page view. Great to pipe output into. (SPACE-bar, b, q, down-arrow, up-arrow)\n\nless -S: option to not wrap lines. (left-arrow, right-arrow)\n\n\n\n\n\n\n2.3.1 (One of) Our Missions…\nIt is pretty typical that Bioinformatic outputs will be spread as small bits of information across multiple files.\nOne motivating example is summarizing the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped, in all 30 samples, in a table.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick Unix Review</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#the-anatomy-of-a-unix-command",
    "href": "nmfs-bioinf/quick-unix-review.html#the-anatomy-of-a-unix-command",
    "title": "2  Quick Unix Review",
    "section": "2.4 The anatomy of a Unix command",
    "text": "2.4 The anatomy of a Unix command\nNearly every line in a bash script, or every line you type when banging away at the Unix terminal is a command that has this structure:\ncommand options arguments\n\nThe command is the name of the command itself (like cd or less).\nThe options are often given:\n\nwith a dash plus a single character, like -l or -S or -a, -v, -z.\n\nIn most commands that are part of Unix, these single options can be combined, so -cd, is the same as -c -d.\n\nwith two dashes and a word, like --long or --version\nSometimes options take arguments, like --cores 20, but sometimes, they stand alone.\n\nWhen they stand alone they are sometimes called flags.\n\n\nThe arguments are typically file or paths.\n\n\n\n\n\n\n\nSelf-study question\n\n\n\nIdentify the command, options, and arguments in:\ntree -d ..\n\n# and\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz\n\n\n\n\n\n\n\n\nSelf-study answer\n\n\n\n\n\nFirst case:\n\ntree is the command\n-d is the option (print only directory names, not files)\n.. is the argument (one directory level up)\n\nSecond case:\n\ngzip is the command\nThe options are -c and -d, contracted into -cd\ndata/samtools_stats/s001_stats.tsv.gz is the argument\n\n\n\n\n\n2.4.1 What do all these options mean?\nEverything you need to know about any Unix command will typically be found with the man command. For example:\n\n\ntype this at your terminal\n\nman tree\n\n\nThat gives you more information than you will ever want to know.\nIt starts with a synopsis of the syntax, which can feel very intimidating.\n\n\n\n\n\n\n\nBonus Tips:\n\n\n\n\n\n\nMan uses the less viewer for presenting contents of the man pages.\nWhen you are viewing man pages, you can scroll down with SPACE-bar and up with b, and get out with q, just like in less\nTo search for patterns in the manual pages, you can type / then the string you want and then RETURN.\n\nWhen in pattern-searching mode, use n to go to the next occurrence, and N to the previous.\nIf searching for a single letter option try searching with [, ] afterward.\nFor example, to search for the -d flag you would type /, then -d[, ], then hit RETURN. Try it on the tree man page.\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nLearn about gzip\n\nUse man to read information about the gzip command\nFind information about the -c and the -d options.\n\nMaybe even search for those using the “slash-pattern” bonus tip from above.\n\n\nLearn about the ls command\n\nUse man to see information about the ls command, which lists directories and their contents\nFind out what the -R option does. Maybe even look for it using the Bonus Tip above.\nDo the same for the -Q option.\nLook at what those do by doing ls -RQ on your terminal.\n\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\n\n\nman gzip\nTo search for -c, type /-c + return. You might have better results with /-c[, ].\n\nuse n or N to go forward or backward through the occurrences of -c.\n\n\n\nYou would do man ls\nTo search for -R in the man pages, a good way to do it would be to type /-Q + RETURN, or maybe /-Q[, ] + RETURN.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick Unix Review</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#streams-and-redirection",
    "href": "nmfs-bioinf/quick-unix-review.html#streams-and-redirection",
    "title": "2  Quick Unix Review",
    "section": "2.5 Streams and redirection",
    "text": "2.5 Streams and redirection\n\nWhen you’ve executed the unix commands above, they have typically responded by writing text or data to the terminal screen.\nThe command is actually writing to a stream that is called stdout, which is short for “standard output.”\nIt turns out that, by default, the stdout stream gets written to the terminal.\n\nAha! But here is where it gets fun:\n\nYou can redirect the stdout stream to a file by using &gt; or &gt;&gt; after the command, options, and arguments.\n\nFor example:\n\n\nPaste this into your terminal\n\nmkdir outputs\ntree -d .. &gt; outputs/repo-tree.txt\n\nNow, you can use the less viewer to see what got into the file outputs/repo-tree.txt:\n\n\nType this at the terminal\n\nless outputs/repo-tree.txt\n\nAha! Instead of writing the output to the screen, it just puts it in the file outputs/repo-tree.txt, as we told it to.\n\n\n\n\n\n\nDanger!\n\n\n\nIf you redirect stdout into a file that already exists, the contents of that file will get erased!!!\nFor example, if you now do:\n\n\nPaste this into the shell\n\necho \"New content coming through...\" &gt; outputs/repo-tree.txt\n\nThen you will no longer have the output of the tree command in the file outputs/repo-tree.txt. Check it out with the less command.\n\n\nIf you want to merely append stdout to an existing file, you can use &gt;&gt;. For example:\n\n\nPaste this into your terminal\n\necho \"Add this line\" &gt;&gt; outputs/repo-tree.txt\necho \"And then add another line\" &gt;&gt; outputs/repo-tree.txt\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\nmkdir: make a new directory\n\n(check out the -p option, which means “make any necessary parent directories and don’t complain if the directory already exists.”)\n\necho: print the argument (usually a string) to stdout.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick Unix Review</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "href": "nmfs-bioinf/quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "title": "2  Quick Unix Review",
    "section": "2.6 Pipes: redirecting into another Unix command",
    "text": "2.6 Pipes: redirecting into another Unix command\nAs we have said, many Unix utilities take files as their arguments, and they operate on the contents of that file. They can also receive input from streams, and almost all Unix utilities are set up to accept input from the stream called stdin, which is short for standard input.\n\nThe most important way to pass the stdin stream to a Unix command is by piping the stdout from one command in as the stdin to the next command.\nThis uses the | which is called the “pipe”.\n\nWe have already used the pipe when we did:\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\nPipe syntax is pretty simple:\ncommand1 | command2\nmeans pipe the stdout output of command1 in as stdin input for command2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick Unix Review</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "href": "nmfs-bioinf/quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "title": "2  Quick Unix Review",
    "section": "2.7 stderr: The stream Unix uses to yell at you",
    "text": "2.7 stderr: The stream Unix uses to yell at you\n\nIf a Unix command fails, typically the program/command will bark at you to tell you why it failed. This can be very useful.\nThe stream it writes this information to is called stderr, which is short for standard error.\nSome bioinformatics programs write progess and log output to stderr, in addition to actual error messages.\n\nIf you are running a program non-interactively, it is extremely valuable and important to redirect stderr to a file, so you can come back later to see what went wrong, if your job failed.\n\nstderr is redirected with 2&gt;.\nThink of the 2 as meaning that stderr is the second-most important stream, after stdout.\n\n\n\n\n\n\n\nBonus side comment:\n\n\n\n\n\nAs you might imagine, you could redirect stdout by using 1&gt; instead of &gt;, since stdout is stream #1.\n\n\n\nFor example, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. &gt; outputs/repo-tree.txt\n\n\nAha! We get a warning note printed on the screen,\nBecause, stderr gets printed to the terminal by default.\nAlso outputs/repo-tree.txt has been overwritten and is now a file with nothing in it.\n\nSo, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. &gt; outputs/repo-tree.txt 2&gt;outputs/error.txt\n\nNow, look at the contents of both outputs/error.txt and outputs/repo-tree.txt:\n\n\nPaste this into your shell\n\nhead outputs/repo-tree.txt outputs/error.txt\n\n\n\n\n\n\n\nStream operators and commands that we just saw:\n\n\n\n\n\n\n&gt; path/to/file: redirect stdout to file at path/to/file. This overwrites any file already at path/to/file.\n&gt;&gt; path/to/file: redirect stdout to append to file at path/to/file. If path/to/file does not exist, it creates it and then adds the contents of stdout to it.\n2&gt; path/to/file: redirect stderr to the file at path/to/file.\n|: the uber-useful Unix pipe. (Just as an aside, when R finally got a similar construct—the %&gt;% from the ‘magrittr’ package—it became much easier for Unixy people to enjoy coding in R).\nhead: print the first ten lines of a file to stdout. If multiple file arguments are given, they are separated by little ==&gt; filename &lt;== lines, which is super convenient if you want to look at the top of a lot of files.\n\nhead -n XX: print the first XX lines (instead of 10).\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nDecompress data/samtools_stats/s001_stats.tsv.gz onto stdout using the gzip -cd command and pipe the output into wc to count how many lines words, and characters are in the file.\nDo the same that you did above, but redirect the stdout to a file so.txt and stderr to a file se.txt in the current working directory.\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nThese could be done like this:\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc &gt; so.txt 2&gt;se.txt\n\nAs an interesting side note, this will only redirect stderr for the wc command into se.txt. If the first command fails, its stderr will to to the screen. Try this:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc &gt; so.txt 2&gt;se.txt\n\nAn interesting fact is that you can redirect stderr from the first command before the pipe. So, to redirect stderr for the gzip command into a file called ze.txt, we could do:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz 2&gt;ze.txt | wc &gt; so.txt 2&gt;se.txt\n\nHave a look at the contents of ze.txt.\n\n\n\n\n\n\n\n\n\nPro-tip: Redirect stdout and stderr to the same place\n\n\n\n\n\nThe astute reader might note that if you redirect stdout to a file, and then redirect stderr to the same file, you might end up overwriting the contents of stdout with stderr.\nIf you want to redirect stdout and stderr to the same place then you first redirect stdout to a file, and then after that, you say “redirect stderr to wherever stdout has been redirected to,” by using 2&gt;&1.\nSo it looks like this:\ncommand &gt; file.out 2&gt;&1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quick Unix Review</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html",
    "href": "nmfs-bioinf/shell-prog.html",
    "title": "3  Shell Programming",
    "section": "",
    "text": "3.1 Variables\nThe bash shell can store strings into variables. These variables can be accessed later.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variables",
    "href": "nmfs-bioinf/shell-prog.html#variables",
    "title": "3  Shell Programming",
    "section": "",
    "text": "3.1.1 Assigning variable values\n\nbash is remarkably picky about how to assign a value to a variable.\nUse the equals sign with no spaces around it!\n\nHere, we can assign a value to a variable called PHONE:\n\n\nPaste this into your shell\n\nPHONE=974-222-4444\n\nIn this case, the digits and dashes 974-222-4444 are treated as just a string of characters and assigned to the variable PHONE.\n\n\n\n\n\n\nLet’s make some mistakes\n\n\n\nPaste these commands into the shell and see what happens:\n\n\nPaste these mistakes into the shell and think about what is happening.\n\nPHONE = 974-222-4444\nPHONE =974-222-4444\nPHONE= 974-222-4444\n\nWhat does this tell us about how bash interprets commands with an equals sign?\n\n\n\n\n3.1.2 Accessing variable values\n\nThe process of accessing the values stored in the variable is called “variable substitution.\nIt means: “Substitute the value for the variable where it appears on the command line.”\nIn many programming languages, you can just write a variable’s name and know that its value will be accessed, like in R:\n\nVariable &lt;- 16\nsqrt(Variable)\n\nHowever, in bash, variable substitution is achieved by prepending $ to the variable’s name.\n\nWitness:\n\n\nPaste this into your shell\n\necho The value of PHONE is: $PHONE\n\nCool!\n\nRemember: if you make a substitution to the menu at a fancy restaurant, it is going to cost you some dollars. Same way when you make a variable substitution in bash: it costs you a dollar sign and you have to pay up front.\n\n\n\n3.1.3 Valid bash variable names\nThe bash shell demands that the names of variables:\n\nStart with _ (an underscore) or a letter\nThen include only _, letters, or numbers\n\n# good variable names\nMY_JOBS\n_Now\nSTRING\ni\ni_2\ni2\n\n# cannot be variable names\n1_node\n4THIS\nBIG-VAR\nfile-name\nSh**t!\n\n\n\n\n\n\nSelf-study\n\n\n\nChoose one of the good variable names from the list above and assign the value Good to it.\nChoose of the bad variable names from the list above and try to assign the value Bad to it.\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nHere is an example. {.sh filename = \"This works\"} _Now=Good\nHere is an example. {.sh filename = \"This does not work\"} BIG-VAR=Bad",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#strings-with-spaces-etc-quoting.",
    "href": "nmfs-bioinf/shell-prog.html#strings-with-spaces-etc-quoting.",
    "title": "3  Shell Programming",
    "section": "3.2 Strings with spaces, etc: Quoting.",
    "text": "3.2 Strings with spaces, etc: Quoting.\n\nIf you want to assign a string to a variable that has spaces in it you can quote the string, which holds it together as one “unit.”\n\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWhen the bash shell is interpreting a line of input, it breaks it into chunks called tokens which are separated by white space (spaces and TABS). If you wrap a series of words in quotation marks, it turns them all into a single token.\n\n\n\nFor example:\n\n\nPaste this into your shell\n\nMandela_Quote=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\necho $Mandela_Quote",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variable-substitution-and-vs",
    "href": "nmfs-bioinf/shell-prog.html#variable-substitution-and-vs",
    "title": "3  Shell Programming",
    "section": "3.3 Variable substitution and \" vs '",
    "text": "3.3 Variable substitution and \" vs '\nWe have two types of quotes:\n\nsingle quotes, like '\ndouble quotes, like \"\n\nThey both chunk their contents into a single unit, but they behave very differently with respect to variable substition.\n\nSingle Quotes: surly and strict, you can’t substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho 'Dessert tonight is $DESSERT'\n\n\nDouble Quotes: soft and friendly, you CAN substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho \"Dessert tonight is $DESSERT\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nAssign values to three shell variables, NAME, FOOD, and ACTIVITY, so that when you run the following command, it makes sense:\n\n\nAfter assigning values to the three variables, run this command\n\necho \"My name is $NAME. I like to eat $FOOD, and I enjoy $ACTIVITY.\" \n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nFor my case, I could put:\nNAME=Eric\nFOOD=\"steamed broccoli\"\nACTIVITY=\"inline skating long distances\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variables-are-not-just-to-be-echoed",
    "href": "nmfs-bioinf/shell-prog.html#variables-are-not-just-to-be-echoed",
    "title": "3  Shell Programming",
    "section": "3.4 Variables are not just to be echoed!",
    "text": "3.4 Variables are not just to be echoed!\nInvariably, when learning how to use shell variables, all the examples have you using echo to print the value of the variable. How boring and misleading.\nIt is important to understand that after a value gets substituted onto the command line, the shell goes right ahead and evaluates the resulting command line.\nSo, you can record shell variables that are command lines that do something, themselves, once they are run as a command line.\nFor example, here we make a variable whose value is the command to decompress a file to stdout:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz\"\n\nAnd now if you just substitute that variable onto the command line\n\n\nType this at the command line\n\n$MyComm\n\nthe uncompressed contents of the file data/samtools_stats/s001_stats.tsv.gz go zooming by on your screen.\n\n3.4.1 Some subtlety about evaluation of substituted values\nIf the value of the variable that is being evaluated includes pipes, redirections, or variable assignment statements, then if you just substitute it into the command line, it won’t properly be evaluated as a command line in full. For example, if MyComm was trying to decompress the file and pipe it to less, it doesn’t work as expected:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\"\n\nAnd now if you just substitute that variable onto the command line the shell gets confused, because it doesn’t recognize the pipe as a pipe!\n\n\nType this at the command line\n\n$MyComm\n\nHowever, you can use the eval keyword before $MyComm to ensure that the shell recognizes that you intend for it to evaluate pipes, redirects, shell variable assignment, etc. in the substituted variable value as it normally would:\n\n\nType this at the command line\n\neval $MyComm\n\nWe will end up using this later.\nRemember, you can hit q to get out of the less page viewer.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#multiple-commands-on-one-line-with",
    "href": "nmfs-bioinf/shell-prog.html#multiple-commands-on-one-line-with",
    "title": "3  Shell Programming",
    "section": "3.5 Multiple commands on one line with ;",
    "text": "3.5 Multiple commands on one line with ;\n\nYou can put a ; after a command, and it will behave like a line ending—the shell will run that command, and then go to the next.\n\nExample:\necho \"Let us do this command and 2 others\"; echo \"here is number 2\"; echo \"and the third\"\nThis comes in handy.\n\n\n\n\n\n\nMore about line endings like ;\n\n\n\nThere are two other things you might find at the end of a line: & and &&\n\n& at the end of the line means “run the command, but don’t wait for it to finish.\n\nThis runs the command “in the background” in some sense.\nThis is not used very often when doing bioinformatics in a SLURM-driving system like that on Sedna\n\n&& at the end of a command means “only run the next command if the previous one did not fail.\n\nThis is very useful for making sure that you don’t keep running later commands if an earlier one failed.\nThere is also a || that is useful in this context, which is all about “exit status” of Unix commands, which is beyond our purview today.\n\n\nExamples:\n\n\nWith just semicolons...Paste it into your shell\n\necho \"Yawp before it fails.\"; ls --not-option data ; echo \"Yawp after it fails.\"\n\n\n\nWith the &&'s after each line...Paste it into your shell\n\necho \"Yawp before it fails.\" && ls --not-option data && echo \"Yawp after it fails.\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#repetition",
    "href": "nmfs-bioinf/shell-prog.html#repetition",
    "title": "3  Shell Programming",
    "section": "3.6 Repetition",
    "text": "3.6 Repetition\nLet’s face it, bioinformatics, or any sort of data analysis or processing often involves doing the same thing to a number of different inputs.\nMost unix utilities are designed so that if you give it multiple inputs it will do the same thing to each and report the results in a way that is easy to understand.\nFor example, to see how many lines, words, and characters are in each Quarto (the successor to RMarkdown) document that I used to make this website, we can use wc on all the files with a .qmd extension that are one directory level above where we are currently:\n\n\nPaste this into your shell\n\nwc ./nmfs-bioinf/*.qmd\n\nThis is nice.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#for-loops",
    "href": "nmfs-bioinf/shell-prog.html#for-loops",
    "title": "3  Shell Programming",
    "section": "3.7 For loops",
    "text": "3.7 For loops\nSometimes, however, we have more complex operations to do, so we can’t just provide multiple files to a single Unix utility.\nFor example, let’s say we want to know how many lines are in each of the samtools stats files in the data/samtools_stats directory. We can’t use wc directly, because these files are gzipped, and the result we get won’t be equal to the number of lines, words, and characters in each file.\nFor repetition in these cases, bash has a for loop. Its syntax looks like this:\nfor VAR in thing1 thing2 ... thingN; do\n  one or more commands where the value of VAR is set to each of the N things in turn\ndone\nThe important “structural” parts of that are:\n\nthe for\nthe in\nthe semicolon after all the things\nthe do\nthe done\n\nHere is an example:\n\n\nPaste this into your terminal\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do \n  echo \"I like $LIKE.\"\ndone\n\nNote that this is written over multiple lines, but we can substitute ; for the ends of statements and put it all on one line. (Useful if we are just hacking away on the command line…)\n\n\nThis does the same as the above one\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do echo \"I like $LIKE.\"; done\n\nNote! Don’t put a semicolon after do.\n\n\n\n\n\n\nSelf study\n\n\n\nWe want to give the number of text lines, words, and characters from all the samtools_stats files.\nPrep: Here is a command that prints the name of each file.\n\n\nPaste this into the terminal\n\nfor FILE in data/samtools_stats/*.gz; do echo $FILE; done\n\nTask: I have added the -n option to the echo command which makes it not print a line ending. Your task is to replace YOUR_STUFF_HERE with an appropriate shell command to decompress each file and then print the number of lines, words, and characters in it:\n\n\nPaste this, edit YOUR_STUFF_HERE, and run it\n\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; YOUR_STUFF_HERE; done\n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nYour edited command line should look like this:\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; gzip -cd $FILE | wc; done",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#redirect-stdout-from-the-done",
    "href": "nmfs-bioinf/shell-prog.html#redirect-stdout-from-the-done",
    "title": "3  Shell Programming",
    "section": "3.8 Redirect stdout from the done",
    "text": "3.8 Redirect stdout from the done\nHere is something that is not always obvious: you can redirect or pipe the stdout of the whole for loop by using &gt; or | immediately after the done keyword.\nUsing the example from the self study above:\n\n\nPaste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do \n  echo -n $FILE; gzip -cd $FILE | wc; \ndone &gt; word_counts.txt\n\nNow look at what is in word_counts.txt.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#a-few-useful-topics-rapidly",
    "href": "nmfs-bioinf/shell-prog.html#a-few-useful-topics-rapidly",
    "title": "3  Shell Programming",
    "section": "3.9 A few useful topics, rapidly",
    "text": "3.9 A few useful topics, rapidly\n\n3.9.1 basename\n\nIf you have a path name to a file this_dir/that_dir/my_file3.txt, but you want to have the string for just the file name, my_file3.txt, you can use the basename command:\n\n\n\nTry these\n\nbasename this_dir/that_dir/my_file3.txt\nbasename ~/Documents/git-repos/CKMRpop/R/plot_conn_comps.R\n\n\n\n3.9.2 Capture stdout into a token to put on the command line\nThis is a pretty cool one, and is really nice if you want to capture a bit of output for use at a later time.\nBasically, if you run a command inside parentheses that are immediately preceded by a $, like $(command), then the stdout output of command gets put onto the command line as a single token.\nObserve:\n\n\nPaste this into your terminal\n\nSTART_TIME=$(date)\nsleep 3\nSTOP_TIME=$(date)\necho \"We started at $START_TIME, and finished at $STOP_TIME, and it is now $(date)\"\n\nOr even:\n\n\nPaste this into your terminal\n\nWCOUT=$(gzip -cd data/samtools_stats/s016_stats.tsv.gz | wc)\necho $WCOUT\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ndate: prints the current time and date to stdout.\nsleep: causes the shell to pause for however many seconds you tell it to, like sleep 3 for three seconds, sleep 180 for three minutes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#fancier-variable-substitution",
    "href": "nmfs-bioinf/shell-prog.html#fancier-variable-substitution",
    "title": "3  Shell Programming",
    "section": "3.10 Fancier variable substitution",
    "text": "3.10 Fancier variable substitution\n\n3.10.1 Wrap it in curly braces ${VAR}\nEspecially if you want to substitute a variable into a string adjacent to a letter or number or underscore, you can wrap it in curly braces.\n\n\nTry this\n\nsample=001\n# this works:\necho \"The sequences are in the file called ${sample}_seqs.fq.gz\"\n\n# this does not work the way you want it to\necho \"The sequences are in the file called $sample_seqs.fq.gz\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nWhy do you think the second echo line above produced the output that it did?\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nSince _ is a valid character for a variable name, $sample_seqs.fq.gz gets broken up by the shell as $sample_seqs plus .fq.gz, and there is no variable named sample_seqs, so the shell just substitutes an empty string into $sample_seqs.\n\n\n\n\n\n3.10.2 Variable modification while substituting it\nbash has a whole lot of tricky syntaxes for manipulating variable values when substituting them onto the command line.\nThe one I use more than any other is ${VAR/pattern/replacement}. This looks for a text pattern pattern in the variable VAR and replaces it with the text replacement.\nHere are some examples.\n\n\nFancy substitution fun. Paste into your terminal.\n\nfile=myfile.eps\necho ${file/eps/pdf}\n\n# or maybe you want to get the sample name, s007\n# out of a file path like data/samtools_stats/s007_stats.tsv.gz\npath=data/samtools_stats/s007_stats.tsv.gz\necho $(basename ${path/_stats.tsv.gz/})\n\nWhoa! On that last one we nested a ${//} inside a $()!\n\n\n\n\n\n\nHot tip!\n\n\n\nYou can use * the way that you might when globbing filenames on the command line in the pattern for variable substitution with ${var/pattern/replacement}:\n\n\nIf we want to extract just the s001 part...\n\nSTRING=\"A-whole-lot-of-junk-before-then_s001_and-a-whole-lot_of-other-garbage.63713973\"\n\n# remove all the garbage in the beginning\nN1=${STRING/*then_/}\n\n# see what we have at this point\necho $N1\n\n# remove the remaining junk off the end\nN2=${N1/_and-*}\n\n# see what we ended up with\necho $N2\n\n\n\n\n\n3.10.3 Grouping multiple commands with (...)\n\nSometimes it is convenient lump a number of commands together into a group.\nThe main reason I do this is to capture stdout or stderr from all of them into a single file with one redirect (as opposed to redirecting (&gt;) output from the first command, and then redirect-appending (&gt;&gt;) output from successive commands to the same place).\nWhen you wrap a series of commands in a pair of parentheses, they get executed as a group and you can redirect that from the right side of the last parenthesis:\n\n\n\nPaste this into your terminal\n\n(\n  echo \"This\"\n  echo \"that\"\n  echo \"and the\"\n  echo \"other\"\n) &gt; group_it.txt\n\nCheck out the result with cat group_it.txt.\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWith grouping parentheses, you can also redirect stderr to a single place even if there are pipes involved. Comparing to the Self-Study answer at the end of the Quick Unix Review session:\n\n\nA stderr example\n\n(gzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc) &gt; so.txt 2&gt;se.txt\n\nEven though the error happened with the gzip command, the error message gets relayed through to be redirected into se.txt.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#leaving-here-for-now",
    "href": "nmfs-bioinf/shell-prog.html#leaving-here-for-now",
    "title": "3  Shell Programming",
    "section": "3.11 Leaving here for now…",
    "text": "3.11 Leaving here for now…\nYou might be thinking, “Wow, could I use a for loop or something like it in bash to cycle over the lines of a text file to process each line in turn?”\nThe answer, is, “You can, but bash is not always the best tool for processing text files…especially if they are large.”\nThere is a Unix utility called awk that is much better for that.\nThat is where we are heading next.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Shell Programming</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html",
    "href": "nmfs-bioinf/awk-intro.html",
    "title": "4  A Brief awk Intro",
    "section": "",
    "text": "4.1 awk’s philosophy and basic syntax\nawk is a utility that:\nLet’s look through a file together. I will do:\nand we will discuss how awk sees such a file.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Brief `awk` Intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#awks-philosophy-and-basic-syntax",
    "href": "nmfs-bioinf/awk-intro.html#awks-philosophy-and-basic-syntax",
    "title": "4  A Brief awk Intro",
    "section": "",
    "text": "Takes text input from a file or from stdin\nIt automatically goes line-by-line. Treating each line of text as a separate unit.\nWhen it is focused on a line of text, it breaks it into fields which you can think of as columns.\nBy default, fields, are broken up on whitespace (spaces and TABs), with multiple spaces or TABs being treated as a single delimiter.\nYou can specify the delimiter.\n\n\n\n\nYou can do this if you want\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\n\n\n4.1.1 Naming fields\nOnce awk has read a line of text into memory, and split it into fields, you can access the value of those fields with special names:\n\n$1 refers to the first field (column)\n$2 refers to the second column.\n\nIf you are beyond the ninth column, the number has to be wrapped in parentheses:\n\n$(13) refers to the thirteenth column\n\n\n\n\n\n\n\nImportant\n\n\n\nThese variables within an awk script are not related to substituted variables in bash. They just happen to share a preceding $. But they are being interpreted by different programming languages (one by bash the other by awk).\n\n\n\n\n4.1.2 So what?\nSo far, this doesn’t seem very exciting, but now we learn that…\n\nYou can tell awk the sorts of lines you want it to pause at and then do some action upon it.\n\nYou tell awk which lines you want to perform an action on by matching them with a logical statement called a pattern in awk parlance.\n\nThis turns out to be incredibly powerful.\n\n\n4.1.3 awk syntax on the command line\nThe syntax for running awk with a script on the command line is like this:\n\n\nDon't try running this. It is just for explanation.\n\nawk '\n  pattern1  {action1}\n  pattern2  {action2}\n' file\n\nWhere file is the name of the file you want to process.\nOr, if you are piping text into awk from some command named cmd it would look like this:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk '\n  pattern1  {action1}\n  pattern2  {action2}\n'\n\nNote that you can pass options (like -v or -F) to awk. They go right after the awk and before the first single quotation mark.\nAlso, the carriage returns inside the single quotation marks are only there for easy reading. You could also write the last example as:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk 'pattern1  {action1} pattern2  {action2}'\n\n…which is great if you are hacking on the command line, but once you have a lot of pattern-action pairs, it gets harder to read.\n\n\n\n\n\n\nSelf-study\n\n\n\nThinking back to the previous session when we talked about the difference between grouping strings with ' vs with \", why do you think it is important that the awk script is grouped with '?\n\n\n\n\n\n\n\n\nBrief answer\n\n\n\n\n\nAs we saw, we will be referring to different fields like $8 within the awk script. If we used \" to group the script, the shell might try to do variable substitution on any $’s in there.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Brief `awk` Intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#enough-talking-lets-start-doing",
    "href": "nmfs-bioinf/awk-intro.html#enough-talking-lets-start-doing",
    "title": "4  A Brief awk Intro",
    "section": "4.2 Enough talking, let’s start doing",
    "text": "4.2 Enough talking, let’s start doing\nAll of this will make more sense with a few examples.\n\n4.2.1 Print all the lines in which the first field is SN\nFor our first foray, let’s just pick out and print a subset of lines from our samtools stats file:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '$1==\"SN\" {print}'\n\nThat is cool.\n\n\n\n\n\n\nSelf-study\n\n\n\nMake sure that you can identify the pattern and the action in the above awk script.\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nThe pattern is $1==\"SN\"\nThe action is print\n\nThis brings up the important point that the “is equals” operator in awk is == (two consecutive equals signs…just like in R)\n\n\n\n\n\n4.2.2 Printing the lines we are interested in\nHow about if we wanted to pick out just a few particular lines from there?\nWell, we can also match lines by regular expression (which you can think of as a very fancy form of Unix word-searching.)\nLet’s say that we want information on the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped using the (cigar) criterion.\nWe can see those lines in there. And we can target them by matching strings associated with them. The awk syntax puts these regular expressions in the pattern between forward slashes.\nSo, we want to match lines that have the first field equal to SN and also match other strings. We do that like this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print}\n  $1==\"SN\" && /reads properly paired:/ {print}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print}\n'\n\n\n\n\n\n\n\nBig Note:\n\n\n\nRegular expressions are lovely and wonderful, but occasionally frustrating. In awk’s case, parentheses have special meanings in the regular expressions, so we have to precede each one in the pattern with a backslash.\nRegular expressions are a bit beyond the scope of what we will be talking about today (entire books are devoted to the topic) but I encourage everyone to learn about them.\nThey are incredibly useful and they are used in multiple programming languages (R, python, perl, etc.)\n\n\n\n\n4.2.3 Printing just the values we are interested in\nThat is nice, but remember, we really just want to put those three values we are interested in into a table of sorts.\nSo, how do we print just the values?\nUse the fields! Count columns for each line and then print just that field:\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print $4}\n  $1==\"SN\" && /reads properly paired:/ {print $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print $5}\n'\n\n\n\n4.2.4 Use variables inside awk\nWe can also assign values to variables inside awk.\nThis lets us store values and then print them all on one line at the end. The special pattern END gives us a block to put actions we want to do at the very end.\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print rm, rpp, bmc}\n'\n\n\n\n\n\n\n\nFor the programmers out there\n\n\n\nVariables in awk are untyped and do not be declared. Basically you can put them anywhere. If code calls for the value from a variable that has not has anything assigned to it yet, the variable returns a 0 in a numerical context, and an empty string in a string context.\n\n\n\n\n4.2.5 That’s great. Can we add the sample name in there?\nYes! We can pass variables from the command line to inside awk with a -v var=value syntax.\nTo do this, we use some shell code that we learned earlier!\n\n\nStudy this, then paste this into your terminal\n\nFILE=data/samtools_stats/s001_stats.tsv.gz\ngzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print samp, rm, rpp, bmc}\n'\n\n\n\n4.2.6 OMG! Do you seen where we are going with this?\nWe can now take that whole thing and imbed it within a bash for loop cycling over values of FILE and get the table talked about wanting in our Motivating Example when we started.\n\n\nStudy this, then paste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nYowzers! That is pretty quick, and it sure beats opening each file, copying the values we want, and then pasting them into a spreadsheet.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Brief `awk` Intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "href": "nmfs-bioinf/awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "title": "4  A Brief awk Intro",
    "section": "4.3 Another example: the distribution of mapping qualities",
    "text": "4.3 Another example: the distribution of mapping qualities\nHere is a fun awk example that just came up a couple of days ago for me.\n\nA colleague was telling me that she has started filtering her whole-genome sequencing BAM files so that she does not use any reads that map with a mapping quality less than 30.\nThe hope is that this will lessen batch effects.\nQuestions:\n\nWhat is the distribution of mapping quality scores in my own data\nIf we imposed such a filter, how many reads would we discard?\n\n\nIt turns out that none of the samtools programs stat, idxstats, or flagstats provide that distribution.\nThere are some other more obscure software packages that provide it, but also a lot of convoluted python code on the BioStars website for doing it.\nHa! It’s quick and easy with awk! And a great demonstration of awk’s associative arrays.\n\n4.3.1 Let’s look at an example bam file\nWe have an example bam file in the repository at data/bam/s0001.bam.\nIt only has only about 25,000 read in it so that it isn’t too large.\nLet’s have a look at it with:\n\n\nPaste this into your terminal\n\nmodule load bio/samtools\nsamtools view data/bam/s001.bam | less -S\n\nThe module load bio/samtools line gives us access to the samtools program, which we need for turning BAM files into text-based SAM files that we can use. Once we have given it in our shell, we have that access until we close the shell. Much more on that tomorrow!\n\n\n\n\n\n\nIf that failed you might need to define your MODULEPATH\n\n\n\n\n\nHere we check to see if the bioinformatics paths on in the MODULEPATH:\n\n\nPaste this in your shell\n\necho $MODULEPATH | grep bioinformatics\n\nIf that command did not return anything to stdout, then you need to add a command to your ~/.bashrc that will add /opt/bioinformatics/modulefiles to your MODULEPATH. You can do that (as detailed in the Sedna Google Doc) like this:\n\n\nIf you need to, paste this into the shell\n\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' &gt;&gt; ~/.bashrc\n\nAnd when you are done with that, reload your .bashrc to get the change to take effect:\n\n\nIf you need to, paste this into the shell\n\nsource ~/.bashrc\n\n\n\n\nThe mapping quality is in the 5th field. It is a number that ranges from 0 to 60. We can count up how many times each of those numbers occurs using awk.\n\n\nHere it is all on one line as I wrote it\n\nsamtools view data/bam/s001.bam | awk 'BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} {n[$5]++} END {for(i in n) tot+=n[i];  for(i=60;i&gt;=0;i--) {sum+=n[i]; print i, n[i], sum/tot;}}'\n\nAnd\n\n\nHere it is broken across lines. Paste that in your shell.\n\nsamtools view data/bam/s001.bam | awk '\n  BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} \n  {n[$5]++} \n  END {for(i in n) tot+=n[i];  for(i=60;i&gt;=0;i--) {sum+=n[i]; print i,n[i],sum/tot;}}\n'\n\nIt’s really compact and requires very little memory to do this. (You couldn’t read a whole BAM file into R and hope to deal with it).\n\n\n\n\n\n\nAll arrays in awk are associative arrays\n\n\n\nIf you come from an R programming background, you will typically think of arrays as vectors that are indexed from 1 to n, where n is the length of the vector.\nThis is not how arrays are implemented in awk. Rather all arrays are associative arrays, which are also called hash arrays, or, in Python dictionaries. Or, if you are familiar with R, you can think of an associative array as an array that has elements that can only be accessed via their names attribute, rather than by indexing them with a number.\nSo, in awk, if we write:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[\"this\"] = \"Boing!\"\n\nThis will create an array called var (if one does not already exist) and then it will set the value of element in var that is associated with the string \"this\" to the string \"Boing!\".\nAt the same time, if you do this:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[30] = 67\n\nthen we are not assigning the value of 67 to the 30-th element of var. Rather, we are assigning the value 67 to the element of var that is associated with the string \"30\".\nIt can take a little getting used to, but it is very useful for counting things.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Brief `awk` Intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#wrap-up",
    "href": "nmfs-bioinf/awk-intro.html#wrap-up",
    "title": "4  A Brief awk Intro",
    "section": "4.4 Wrap-Up",
    "text": "4.4 Wrap-Up\nThat was just a brief whirlwind tour of how one can use bash and awk together to automate tasks that come up on an everyday basis when doing bioinformatics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Brief `awk` Intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#looking-toward-tomorrow",
    "href": "nmfs-bioinf/awk-intro.html#looking-toward-tomorrow",
    "title": "4  A Brief awk Intro",
    "section": "4.5 Looking toward tomorrow",
    "text": "4.5 Looking toward tomorrow\nThe bulk of day #2 is going to be focused on working within a cluster environment, and specifically on using SLURM for launching jobs on the Sedna cluster.\nTo prepare for tomorrow, please be sure to read Chapter 8 from beginning and up to and including section 8.2. This is just a small bit to read, but it should set you up for an understanding of why and how computing clusters work differently than your desktop machine when it comes to allocating resources for computation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A Brief `awk` Intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html",
    "href": "nmfs-bioinf/scripts-and-functions.html",
    "title": "5  Bash scripts and functions",
    "section": "",
    "text": "5.1 Prepare for this\nSync your fork of the repository. Then, make sure that you have all the latest updates from the repository by pulling them down with git. use git to pull down any new changes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bash scripts and functions</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#prepare-for-this",
    "href": "nmfs-bioinf/scripts-and-functions.html#prepare-for-this",
    "title": "5  Bash scripts and functions",
    "section": "",
    "text": "Use something like this in to be sure you have the most up-to-date resources\n\ncd YOUR-CLONE-OF-YOUR-FORK-OF-THE-REPO\ngit pull origin main",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bash scripts and functions</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#bash-shell-scripts",
    "href": "nmfs-bioinf/scripts-and-functions.html#bash-shell-scripts",
    "title": "5  Bash scripts and functions",
    "section": "5.2 Bash shell scripts",
    "text": "5.2 Bash shell scripts\nWe have been doing all of our bash scripting by writing commands on the command line.\nUseful bash code can be stored in a text file called a script, and then run like a normal Unix utility.\nTo illustrate this, we will copy our samtools-stats-processing commands from before into a file using the nano text editor.\nAt your command line, type this:\n\n\nType this at the command line\n\nnano sam-stats.sh\n\nThis opens a file called sam-stats.sh with a text editor called nano.\nThe convention with bash shell scripts is to give them a .sh extension, but this is not required.\nNow we copy our commands into nano.\n\n\nCopy this onto your clipboard and paste it into nano in your terminal\n\n#!/bin/bash\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nThen:\n\ndo cntrl-X\nAnswer Y when asked if you want to save the file\nHit return to save the file as sam-stats.sh\n\nVoila! That should exit the nano editor, and now you have a script containing that bash code.\nCheck out its contents with:\n\n\nType this at the command line\n\ncat sam-stats.sh\n\n\n\n\n\n\n\nWhat’s this #! at the top of the file?\n\n\n\nThat is colloquially referred to as the shebang line.\nA # usually tells the shell to “ignore everything to the right of the # on this line”\n# is used to precede comments in your code.\nHowever, in this case, at the top of the file and followed by a ! it tells the computer what language to use to interpret this file.\nIn our case /bin/bash is where the bash shell interpreter typically is found on most Unix or Linux system.\n(If bash is the default shell on your system, you may not always need to have the shebang line, but it is good practice to do so.)\n\n\n\n5.2.1 Script files must be executable\nThe Unix operating system distinguishes between files that just hold data, and files that can the run or be “executed” by the computer.\nFor your bash commands in a script to run on the computer, it must be of an executable type.\nWe can make the file executable using the chmod command, like this:\n\n\nPaste this into your shell\n\nchmod u+x sam-stats.sh\n\nIf we then use ls -l to list the file in long format like this:\n\n\nType this in\n\nls -l sam-stats.sh\n\nwe see:\n-rwxrw-r-- 1 eanderson eanderson 317 Oct 14 13:40 sam-stats.sh\nThe x in the first field of that line indicates that the file is executable by the user.\n\n\n5.2.2 Running a script\nTo run a script that is executable, you type the path to it.\nOn some clusters, by default, the current working directory is not a place the computer looks for executable scripts, so we have to prepend ./ to its path:\n\n\nType this on the command line and hit RETURN.\n\n./sam-stats.sh\n\nThat runs our script.\n\n\n5.2.3 Scripts are more useful if you can specify the inputs\nSo, that runs our script and produces results, but that is not so useful. We already had those results, in a sense.\nShell scripts become much more useful when you can change the inputs that go to them.\nOne way to do so involves using positional parameters\n\n\n5.2.4 Arguments following a script can be accessed within the script\nIf you put arguments after a script on the command line, for example like:\n\n\nDon't paste this in anywhere\n\nscript.sh arg1 some_other_arg  And_another_arg\n\nthen in the script itself:\n\nthe value of the first argument (arg1) is accessible as $1\nthe value of the second argument (some_other_arg1) is accessible as $2\nthe value of the third argument (And_another_arg) is accessible as $3\n\n…and so forth for as many arguments as you want.\nSo, we can rewrite our script as:\n\n\nJust give this a read\n\n\n#!/bin/bash\n\nfor FILE in $1; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nSee that we have replaced data/samtools_stats/*.gz with $1.\nEdit sam-stats.sh to look like the above. Now we can use it like this:\n\n\nPaste this into the shell\n\n./sam-stats.sh \"data/samtools_stats/s0*.gz\"\n\nHere, we have passed in \"data/samtools_stats/s0*.gz\" as the first argument to the script, and, since we have a $1 in the script where that goes, it does what it did before.\n\n\n5.2.5 Now we can use it on other samtools stats files\nIt is not very exciting to see it just run again on the same set of files.\nBut, now we could direct the script to operate on a different set of files, just by changing the argument that we pass to the script.\nI have put a much larger set of samtools stats files within subdirectory of the /share directory on Sedna. You should be able to list them all with:\n\n\nPaste this into your shell. This should work...\n\n ls /share/all/eriq/big_sam_stats/s*.gz\n\nWhoa! 275 files. (They are here on Sedna, in the shared folder, because I didn’t want to put them all on GitHub.)\nBut, now, we can summarize them all just by pointing our script to them:\n\n\nThis should work on Sedna, Paste it into your shell\n\n./scripts/sam-stat-pp.sh \"/share/all/eriq/big_sam_stats/s*.gz\"\n\nThat is fast.\nIf we wanted to redirect that into a file we could do so\n\n\n\n\n\n\nFun Tip! – Record the positional parameters passed to a script.\n\n\n\n\n\nSometimes it is nice to know and record the values of all the arguments passed to a script that you have written. This can be done by adding something like this to your script:\necho \"Script $0 started at $(date) with arguments $* \" &gt; /dev/stderr\nIn that:\n\n$0 is the path to the script\n$(date) puts the current date and time in the line\n$* expands to a single string with all the arguments passed to the script\n&gt; /dev/stderr redirects stdout from echo to stderr\n\nSo, for the last script, that might look like:\n#!/bin/bash\n\necho \"Script $0 started at $(date) with arguments $* \" &gt; /dev/stderr\n\nfor FILE in $1; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bash scripts and functions</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#bash-functions",
    "href": "nmfs-bioinf/scripts-and-functions.html#bash-functions",
    "title": "5  Bash scripts and functions",
    "section": "5.3 Bash functions",
    "text": "5.3 Bash functions\nLike most programming languages, you can define functions in bash.\nThe syntax for defining a function named MyFunc is:\n\n\nJust read this\n\nfunction MyFunc {\n  ...code that MyFunc executes...\n}\n\nIn this regard, it has a similar syntax to R.\nOnce defined, you can use the name of the function (in the above case, MyFunc) as if it were just another Unix command.\n\n5.3.1 Example: congratulate yourself on making it through your day\nHere is a gratuitous example: we make a bash function called congrats that tells us what time it is and encourages us to keep getting through our day:\n\n\nPaste this into your terminal\n\nfunction congrats { echo \"It is now $(date).  Congrats on making it this far in your day.\"; }\n\n\n\n\n\n\n\nWarning\n\n\n\nCurly braces in bash are extremely finicky. They don’t like to be near other characters. In the above, the space after the { is critical, as is the ; before the }. (The last } needs to have a ; before it if it does not have a line ending before it).\n\n\nNow, you can just type congrats at the command line:\n\n\nType this on the command line\n\ncongrats\n\n\n\n5.3.2 Bash functions take positional parameters too\nWe can rewrite our function as congrats2 so that it can use two arguments, the first a name, and the second an adjective:\n\n\nPaste this into your terminal\n\nfunction congrats2 { \n  echo \"It is now $(date).  Congrats, $1, you are $2.\"\n}\n\nNow, you can use that function and supply it with whatever names and adjectives you would like:\n\n\nPaste these in and see what it does.\n\ncongrats2 Fred splendid\ncongrats2 Eric tired\ncongrats2 Amy amazing\n\nWe will write a few functions, later, to simplify our life in SLURM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bash scripts and functions</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html",
    "href": "nmfs-bioinf/slurm.html",
    "title": "6  Sedna and SLURM intro",
    "section": "",
    "text": "6.1 Why do we need SLURM?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#why-do-we-need-slurm",
    "href": "nmfs-bioinf/slurm.html#why-do-we-need-slurm",
    "title": "6  Sedna and SLURM intro",
    "section": "",
    "text": "The fundamental problem of cluster computing.\nA cluster does not operate like your laptop.\nMost compute-intensive jobs run most efficiently on a dedicated processor or processors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#hpcc-architecture-in-a-nutshell",
    "href": "nmfs-bioinf/slurm.html#hpcc-architecture-in-a-nutshell",
    "title": "6  Sedna and SLURM intro",
    "section": "6.2 HPCC Architecture in a Nutshell",
    "text": "6.2 HPCC Architecture in a Nutshell\n\n\nNodes: the closest thing to what you think of as a computer. (“Pizza-boxes” with no displays).\n\nEach node is attached via a fast connection to centralized attached storage (A big set of hard drives attached via “Infiniband.”)\nWithin each node are some numbers of Cores or CPUs\n\nCores/CPUs are the actual processing units within a node. (Usually 20 to 24)\n\n\nSedna, like almost all other HPCCs has a login node\n\nThe login node is dedicated to allowing people to communicate with the HPCC.\nDO NOT do computationally intensive, or input/output-intensive jobs on the login node\nNot surprisingly, when you login to Sedna you are on the login node.\n\n\n\n\n\n\n\n\nHot tip!\n\n\n\nIn the default configuration on Sedna, your command prompt at the shell tells you which node you are logged into. The default command prompt looks like:\n[username@computer directory]\nSo, for example, mine at the moment is:\n[eanderson@sedna playground]$\nWhich tells me that I am user eanderson and I am logged in to sedna in the directory whose basename is playground.\nThe login node for Sedna is named sedna. So, this is telling me that I am logged into the login node of Sedna.\nIf you don’t have such an informative prompt…\nOn Alpine, I am not sure what the default command prompt is, but you can always change yours by setting the PS1 variable. Try doing:\nexport PS1='[\\h: \\W]--% '\nThat will give you a command prompt with the hostname (\\h) and the working directory basename (\\W), which is helpful. Mine, right now, looks like:\n[login11: con-gen-csu]--%\nOf course, you can put the export PS1='[\\h: \\W]--% ' into your ~/.bashrc file and then always have an informative prompt.\n\n\n\n\n\n\n\n\nSelf study\n\n\n\nInvestigate your own command prompt and make sure you understand what the different parts in it mean.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#say-it-again-dont-do-heavy-computing-on-the-login-nodes",
    "href": "nmfs-bioinf/slurm.html#say-it-again-dont-do-heavy-computing-on-the-login-nodes",
    "title": "6  Sedna and SLURM intro",
    "section": "6.3 Say it again: Don’t do heavy computing on the login nodes!",
    "text": "6.3 Say it again: Don’t do heavy computing on the login nodes!\nIf you run a big job that is computationally intensive on the login node, it can interfere with other people being able to access the cluster.\nDon’t do it!!\nThat is all fine and well, but how do we avoid computing on the login nodes?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#do-your-computation-on-the-compute-nodes",
    "href": "nmfs-bioinf/slurm.html#do-your-computation-on-the-compute-nodes",
    "title": "6  Sedna and SLURM intro",
    "section": "6.4 Do your computation on the compute nodes",
    "text": "6.4 Do your computation on the compute nodes\n\nTo run jobs on the compute node you need to ask SLURM to give you “compute resources” to run the job.\nThe basic axes of “compute resources” are:\n\nThe number of cores to use.\nThe maximum amount of RAM memory you might need.\nThe amount of time that you will need those resources for.\n\nTo prepare for the next part of the course on SEDNA, each one of us is going to “check out” 2 cores for 3 hours for interactive work.\n\nInteractive here means that we will have a Unix shell that has access to compute power on the cores that we checked out.\nHere is the command to checkout 2 cores for 3 hours for interactive use:\n\n\n\nPaste this into your shell on SEDNA\n\nsrun -c 2 -t 03:00:00 --pty /bin/bash\n\nIf you are working on ALPINE then you can get on a compute node differently:\n\nLoad the slurm/alpine module\nRequest a shell on the oversubscribed atesting partition.\nThe commands for that look like:\n\n\n\nPaste this into your shell on ALPINE\n\nmodule load slurm/alpine\nsrun --partition atesting --pty /bin/bash\n\n\n\n\n\n\n\n\nSelf study:\n\n\n\nHave a look at your command prompt now. If you are on SEDNA, it should show that you are logged in to nodeXX where XX is a number like 04 or 33. If you are on Alpine, then the node you are on will be named something more cryptic, like c3cpu-a5-u34-3.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#what-does-sedna-have-under-the-hood",
    "href": "nmfs-bioinf/slurm.html#what-does-sedna-have-under-the-hood",
    "title": "6  Sedna and SLURM intro",
    "section": "6.5 What does Sedna have under the hood?",
    "text": "6.5 What does Sedna have under the hood?\nSedna is not an outrageously large cluster, but it is well-maintained and provides a lot of computing power.\nThe nodes in Sedna are broken into three different partitions.\nA partition is a collection of nodes that tend to be similar.\n\nnode partition: This is a collection of 36 nodes:\n\nnode01 – node28: “standard compute nodes”\nnode29 – node36: “standard compute nodes with twice as much memory”\n\nhimem partition: Four nodes with a boatload of memory for high-memory jobs\n\nhimem01 – himem04\n\nbionode partition: legacy machines from an older NWFSC cluster\n\nI’m not sure if all of us have access to this.\nWhen I check, it seems like a lot of the nodes are down, occasionally.\nBut I have been able to checkout resources on it.\nMight be an option, but it doesn’t perform at the same level as the node partition",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#what-does-alpine-have-under-the-hood",
    "href": "nmfs-bioinf/slurm.html#what-does-alpine-have-under-the-hood",
    "title": "6  Sedna and SLURM intro",
    "section": "6.6 What does ALPINE have under the hood?",
    "text": "6.6 What does ALPINE have under the hood?\nAlpine is a pretty darn large cluster, with a boatload of different machines, many of them with 64 cores. It also has some GPU machines, etc.\nWe will have a look at it with the commands, like sinfo described next.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#ask-slurm-about-what-nodes-are-available-and-how-busy-they-are",
    "href": "nmfs-bioinf/slurm.html#ask-slurm-about-what-nodes-are-available-and-how-busy-they-are",
    "title": "6  Sedna and SLURM intro",
    "section": "6.7 Ask SLURM about what nodes are available, and how busy they are",
    "text": "6.7 Ask SLURM about what nodes are available, and how busy they are\nWhen I login to Sedna, before launching and jobs or working on anything, I always like to get a summary of how hard Sedna is currently working.\nFor this we have the sinfo command:\n\n\nType this at your command prompt\n\nsinfo\n\nThe answer will look somethign like this:\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnode*        up   infinite      5    mix node[01,29-31,33]\nnode*        up   infinite      1  alloc node32\nnode*        up   infinite     30   idle node[02-28,34-36]\nhimem        up   infinite      1    mix himem01\nhimem        up   infinite      3   idle himem[02-04]\nbionode      up   infinite      8  down* bionode[10-11,13-18]\nbionode      up   infinite      1  drain bionode12\nbionode      up   infinite     10   idle bionode[01-09,19]\nThis is a hugely informative summary for being so compact.\n\nThe output is sorted by partition.\nWithin each partition it tells us how many nodes are in different states:\nThe main states:\n\nidle: just sitting there with all cores available for checkout,\nmix: some, but not all, cores are checkout out and in use,\nalloc: all the cores on the node are checkout out and in use.\n\nOther states you might see:\n\ndown: node is unavailable because it is not working properly,\ndrain: node is not available because the sys-admins are not letting any new jobs to start because they are going to be working on the system soon, etc.\n\nThe node partition is starred node* because it is the default partition.\nNotation like node[01,29-31,33] gives the specific node numbers, compactly.\n\nIf you type sinfo on ALPINE you get a similar output, but it is much larger and more complex because there are more partitions and more, different nodes in each partition.\nThe CURC lists the partitions on a web page here.\nIf you are going to be working on Alpine, it is worth reading over the entire CURC documentation for it, starting from here.\n\n\n\n\n\n\nSelf-study\n\n\n\nReview how many nodes are currently in use.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#more-detailed-sinfo-output",
    "href": "nmfs-bioinf/slurm.html#more-detailed-sinfo-output",
    "title": "6  Sedna and SLURM intro",
    "section": "6.8 More detailed sinfo output",
    "text": "6.8 More detailed sinfo output\nSometimes you want more information about what is going on with the cluster. sinfo can do that, too, but the syntax is hard to remember and even harder to type.\nHere is a bash function we will define called slinfo for (s-long-info) that uses sinfo to provide more detailed output.\n\n\nPaste this into your shell\n\nfunction slinfo {\n  sinfo -N -O nodelist,cpusstate,memory,allocmem\n}\n\nOnce you have done that:\n\n\nType this at your command prompt\n\nslinfo\n\nThis gives information about each node.\n\nIn CPUS(A/I/O/T):\n\nA = allocated\nI = idle\nO = offline\nT = total\n\nMEMORY is total RAM available on the node (in Megabytes)\nALLOCMEM is the total RAM allocated to jobs.\n\nThis view shows each node just once. But it might be useful to also see what partition(s) each of those nodes are in (especially on ALPINE). So, here is a quick function to do that:\n\n\nPaste this into your shell\n\nfunction slinfop {\n  sinfo -N -O nodelist,partition,cpusstate,memory,allocmem\n}\n\nOnce you have done that:\n\n\nType this at your command prompt\n\nslinfop\n\nNote that many nodes are included in multiple partitions.\nSince the main partition on Alpine for general computing is amilan you can look specifically at nodes in that partition with:\n\n\nCopy this into your command prompt\n\nslinfop | awk '$2~/amilan/'\n\n\n\n\n\n\n\nALPINE Self-study\n\n\n\nQuickly scan the output of the above command and estimate the number of idle (i.e. available) cores in the amilan partition.\nThen, read about the amilan partition (here)[https://curc.readthedocs.io/en/stable/clusters/alpine/alpine-hardware.html#partitions]. In particular, note how much memory is available per core, and the default and max wall times.\nWe will discuss this.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#summarize-what-the-cluster-is-doing-by-job",
    "href": "nmfs-bioinf/slurm.html#summarize-what-the-cluster-is-doing-by-job",
    "title": "6  Sedna and SLURM intro",
    "section": "6.9 Summarize what the cluster is doing by job",
    "text": "6.9 Summarize what the cluster is doing by job\nSLURM keeps track of individual jobs that get submitted.\nFor SLURM, each job is basically a request for resources.\nIf SLURM finds the requested resources are available, it provides the resources and starts the job.\nIf resources are not available because other jobs are running, the reqested job enters the “queue” in a WAITING state.\nWe can see how many jobs are running and how many are waiting, by using the SLURM squeue command:\n(By the way, notice the pattern? All SLURM command start with an s).\n\n\nType this at your command prompt\n\nsqueue\n\nThis tells us a little about all the jobs that are currently allocated resources or are waiting for resources.\nOne thing to note: - Every job is assigned a SLURM_JOB_ID. Which is a unique integer (that gets assigned successively). - And jobs might have a job NAME, that is assigned by the user, which is the name of the script that the job runs, or is defined in the SBATCH directives (we will talk about that later!)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#i-find-the-standard-default-squeue-output-sorely-lacking",
    "href": "nmfs-bioinf/slurm.html#i-find-the-standard-default-squeue-output-sorely-lacking",
    "title": "6  Sedna and SLURM intro",
    "section": "6.10 I find the standard, default, squeue output sorely lacking",
    "text": "6.10 I find the standard, default, squeue output sorely lacking\nThe standard way that squeue presents information is not super informative, especially if the job names are not very short, or if you are interested in how many cores (not just nodes) each job involves.\nOnce again, we make a bash function, alljobs, that runs squeue but provides more information.\n\n\nPaste this at your command prompt\n\nfunction alljobs {\n  if [ $# -ne 1 ]; then\n    JL=10;\n  else\n    JL=$1;\n  fi;\n  squeue -o  \"%.12i %.9P %.${JL}j %.15u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L\";\n}\n\nAlso, if you follow the command with an integer like 20, that is how many spaces the output will allow for the job NAME.\nFor example, try:\n\n\nType this at the command prompt\n\nalljobs 30\n\nThis is kind of fun to see the full names of the jobs.\n\n\n\n\n\n\nHot Tip!\n\n\n\nIt is worth taking special note of the columns, NODELIST(REASON) and CPUS:\n\nNODELIST(REASON): If the job is running, then this gives the name(s) of the node(s) upon which it is running. If it is not running, it says why it is not yet running. The most typical reason it:\n\nPriority: Too many other users are already using the cluster. Your job is waiting for resources to become available.\n\nCPUS: the number of CPUs the job is using\n\nIt is worth noting that, when I look at alljobs on Alpine, most of the jobs that are waiting due to Priority are jobs that are requesting all or most of the CPUs (like 64 or 32) on a single node. Many of the jobs that are not waiting are those that are requesting only one or a few cores.\nIn general, if you can break your workflows down into a small chunks that run on just one or a few cores, you have a better chance of getting compute time, than if you request all the cores on a node.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#just-tell-me-about-my-jobs",
    "href": "nmfs-bioinf/slurm.html#just-tell-me-about-my-jobs",
    "title": "6  Sedna and SLURM intro",
    "section": "6.11 Just tell me about my jobs!",
    "text": "6.11 Just tell me about my jobs!\nWe can make a similar function that only tells us about our own jobs that are running or waiting in the queue:\n\n\nPaste this into your terminal\n\nfunction myjobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -u $(whoami) -o  \"%.12i %.9P %.${JL}j %.10u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L %p\";\n}\n\nYou can run that the same way as alljobs, and you should see the 2-core job you started with srun:\n\n\nType this into your shell\n\nmyjobs",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#finding-software-on-a-cluster",
    "href": "nmfs-bioinf/slurm.html#finding-software-on-a-cluster",
    "title": "6  Sedna and SLURM intro",
    "section": "6.12 Finding Software on a Cluster",
    "text": "6.12 Finding Software on a Cluster\n\nAnalysis software is not automatically available on a cluster\nOn many clusters you have to install it yourself\nOn Sedna, we are very lucky to have a great support crew that will install software for us, and also make it available for everyone else to use.\n\nHowever, software can be complicated:\n\nSome software might conflict with other software.\nSome users might want different software versions\n\nThe solution: maintain software (and its dependencies) in (somewhat) isolated modules.\n\n\n\n\n\n\nBioinformatic modules on Sedna are only available if the MODULEPATH is specified\n\n\n\nHere we check to see if the bioinformatics paths are in the MODULEPATH:\n\n\nPaste this in your shell\n\necho $MODULEPATH | grep bioinformatics\n\nIf that command did not return anything to stdout, then you need to add a command to your ~/.bashrc that will add /opt/bioinformatics/modulefiles to your MODULEPATH. You can do that (as detailed in the Sedna Google Doc) like this:\n\n\nIf you need to, paste this into the shell\n\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' &gt;&gt; ~/.bashrc\n\nAnd when you are done with that, reload your .bashrc to get the change to take effect:\n\n\nIf you need to, paste this into the shell\n\nsource ~/.bashrc\n\n\n\nMost modules on Alpine are only available if you are on a compute node.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#example-lets-try-running-samtools",
    "href": "nmfs-bioinf/slurm.html#example-lets-try-running-samtools",
    "title": "6  Sedna and SLURM intro",
    "section": "6.13 Example: Let’s try running samtools",
    "text": "6.13 Example: Let’s try running samtools\nsamtools is a common bioinformatic utility.\nTypically, if you run it with no arguments, it gives you some usage information.\nHowever, on Sedna, unless you have installed samtools yourself, or have set your environment up to always have it available, when you type samtools at the shell, you will get a response saying that it is not available:\n\n\nType this at the command prompt\n\nsamtools\n\nThe response I get from the computer is:\nbash: samtools: command not found...\nYou will likely get that response, too.\nOn Alpine, you could install samtools via mamba or you could see if it is available in the system modules.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#check-which-software-programs-are-available-in-the-modules",
    "href": "nmfs-bioinf/slurm.html#check-which-software-programs-are-available-in-the-modules",
    "title": "6  Sedna and SLURM intro",
    "section": "6.14 Check which software programs are available in the modules",
    "text": "6.14 Check which software programs are available in the modules\nThe command module and its subcommands let you interact with the software modules.\nmodule avail tells you what software is available in the modules.\n\n\nType this into your shell\n\nmodule avail\n\nOn SEDNA, there is a lot of tasty bioinformatics software there. On Alpine, there is now a dedicated “Bioinformatics” section in the output of module avail, which is a nice change from Alpine’s predecessor, SUMMIT, which was mostly loaded up with software for geoscientists.\nThe numbers are the version numbers. The (D) after some of them tells us that version is the default.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#load-the-samtools-module",
    "href": "nmfs-bioinf/slurm.html#load-the-samtools-module",
    "title": "6  Sedna and SLURM intro",
    "section": "6.15 Load the samtools module",
    "text": "6.15 Load the samtools module\nWe will load the module for version 1.15.1 of samtools. That can be done like this:\nSEDNA\n\n\nOn SEDNA, Paste this into your shell\n\nmodule load bio/samtools/1.15.1\n\nAlpine\n\n\nOn Alpine, Paste this into your shell\n\nmodule load samtools\n\n(using the load subcommand of module)\n\n\n\n\n\n\nAbout the default versions\n\n\n\nThe same would have been achieved with\nmodule load bio/samtools\nbecause 1.15.1 is the default samtools version.\nNonetheless, it is typically best from a reproducibility standpoint to be explicit about version numbers of software you are using.\n\n\nNow, that it is loaded, you can run the samtools command and get the usage message.\n\n\nType this at your shell\n\nsamtools\n\n\n\n\n\n\n\nInventory and Unload modules\n\n\n\nTo know which modules you have loaded:\nmodule list\nTo unload all the modules you have loaded\nmodule purge\nThere are many more module commands, but those are the only ones that you really need to know.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#two-notes-about-modules",
    "href": "nmfs-bioinf/slurm.html#two-notes-about-modules",
    "title": "6  Sedna and SLURM intro",
    "section": "6.16 Two notes about modules",
    "text": "6.16 Two notes about modules\n\n\n\n\n\n\nLoaded modules are only active in the current session\n\n\n\nIf you open a new shell, or allocate an interactive session on a new core, or logout and log back in again…\n…You will have to reload the modules that you need.\nSo: Any time you write a script that needs some software, you should load the module for that software in the initial lines of the script.\n\n\n\n\n\n\n\n\nModules are not always portable\n\n\n\nNot every cluster that you use will have the same modules set up in the same way. (Though it does seem that Giles has set the modules up on Sedna according to the accepted best practices!).\nSo, if you use code you developed for Sedna (or Alpine) on another cluster, you might have to modify things to ensure that software is available.\nThe conda/mamba package manager is an option if you are going to be running your scripts on different clusters.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-sedna",
    "href": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-sedna",
    "title": "6  Sedna and SLURM intro",
    "section": "6.17 If you need more software or newer versions on Sedna",
    "text": "6.17 If you need more software or newer versions on Sedna\nYou can request software installations by using the Sedna work request form",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-alpine",
    "href": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-alpine",
    "title": "6  Sedna and SLURM intro",
    "section": "6.18 If you need more software or newer versions on Alpine",
    "text": "6.18 If you need more software or newer versions on Alpine\nYou probably can request it from the sys admins, but you will almost indubitably have your required software up and running faster if you install it with mamba.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#before-we-go-make-our-convenience-functions-available-whenever-we-are-on-our-cluster",
    "href": "nmfs-bioinf/slurm.html#before-we-go-make-our-convenience-functions-available-whenever-we-are-on-our-cluster",
    "title": "6  Sedna and SLURM intro",
    "section": "6.19 Before we go, make our convenience functions available whenever we are on our cluster",
    "text": "6.19 Before we go, make our convenience functions available whenever we are on our cluster\nWe defined four convenience functions in bash: slinfo, slinfop, alljobs, and myjobs.\nOnce we log out of the current session, or login to a different session, those convenience functions will be lost.\nTo make sure that they are accessible the next time we login, we will put them into our own .bashrc files.\nThe .bashrc file is a file in your home directory in which you can put function definitions and other configurations that will be applied whenever you open a new bash shell.\nTo add our four convenience functions to your .bashrc file, first start editing your ~/.bashrc file with nano.\n\n\nPaste this into your shell\n\nnano ~/.bashrc\n\nThen come back to this web page and copy the following text:\n\n\nCopy this onto your clipboard\n\nfunction slinfo {\n  sinfo -N -O nodelist,cpusstate,memory,allocmem\n}\n\nfunction alljobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -o  \"%.12i %.9P %.${JL}j %.15u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L\";\n}\n\nfunction myjobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -u $(whoami) -o  \"%.12i %.9P %.${JL}j %.10u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L %p\";\n}\n\nThen paste those function definitions into the editor in your .bashrc file (before your conda/mamba block if you have one), and then do cntrl-X, Y, RETURN to save the file and get out of nano.\nNow, the next time you login you will have all those functions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#bonus-thoughts-what-are-other-people-doing",
    "href": "nmfs-bioinf/slurm.html#bonus-thoughts-what-are-other-people-doing",
    "title": "6  Sedna and SLURM intro",
    "section": "6.20 Bonus Thoughts — What are other people doing?",
    "text": "6.20 Bonus Thoughts — What are other people doing?\nEspecially when you are working on a smaller workgroup cluster and you are wondering if other users are using it efficiently, there are a few ways you can check up on what they are doing.\nUsing alljobs you can see what other people are doing. Then you can ssh to the node that they are running jobs on, and then do ps -auxww | grep user (where user is their username) and that will show the currently running command lines. This can be useful, for example, to see if their jobs are multithreaded while they are using many cores or not.\nYou can also get information about failures on different nodes with dmesg -T.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sedna and SLURM intro</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html",
    "href": "nmfs-bioinf/sbatch.html",
    "title": "7  Submitting jobs with sbatch",
    "section": "",
    "text": "7.1 Prepare for this session\nGet on your cluster and navigate to the top level (con-gen-csu) of your fork of the class repository. Then, to make sure that you have all the latest updates from the repository, sync the main branch of your fork with the main branch of eriqande/con-gen-csu on GitHub, then in your shell, use git switch main to make sure you are on the main branch of your fork, and then use git pull origin main to pull down those changes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#the-sbatch-command-and-its-options",
    "href": "nmfs-bioinf/sbatch.html#the-sbatch-command-and-its-options",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.2 The sbatch command and its options",
    "text": "7.2 The sbatch command and its options\nIf you do man sbatch you will see an insanely large number of options and all sorts of complicated stuff.\nYou can get by with a fairly minimal set of options for almost everything you need to do. They are:\n\n--cpus-per-task=&lt;n&gt;: the number of cores to use for the job. The syntax has you using it like this --cpus-per-task=2\n\nOn Sedna, the default is 1.\n\n--mem=&lt;size[units]&gt;: How much total memory for the job. Example: --mem=4G\n\nOn Sedna, the default is about 4.7 Gb for each requested core.\nOn Alpine’s amilan partition, the machines have 3.74 Gb per core.\n\n--time=[D-]HH:MM:SS: How much time are you requesting? You don’t have to specify days, so you could say, --time=1-12:00:00 for one day and twelve hours, or you could say --time=36:00:00 for 36 hours.\n\nOn Sedna, the default is 8 hours.\n\n--output=&lt;filename pattern&gt;: Where should anything on stdout that is not otherwise redirected be written to?\n--error=&lt;filename pattern&gt;: Where should anything on stderr that is not otherwise redirected be written to?\n\nOn top of the options, sbatch takes a single required argument, which must be the path to a shell script (we know about those!) that the job will run.\n\n\n\n\n\n\nFun fact:\n\n\n\nIf you pass any arguments after the name of the shell script that you want sbatch to execute, those are interpreted as arguments to the shell script itself.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#what-an-invocation-of-sbatch-could-look-like",
    "href": "nmfs-bioinf/sbatch.html#what-an-invocation-of-sbatch-could-look-like",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.3 What an invocation of sbatch could look like",
    "text": "7.3 What an invocation of sbatch could look like\nSo, if we wanted to schedule a script called MyScript.sh to run with 4 cores and memory of 80Gb, with an allowance of 16 hours, and we wanted to tell SLURM where to capture any otherwise un-redirected stdout and stderr, we would type something like this:\n\n\nDon't bother copying or pasting this.\n\nsbatch --cpus-per-task=4 --mem=80G --time=16:00:00 --output=myscript_stdout --error=myscript_error MyScript.sh\n\nSome points about that:\n\nTyping all of that is a huge hassle.\nMost of the options will be specific to the actual job in MyScript.sh\n\nSo…sbatch allows you to store the options in your shell script on lines after the shebang line that are preceded by #SBATCH.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#storing-slurm-options-in-your-shell-script",
    "href": "nmfs-bioinf/sbatch.html#storing-slurm-options-in-your-shell-script",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.4 Storing SLURM options in your shell script",
    "text": "7.4 Storing SLURM options in your shell script\nLet’s look at an example like one might do in an sbatch script:\n\n\n\nContents of scripts/bwa_index.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=2:00:00\n#SBATCH --mem=3G\n#SBATCH --output=bwa_index-%j.out\n#SBATCH --error=bwa_index-%j.err\n\n# this is a little script to index the genome given\n# by the first positional parameter ($1)\n\n# load the module that gives us the bwa software\nmodule load bwa\n\n# make a directory for log output if it does not already exist\nDIR=results/log/bwa_index\nmkdir -p $DIR\n\n# run bwa index on the input\nbwa index $1 &gt; $DIR/log.txt 2&gt;&1\n\n\n\n\n\n\n\n\nWhat’s that %j in the output and error options?\n\n\n\nIn the above script, you will see\n#SBATCH --output=bwa_index-%j.out\n#SBATCH --error=bwa_index-%j.err\nIn this context, the %j gets replaced by sbatch with the SLURM_JOB_ID.\n\n\n\n\n\n\n\n\nGet email from SLURM.\n\n\n\n\n\nOne useful feature of SLURM—especially if you are running just a few long jobs—is that you can tell it to send you email whenever some event occurs related to a job (like it Starts, or Finishes, or Fails).\nThe SBATCH directives for that look like:\n#SBATCH --mail-user=myemail@emailserver.com\n#SBATCH --mail-type=ALL\nwhere you would replace myemail@emailserver.com with your own email address.\nNote that option --mail-type can be tailored to modulate how much email you will get from SLURM.\nTBH, though, I don’t have SLURM email any news about my jobs anymore. Workflows that are optimized into a lot of smaller jobs would fill your inbox pretty quickly!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#let-us-submit-the-bwa_index.sh-job-to-slurm",
    "href": "nmfs-bioinf/sbatch.html#let-us-submit-the-bwa_index.sh-job-to-slurm",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.5 Let us submit the bwa_index.sh job to SLURM",
    "text": "7.5 Let us submit the bwa_index.sh job to SLURM\nSince all of the sbatch options are imbedded within the shell script it is easy to write the command to launch the script.\nLet’s prepare for that first. If you are on Alpine, make sure the slurm module is loaded.\n\n\nIf you are working on Alpine, be sure to paste this into your shell\n\nmodule load slurm/alpine\n\nNote that you do not need to be on a compute node to submit a job via sbatch—you can do that from a login node.\nTo launch the script using sbatch, all you do is:\n\n\nPaste this into your shell at the top level of the con-gen-csu repo\n\nsbatch scripts/bwa_index.sh data/genome/genome.fasta\n\nThe first argument is the script scripts/bwa_index.sh and the second, resources/genome.fasta, is the path to the genome that we want to index with bwa.\nWhen this command executes, it returns the SLURM_JOB_ID. Make a note of it.\nOnce you have launched the job, try using myjobs to see your job running. (You don’t have much time, because it doesn’t take very long).\n\n\n\n\n\n\nHey! That job ran in the current working directory\n\n\n\n\n\nNote that our script ran bwa index by passing it the path of a reference genome specified as a relative path: the path was relative to our current working directory.\nOne of the wonderful features of SLURM is that, when sbatch runs your script, it does so from the current working directory of the shell in which you ran sbatch.\n(I mention this because the first cluster I used was set up differently, and you had to explicitly tell it to run from the current working directory—which was the source of endless gnashing of teeth)\n\n\n\nOnce that job is done, use ls -lrt data/genome to see all the files that were newly created by the bwa-index.sh script.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#how-many-resources-did-that-job-use-seff",
    "href": "nmfs-bioinf/sbatch.html#how-many-resources-did-that-job-use-seff",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.6 How many resources did that job use? — seff",
    "text": "7.6 How many resources did that job use? — seff\nWhen you first start doing bioinformatics, you will not be very familiar with how long each job will run, or how much memory it will need.\nThat takes some time, but one helpful utility, seff, will tell you the effective usage of the allocated resources by any completed job.\nIt’s simple to use:\n\n\nHere is the sytnax\n\nseff slurm-jobid\n\n\n\n\n\n\n\nSelf-study\n\n\n\nTry that command, seff slurm-jobid, replacing slurm-jobid with the actual SLURM_JOB_ID of the job that you just ran.\n\n\n\n\n\n\n\n\nMore tips on learning about past job resource use\n\n\n\nYou can also use the sacct command. Check it out with man sacct.\nYou can get information much like seff for your recent jobs with sacct and it is somewhat easier to look at all the jobs that have run (or are running) in the last 12 to 48 hours with it. So, here we define a function to use it easily, and also we can give it more space to print the job names:\n\n\nPaste this into your shell to get a myacct function\n\nfunction myacct {\n  if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n  sacct  --format JobID,JobName%${JL},User,Group,State%20,Cluster,AllocCPUS,REQMEM,TotalCPU,Elapsed,MaxRSS,ExitCode,NNodes,NTasks -u $(whoami)\n}\n\nIf you like being able to use this, you ought to add it to your ~/.bashrc file.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-simple-job-with-default-sbatch-settings",
    "href": "nmfs-bioinf/sbatch.html#a-simple-job-with-default-sbatch-settings",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.7 A simple job with default sbatch settings",
    "text": "7.7 A simple job with default sbatch settings\nWe jumped right into talking about all the most useful options for sbatch.\nHowever, on Sedna (and, indeed, on most clusters), SLURM defines reasonable defaults for an sbatch job.\nIt even sends the output and error to reasonably named files, as we shall see.\nThe following lists the contents of a script called simple-15.sh that doesn’t do much:\n\nIt writes out the SLURM_JOB_ID to stdout\nIt writes a message to stderr\nThen it just sits there for 15 minutes.\n\n\n\n\nContents of scripts/simple-15.sh\n\n#!/bin/bash\n\n\n(\n  echo \"(stdout) Running with Slurm Job ID: $SLURM_JOB_ID\" \n  echo \"(stdout) Note that we are running this thing through\"\n  echo \"(stdout) the tee utility to try to unbuffer it.\"\n  echo\n) | tee simple-15.stdout\n\necho \"(stderr) This line is being written to stderr\" &gt; /dev/stderr\necho \"(stderr) Interestingly, stderr does not seem to get buffered.\" &gt;&gt; /dev/stderr\n\n\nsleep 900\n\n\n\n\n\n\n\n\nScripts running under SLURM have access to SLURM_* environment variables\n\n\n\nWhen a job is run by SLURM, it is done so in a shell environment that has a number of extra shell variables defined. In the above, we print the value of one of those: SLURM_JOB_ID.\n\n\nNow we will submit that script to run as a SLURM job with:\n\n\nPaste this into your shell\n\nsbatch scripts/simple-15.sh\n\nNow, use myjobs to see how many cores this job is using (1) and how much memory (4700 Mb on Sedna, 3840 on Alpine). Those are the default values.\nBy default, on some systems both stdout and stderr get written to slurm-%j.out (where %j% is replaced with the SLURM_JOB_ID).\nYou can see that is in that by doing:\n\n\nType this, but replace 331979 with whathever your actual SLURM job id is\n\ncat slurm-331979.out\n\nWe see that stderr gets written to slurm-%j.out immediately. The stdout stream is supposed to get written there, as well, but it seems that there is some hard-core buffering that goes on with slurm: we can see the output in simple-15.stdout, but not in slurm-%j.out.\n\n\n\n\n\n\nDon’t leave stdout and stderr un-redirected in your scripts\n\n\n\nSeeing the buffering issues above hardens my own convictions that you should not rely on SLURM to capture any output to stdout or stderr that is not otherwise redirected to a file. You should always be explicit about redirecting stdout or stderr within your own scripts to files where we want it to go.\nThat way the slurm output logs are left to mostly capture messages from SLURM itself (for example, telling you that you ran out of memory or the job was cancelled.)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#oh-no-i-need-to-stop-my-jobs-scancel",
    "href": "nmfs-bioinf/sbatch.html#oh-no-i-need-to-stop-my-jobs-scancel",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.8 Oh no! I need to stop my job(s): scancel",
    "text": "7.8 Oh no! I need to stop my job(s): scancel\nNow, we have a job that is sitting around doing nothing for the next 15 minutes or so.\nThis is a great time to talk about how to abort jobs that are running under sbatch:\n\n\n\n\n\n\nCancelling jobs started with sbatch\n\n\n\nIf you have the job number (which is returned when sbatch launched the job or which you can see on the corresponding line of myjobs) you can use scancel followed by the job number.\nFor example:\nscancel 329656\n…but you have to replace the number above with your job’s job number.\nPlease do that now!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-note-about-memory",
    "href": "nmfs-bioinf/sbatch.html#a-note-about-memory",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.9 A note about memory",
    "text": "7.9 A note about memory\n\n\n\n\n\n\nMemory gets allocated with cores or via --mem\n\n\n\nIt is quite clear that the --mem option to sbatch is intended to increase the memory allocated to the job; however adding cores with --cpus-per-task, without adding any options to dictate memory use, also increases the memory available.\nOn Sedna’s standard compute nodes, which each have 20 cores, if you ask for \\(x\\) cores, the total amount of memory your job will get is about \\(\\frac{x}{20} T\\), where \\(T\\) is a little less than the total memory on the machine.\nOn the standard memory compute nodes, that means your job gets roughly 4700 Mb (4.7 Gb) of RAM for each core that it is allocated. (9400 Mb or 9.4 Gb for each core on the “higher-memory” standard compute nodes, node[29-36])\nNote, however, that if the programs running in your job are not multithreaded, then you might not be able to use all those cores. In which case, it might be better to specify additional memory allocation with --mem, and leave the other cores to other users.\nHowever, on Alpine, if you ask for 3.74 Gb * 10 = 37.4 Gb of memory and only one core for computing, you will still be “charged” (i.e., you will run through your quota) as if you were using 10 cores! (i.e., billing is by the core, and you are charged for the number of cores that would give you a certain amount of memory).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#what-happens-if-we-exceed-our-resources",
    "href": "nmfs-bioinf/sbatch.html#what-happens-if-we-exceed-our-resources",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.10 What happens if we exceed our resources?",
    "text": "7.10 What happens if we exceed our resources?\nR uses 8 bytes for each numeric value it stores in a vector. But it also seems to do some fancy stuff with delayed evaluation, etc. So it is hard to know exactly how much system RAM R’s process will use.\nNonetheless, I have found that the following R commands will exceed 4700 Mb of RAM:\nx &lt;- runif(1e9); y &lt;- x + 1; y[1:10]\nSomewhere in the y &lt;- x + 1 command, memory usage will exceed the default 4700 Mb (4.7 Gb) of RAM that SLURM allows by default (on SEDNA) or the 3.74 Gb of RAM allowed on Alpine.\nSo, what we are going to do here is run those commands in an sbatch script and see what happens. (You will probably exceed your allocated memory while doing bioinformatics, so you might as well get used to what that looks like.)\nHere is a listing of a shell script to run under sbatch to exceed our memory usage:\n\n\n\nContents of scripts/r-too-much-mem.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=2:00:00\n#SBATCH --output=r-too-much-mem-%j.out\n#SBATCH --error=r-too-much-mem-%j.err\n\n\nmkdir -p outputs\nEDIR=results/log/r-too-much-mem\nmkdir -p $EDIR\n\nmodule load R\nRscript --vanilla -e \"x &lt;- runif(1e9); y &lt;- x + 1; y[1:10]\" &gt; outputs/r-too-much.out 2&gt; $EDIR/log.txt\n\n\nRun it like this:\n\n\nPaste this into your shell\n\nsbatch scripts/r-too-much-mem.sh\n\nIt takes about a minute before it runs out of memory. During the time, check that it is still running using myjobs.\nOnce you see it is no longer running, check on the status of that job using myacct 15. The bottom line of output should show the results for that last job:\n\nThe State column will show OUT_OF_MEMORY\nThe MaxRSS column shows how much memory it used before failing. In my case, that was 4805260K.\n\n\n\n\n\n\n\nSelf study\n\n\n\nHow would you modify the scripts/r-too-much-mem.sh so that it did not run out of memory?\n\n\n\n\n\n\n\n\nSelf study answer\n\n\n\n\n\nYou need to allocate more memory to the job. You can do this by adding an sbatch directive line like:\n#SBATCH --mem=12G\namongst all the other sbatch directives. I don’t know if 12G of ram will be enough, but it might be. You could try it.\nOR as we will see below, you could add that memory option on the command line.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#sbatch-options-on-the-command-line-will-override-the-sbatch-directives-in-the-file",
    "href": "nmfs-bioinf/sbatch.html#sbatch-options-on-the-command-line-will-override-the-sbatch-directives-in-the-file",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.11 sbatch options on the command line will override the #SBATCH directives in the file",
    "text": "7.11 sbatch options on the command line will override the #SBATCH directives in the file\nYou can mix and match sbatch options on the command line with sbatch options in the #SBATCH directives in the script.\nThe option on the command line takes precedence over the same option in the file.\nSo, we could provide sufficient memory to scripts/r-too-much-mem.sh and at the same time, override the time it is allotted with this command:\n\n\nDon't paste this into your shell---it uses too much resources\n\nsbatch --mem=16G --time=01:00:00 scripts/r-too-much-mem.sh\n\nIf you launched that job, you could use your myjobs function to see that the MIN_MEMORY is 16G and the TIME_LIMIT is 1 hour.\nWhen it finished you could run myacct to see that it successfully completed, and look in the output file outputs/r-too-much.out to see that it printed the first 10 of one billion random numbers with 1 added to them.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#lets-run-out-of-time",
    "href": "nmfs-bioinf/sbatch.html#lets-run-out-of-time",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.12 Let’s run out of time",
    "text": "7.12 Let’s run out of time\nWe are going to rerun our simple-15.sh script, but only allocate one minute for it. So we should see it fail after only 1 minute.\n\n\n\n\n\n\nEric, why are making us run so many jobs that fail?\n\n\n\n\n\nLet’s be honest, as you embark on your bioinformatic career with SLURM, you are going to spend a significant amount of your time dealing with jobs that fail.\nYou might as well see jobs failing in several different ways so that you can recognize them when you see them later.\n\n\n\nSo, try this:\n\n\nPaste this into your shell\n\nsbatch --time=00:01:00 scripts/simple-15.sh\n\nThat will fail in a minute, after which, use myacct to see what it says about how it failed.\nIt will tell you that the main script hit a TIMEOUT and the batch job running in that script was CANCELLED",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-major-gotcha-do-not-direct-sbatchs-output-or-error-to-a-path-that-does-not-exist",
    "href": "nmfs-bioinf/sbatch.html#a-major-gotcha-do-not-direct-sbatchs-output-or-error-to-a-path-that-does-not-exist",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.13 A major gotcha: Do not direct sbatch’s output or error to a path that does not exist",
    "text": "7.13 A major gotcha: Do not direct sbatch’s output or error to a path that does not exist\n\n\n\n\n\n\nBeware!\n\n\n\nOn some versions of SLURM, If you try something like:\n#SBATCH output=outputs/mydir/outfile.txt\n#SBATCH error=outputs/mydir/errfile.txt\nbut do so without having actually made the directory outputs/mydir, then sbatch will fail, and it won’t be able to write out why it failed.\nThis is dynamite. If you ever find that your sbatch jobs are failing immediately but with no reasonable error messages anywhere, check to make sure that you aren’t sending SLURM’s output or error to a directory that does not exist.\nNote that you cannot make outputs/mydir within your script, because SLURM needs those directories before your script even gets executed.\n(Also, some later versions of SLURM actually seem to make the directories, but you probably don’t want to rely on that. At any rate, if you have a job that fails and leave no error messages, check to make sure that your –output and –error paths exist!)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#schedule-multiple-jobs-with-sbatch-using-a-for-loop",
    "href": "nmfs-bioinf/sbatch.html#schedule-multiple-jobs-with-sbatch-using-a-for-loop",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.14 Schedule multiple jobs with sbatch using a for loop",
    "text": "7.14 Schedule multiple jobs with sbatch using a for loop\nWe now consider a small job of mapping some paired end reads to the genome that we indexed a few steps ago.\nLet’s review the shell code in scripts/bwa-map.sh:\n\n\n\nContents of file scripts/bwa_map.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1:00:00\n#SBATCH --output=bwa_map-%j.out\n#SBATCH --error=bwa_map-%j.err\n\n# this is a little script to map a pair of fastq files\n# that are in data/fastqs.\n#\n# The first positional parameter ($1)\n# should be something like: DPCh_plate1_B10_S22\n\n# load the module that gives us the bwa software\nmodule load bwa\nmodule load samtools\n\n# make a directory for log output if it does not already exist\nLDIR=results/log/bwa_map\nODIR=results/mapped\n\nmkdir -p $LDIR $ODIR\n\nSAMP=$1\n\n# run bwa mem on the input and pipe it to samtools to sort it\nbwa mem data/genome/genome.fasta \\\n  data/fastqs/${SAMP}_R1.fq.gz \\\n  data/fastqs/${SAMP}_R2.fq.gz  2&gt; $LDIR/bwa_mem_${SAMP}.log | \\\n(\n  samtools view -u - | \\\n  samtools sort -o $ODIR/$SAMP.bam\n) 2&gt; $LDIR/samtools_$SAMP.log\n\n\nNote that we are using backslashes (\\) at the ends of the lines so that we can break a long line up onto separate lines of text.\nAlso note that grouping of commands by parentheses.\nThis script is run by passing it DPCh_plate1_B10_S22, DPCh_plate1_B11_S23, or DPCh_plate1_B12_S24, etc., as the first postitional parameter.\nSo, if we wanted to schedule three of those mapping jobs, we could do:\n\n\nPaste this code into your shell\n\nfor S in DPCh_plate1_B10_S22 DPCh_plate1_B11_S23 DPCh_plate1_B12_S24; do sbatch scripts/bwa_map.sh $S; done\n\nWhen that is done, check what is running with myjobs and alljobs. (But do it fast! Because these jobs finish very quickly).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#final-thought",
    "href": "nmfs-bioinf/sbatch.html#final-thought",
    "title": "7  Submitting jobs with sbatch",
    "section": "7.15 Final thought",
    "text": "7.15 Final thought\nThough multiple jobs can be submitted via sbatch easily using this sort of for loop construct, there is another way of launching multiple repetitions of the same job in SLURM: using job arrays\nWe will discuss that in the next section.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Submitting jobs with `sbatch`</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html",
    "href": "nmfs-bioinf/slurm-arrays.html",
    "title": "8  Slurm Job Arrays",
    "section": "",
    "text": "8.1 Prepare for this session\nGet on your cluster and navigate to the top level (con-gen-csu) of your fork of the class repository. Then, to make sure that you have all the latest updates from the repository, sync the main branch of your fork with the main branch of eriqande/con-gen-csu on GitHub, then in your shell, use git switch main to make sure you are on the main branch of your fork, and then use git pull origin main to pull down those changes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slurm Job Arrays</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#sbatchs---array-option",
    "href": "nmfs-bioinf/slurm-arrays.html#sbatchs---array-option",
    "title": "8  Slurm Job Arrays",
    "section": "8.2 sbatch’s --array option",
    "text": "8.2 sbatch’s --array option\nWhen you give sbatch the --array=1-10 option, say, then it runs your job 10 separate times, each time with the environment variable SLURM_ARRAY_TASK_ID set to a different number between 1 and 10.\nLet’s see a quick example, using the following script:\n\n\n\nContents of scripts/array_example.sh\n\n#!/bin/bash\n#SBATCH --time=00:05:00\n#SBATCH --output=my_output_%A_%a\n#SBATCH --error=my_error_%A_%a\n#SBATCH --array=1-10\n\nODIR=results/array_example\nmkdir -p $ODIR\n\n(\n  echo \"The SLURM_ARRAY_JOB_ID is : $SLURM_ARRAY_JOB_ID\"\n  echo \"The SLURM_ARRAY_TASK_ID is: $SLURM_ARRAY_TASK_ID\"\n  echo \"The SLURM_JOB_ID is: $SLURM_JOB_ID\"\n  echo\n  echo \"You can refer to this individual SLURM array task as: ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}\"\n  echo\n) &gt; $ODIR/output_$SLURM_ARRAY_TASK_ID.txt\n\nsleep 20\n\n\nThis is helpful to see a few of the variables that are defined in the environment of an array job:\n\nSLURM_ARRAY_JOB_ID: the overall JOB_ID for the whole array\nSLURM_ARRAY_TASK_ID: the index that runs from 1 to 10 in this case\nSLURM_JOB_ID: The underlying job_id\n\nOne can run this script, but we don’t want a whole classroom full of people throwing all these jobs on the cluster. So, get into groups of three or four, talk amongst yourselves about what you think this script will do, and then have just one person from each group launch the job with the following command:\n\n\nOne person from each group, paste this into your shell\n\nsbatch scripts/array_example.sh\n\nAnd then use myjobs and alljobs to watch what is happening on the cluster.\nWhen that job is done, or when it is running, look at the values written to the first output files:\n\n\nPaste this into you shell\n\nhead results/array_example/output_{1..10}.txt\n\n\n\n\n\n\n\nCool syntax interlude: {1..10}\n\n\n\n\n\nOn the shell, if you do something like:\n\n{2..7}: that will expand to 2 3 4 5 6 7\n{a..g}: that will expand to a b c d e f g\n{0001..0015}: that will expand to 0001 0002 0003 0004 0005 0006 0007 0008 0009 0010 0011 0012 0013 0014 0015\n{F..M}: that will expand to F G H I J K L M\n\n\n\n\nFrom looking at the array_example output files, we can infer that:\n\nSLURM_ARRAY_JOB_ID: is a unique number that refers to the entire set of jobs in the array\nSLURM_ARRAY_TASK_ID: is the integer that is being cycled over in the array job\nSLURM_JOB_ID: is a unique SLURM_JOB_ID of the specific array task.\n\nAlso, in alljobs and myjobs you see that jobs can be referred to like 331989_4. That is SLURM_ARRAY_JOB_ID + underscore + SLURM_ARRAY_TASK_ID. For example: scancel 376578_12.\nIn the slurm output files specified like:\nSBATCH --output=my_output_%A_%a\n\n%A expands to SLURM_ARRAY_JOB_ID\n%a expands to SLURM_ARRAY_TASK_ID\n\nIf you need to cancel a particular array task using scancel you would typically use SLURM_ARRAY_JOB_ID + underscore + SLURM_ARRAY_TASK_ID, unless you wanted to cancel all remaining instances of an array task, in which case you would use\nscancel SLURM_ARRAY_JOB_ID",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slurm Job Arrays</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#variations-on-the-array_spec",
    "href": "nmfs-bioinf/slurm-arrays.html#variations-on-the-array_spec",
    "title": "8  Slurm Job Arrays",
    "section": "8.3 Variations on the <array_spec>",
    "text": "8.3 Variations on the &lt;array_spec&gt;\nThere are some important variations to how you can specify those array numbers:\n\n--array=1-50: simple, consecutive numbers\n--array=1-10:3: 1 through 10 by threes, (so 1,4,7,10)\n--array=1,2,3,6,9,15 non-consecutive numbers\n--array=1-21:10,100-200:50: non-consecutive ranges. It becomes (1,11,21,100,150,200)\n--array=1-10,4: WARNING, this becomes 1,2,3,4,5,6,7,8,9,10,4. SLURM does not check that the array numbers are unique, so task array 4 would be run twice (possibly concurrently overwriting the output.)\n--array=1-20%5 VERY IMPORTANT SYNTAX: Run the jobs, but don’t ever have more the 5 running at a time. This is useful for making sure your jobs don’t consume every last CPU on Sedna.\n\nLet’s try putting all these together. Read the following command and, talking among the members of your group, figure out what the array spec is doing. Then have one person from each group submit the job with the following command:\n\n\nOne person from each group, aste this into your shell\n\nsbatch --array=100,200,300-400:10%5 scripts/array_example.sh\n\nThen use myjobs and alljobs to see what is going on in the cluster.\nDo you ever have more than 5 jobs running?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slurm Job Arrays</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#translating-array-indexes-to-job-instances",
    "href": "nmfs-bioinf/slurm-arrays.html#translating-array-indexes-to-job-instances",
    "title": "8  Slurm Job Arrays",
    "section": "8.4 Translating Array Indexes to Job Instances",
    "text": "8.4 Translating Array Indexes to Job Instances\n\nThe user is left to translate what a job array index of, say, 7, means in terms of what actions that array task should take.\nQuite often you will want to map an array index to a different file to analyze, or perhaps a different region of a chromosome to do variant calling on, etc.\nA flexible and generic way of doing this mapping from array indexes to job specifics is to first define the variables (things like filenames, etc.) required by each array task in a simple TAB-delimited text file in which the first row holds the names of the variables in different TAB-separated columns, and each row below that holds the values that those variables should take for different values of the array index.\nThe array index itself should be listed in the first column, whose name is index.\n\n\n8.4.1 An example TAB-delimited file\nLet’s explore the following file, which is at data/sample-array-info.tsv:\n\n\n\nContents of data/sample-array-info.tsv\n\nindex   sample  library flowcell    platform    lane    barcode fq1 fq2\n1   DPCh_plate1_B10_S22 plate1  HTYYCBBXX   ILLUMINA    1   GACGTCAT+TCAACTGG   data/fastqs/DPCh_plate1_B10_S22_R1.fq.gz    data/fastqs/DPCh_plate1_B10_S22_R2.fq.gz\n2   DPCh_plate1_B11_S23 plate1  HTYYCBBXX   ILLUMINA    1   CCGCTTAA+ACGATGAC   data/fastqs/DPCh_plate1_B11_S23_R1.fq.gz    data/fastqs/DPCh_plate1_B11_S23_R2.fq.gz\n3   DPCh_plate1_B12_S24 plate1  HTYYCBBXX   ILLUMINA    1   GACGAACT+TCGCATTG   data/fastqs/DPCh_plate1_B12_S24_R1.fq.gz    data/fastqs/DPCh_plate1_B12_S24_R2.fq.gz\n4   DPCh_plate1_C10_S34 plate1  HTYYCBBXX   ILLUMINA    1   AGTCACAT+CCATAATG   data/fastqs/DPCh_plate1_C10_S34_R1.fq.gz    data/fastqs/DPCh_plate1_C10_S34_R2.fq.gz\n5   DPCh_plate1_C11_S35 plate1  HTYYCBBXX   ILLUMINA    1   CCTTTCAC+AAACAAGA   data/fastqs/DPCh_plate1_C11_S35_R1.fq.gz    data/fastqs/DPCh_plate1_C11_S35_R2.fq.gz\n6   DPCh_plate1_C12_S36 plate1  HTYYCBBXX   ILLUMINA    1   GCACACAA+TCGTCAAG   data/fastqs/DPCh_plate1_C12_S36_R1.fq.gz    data/fastqs/DPCh_plate1_C12_S36_R2.fq.gz\n7   DPCh_plate1_D09_S45 plate1  HTYYCBBXX   ILLUMINA    1   CTTTCCCT+AGTAAGCC   data/fastqs/DPCh_plate1_D09_S45_R1.fq.gz    data/fastqs/DPCh_plate1_D09_S45_R2.fq.gz\n8   DPCh_plate1_D11_S47 plate1  HTYYCBBXX   ILLUMINA    1   GACAATTC+CATATCGT   data/fastqs/DPCh_plate1_D11_S47_R1.fq.gz    data/fastqs/DPCh_plate1_D11_S47_R2.fq.gz\n9   DPCh_plate1_F10_S70 plate1  HTYYCBBXX   ILLUMINA    1   ACACGACT+CTGCGGAT   data/fastqs/DPCh_plate1_F10_S70_R1.fq.gz    data/fastqs/DPCh_plate1_F10_S70_R2.fq.gz\n10  DPCh_plate1_F11_S71 plate1  HTYYCBBXX   ILLUMINA    1   TCCACGTT+GTTCAACC   data/fastqs/DPCh_plate1_F11_S71_R1.fq.gz    data/fastqs/DPCh_plate1_F11_S71_R2.fq.gz\n11  DPCh_plate1_F12_S72 plate1  HTYYCBBXX   ILLUMINA    1   AACCAGAG+AACCGAAG   data/fastqs/DPCh_plate1_F12_S72_R1.fq.gz    data/fastqs/DPCh_plate1_F12_S72_R2.fq.gz\n12  DPCh_plate1_G09_S81 plate1  HTYYCBBXX   ILLUMINA    1   CGAATACG+GTCGGTAA   data/fastqs/DPCh_plate1_G09_S81_R1.fq.gz    data/fastqs/DPCh_plate1_G09_S81_R2.fq.gz\n13  DPCh_plate1_G10_S82 plate1  HTYYCBBXX   ILLUMINA    1   CAGTGCTT+ACCTGGAA   data/fastqs/DPCh_plate1_G10_S82_R1.fq.gz    data/fastqs/DPCh_plate1_G10_S82_R2.fq.gz\n14  DPCh_plate1_G12_S84 plate1  HTYYCBBXX   ILLUMINA    1   TCCATTGC+AGACCGTA   data/fastqs/DPCh_plate1_G12_S84_R1.fq.gz    data/fastqs/DPCh_plate1_G12_S84_R2.fq.gz\n15  DPCh_plate1_H09_S93 plate1  HTYYCBBXX   ILLUMINA    1   GTCGATTG+ACGGTCTT   data/fastqs/DPCh_plate1_H09_S93_R1.fq.gz    data/fastqs/DPCh_plate1_H09_S93_R2.fq.gz\n16  DPCh_plate1_H10_S94 plate1  HTYYCBBXX   ILLUMINA    1   ATAACGCC+TGAACCTG   data/fastqs/DPCh_plate1_H10_S94_R1.fq.gz    data/fastqs/DPCh_plate1_H10_S94_R2.fq.gz\n\n\nAn easier way to view this might be to look at in on GitHub at: here\nSomething that would be really handy would be a little shell script that would pick out a particular line of that file that corresponds to the value in the index column and then define some shell variables according to the columns names: index, sample, library, flowcell, platform, lane, barcode, fq1, fq2 so that they could be used in an array script.\nWe have already seen some of the bash and awk machinery that would make that possible, and we have wrapped it up in the line-assign.sh script.\n\n\n8.4.2 The line-assign.sh script\nWithin the repository is a script called scripts/line-assign.sh, that looks like this:\n\n\n\nContents of scripts/line-assign.sh\n\n#!/bin/bash\n# simple script.  Arguments are:\n#  1. The index to pick out (like 1, or 2, or 7, etc)\n#  2. Path to a TAB delimited file with the first column named index\n#     and subsequent columns named valid shell variable names.\n#\n# This script will pick out the line that matches $1, and return a command\n# line assigning the column header names as shell variables whose values are the\n# values in each cell in the file $2.\n#\n# Note: this is not written with a whole lot of error checking or catching.\n#\n\n\nif [ $# -ne 2 ]; then\n  echo \"Wrong number of arguments in $0 \" &gt; /dev/stderr\nfi\n\nawk -F\"\\t\" -v LINE=$1 '\n  $1 == \"index\" {for(i=1; i&lt;=NF; i++) vars[i]=$i; next}\n  $1 == LINE {for(i=1; i&lt;=NF; i++) printf(\"%s=\\\"%s\\\"; \", vars[i], $i); printf(\"\\n\");}\n' $2\n\n\nLet’s see what sorts of results this produces:\n\n\nPaste this into your shell\n\n ./scripts/line-assign.sh 3 data/sample-array-info.tsv\n\nWhoa! It returns a command line that assigns values to a lot of shell variables.\nSo, if we wanted to run that command line, we would have to precede it with the eval keyword (because the command line itself includes special characters like =). Let’s do that like this:\n\n\nPaste this into your shell\n\nCOMM=$(./scripts/line-assign.sh 3 data/sample-array-info.tsv)\neval $COMM\n\nNow, that you have done that, you can see that a lot of variables have been assigned the values on the index == 3 line of our TSV file:\n\n\nPaste this into your shell\n\n(\necho \"index:      $index\"\necho \"sample:     $sample\"\necho \"library:    $library\"\necho \"platform:   $platform\"\necho \"lane:       $lane\"\necho \"barcode:    $barcode\"\necho \"fq1:        $fq1\"\necho \"fq2:        $fq2\"\n)\n\nHoly Smokes! These are variables that we could use in a job array script.\nIn this case, we can assign values to the pesky Read Group string for bwa mem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slurm Job Arrays</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#putting-it-all-together-a-read-mapping-job-array",
    "href": "nmfs-bioinf/slurm-arrays.html#putting-it-all-together-a-read-mapping-job-array",
    "title": "8  Slurm Job Arrays",
    "section": "8.5 Putting it all together: a read-mapping job array",
    "text": "8.5 Putting it all together: a read-mapping job array\nWe can now elaborate on our simple bwa_map.sh shell script and turn that into a SLURM job array script. The key here is that when SLURM runs the script as a slurm array, there will be an environment variable called SLURM_ARRAY_TASK_ID that we can use to pick out the desired sample from the data/sample-array-info.tsv file.\nHere is what it looks like:\n\n\n\nContents of scripts/bwa_map_array.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1:00:00\n#SBATCH --output=bwa_map_array-%A_%a.out\n#SBATCH --error=bwa_map_array-%A_%a.err\n#SBATCH --array=1-16\n\n# load the module that gives us the bwa software\nmodule load bwa\nmodule load samtools\n\n# make a directory for log output if it does not already exist\nLDIR=results/log/bwa_map_array\nODIR=results/mapped\n\nmkdir -p $LDIR $ODIR\n\n# get shell variables for this array task:\nCOMM=$(./scripts/line-assign.sh $SLURM_ARRAY_TASK_ID data/sample-array-info.tsv)\neval $COMM\n\n\n# run bwa mem on the input and pipe it to samtools to sort it\nbwa mem \\\n  -R \"@RG\\tID:${sample}.${library}.${flowcell}.${lane}\\tSM:${sample}\\tLB:${library}\\tBC:${barcode}\\tPU:\\tID:${sample}.${library}.${flowcell}.${lane}.${barcode}\\tPL:ILLUMINA\" \\\n  data/genome/genome.fasta \\\n  $fq1 $fq2  2&gt; $LDIR/bwa_mem_$sample.log | \\\n(\n  samtools view -u - | \\\n  samtools sort -o $ODIR/$sample.bam\n) 2&gt; $LDIR/samtools_$sample.log\n\n\nThe main differences from our simple bwa_map.sh script are:\n\nThe SLURM_ARRAY_TASK_ID is used to pick out the right line from our TSV file of information\nThe slurm output and error have been changed to use %A_%a notation.\nThere is a #SBATCH --array=1-16 directive, to run it over the 16 different files.\nThe shell code that runs bwa and samtools uses the variables that were defined by eval-ing the results of the call to the line-assign.sh script.\n\nYou can launch that job array and see how it goes:\n\n\nPaste this into your shell\n\nsbatch scripts/bwa_map_array.sh\n\nThen check with myjobs and alljobs to see what is happening on Sedna.\nThat runs really fast.\nAfterward you can check that the results exist:\n\n\nPaste this into your shell\n\nls -l results/mapped\n\nAnd you can look at the logs for bwa mem:\n\n\nPaste this into your shell\n\ntail  results/log/bwa_map_array/bwa_mem_*\n\nTo see what one of the output files looks like, you can use samtools. On Alpine, you have to get onto a compute node to do that, so you would first do:\n\n\nDo this if you are on the Alpine cluster\n\nmodule load slurm/alpine\nsrun --partition=atesting --pty /bin/bash\n\nThen do like this:\n\n\nPaste this into your shell\n\nmodule load samtools\nsamtools view results/mapped/DPCh_plate1_F10_S70.bam | less -S\n\nUse the left and right arrows, and space bar and backspace to see all parts of the file.\nRemember to hit q to get out of the less viewer.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slurm Job Arrays</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#wrap-up",
    "href": "nmfs-bioinf/slurm-arrays.html#wrap-up",
    "title": "8  Slurm Job Arrays",
    "section": "8.6 Wrap Up",
    "text": "8.6 Wrap Up\nSo, that was a quick tour of the capabilities of sbatch.\nOddly enough, I haven’t directly launched any jobs using sbatch (apart from the example jobs for this course) in many months, because I drive all my bioinformatics projects using Snakemake, which sends my jobs to SLURM for me.\nEventually we will have an introduction to Snakemake!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slurm Job Arrays</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html",
    "href": "nmfs-bioinf/sequence-alignment.html",
    "title": "9  Sequence alignment and handling BAM files",
    "section": "",
    "text": "9.1 The Journey of each DNA Fragment from Organism to Sequencing Read\nIn Section @ref(get-seqs) we discussed one solution for downloading gzipped FASTQ files from a sequencing center. This turns out to be the last step in a very long process of acquiring sequencing data from a collection of organisms that you are studying. We could say that the DNA of each organism that you are studying took an extraordinary journey to get to you. This journey included, 1) you (or a colleague) sampling tissue or blood from that organism, 2) DNA being extracted from that tissue or blood, 3) that DNA getting Illumina adapters (with sample-specific barcodes) attached to it and then getting PCR amplified some number of times in the library preparation stage, 4) the products of the library preparations are sequenced, sometimes on multiple flow cells and often across multiple lanes, during the sequencing stage, 5) the sequencer processes the read data into FASTQ files, 6) finally, you download the data.\nAnyone who has ever been involved in research knows that at every stage of the above process there are opportunities for errors or biases to occur that could affect the final data (and inferences) produced. The effects of some of the errors and biases that occur during the library prep stage and the sequencing stage can be eliminated or lessened. Doing so requires that we keep careful track of the route that each read took during those those steps in its journey. This is the role of read groups, which we discuss below. For now, however, we will offer that the main errors/effects that can be controlled for by careful accounting of reads into read groups are: 1) controlling for effects on the reported base quality scores of the particular flowcell or the particular lane on a flowcell that a read was sequenced on; 2) The identification of sequencing reads which are PCR duplicates of another read in the data set. Thus, most of the focus on defining read groups is concerned with identifying features of a read’s journey that can be used to ameliorate those two types of errors/biases.\nThe process of controlling for batch effects of flowcells and lanes on base quality scores is called, unsurprisingly, Base Quality Score recalibration. This might be discussed in a later section. It is a very important process when one is searching for de novo mutations, or otherwise attempting to very accurately identify variants that occur at very low frequency. Doing base quality score recalibration is considered a “best practice” when using GATK (Section XX), but I have my doubts as to whether it will make appreciable differences in analyses where the importance of rare variants is downweighted. Nonetheless, whether you are going to do base quality score recalibration or not, you should certainly prepare your data so that you can do it should you decide to. In order to do it, you must know which flowcell and upon which lane each read was sequenced. That information can be found in the FASTQ file as part of the name for each sequence, however, methods for doing base quality score recalibration do not typically use the names of the reads to access that information. Rather it must be stored in the read group information.\nA PCR duplicate is a template sequence that happens to be a copy (made by PCR) of another sequence that has also been sequenced. They are considered “errors” in bioinformatics, because many of the models for inferring genotypes from next generation sequencing data assume a process of independent sampling of gene copies (i.e. the maternal and paternal copies of a choromosome) within a diploid individual, and that model and the data are used to estimate how likely an individual is a homozygote versus a heterozygote at each position. For example, if Individual X produces one read covering position Y—a position known to be a variable SNP with alleles A and G in the species—and the read carries an A at position Y, then you would conclude that Individual X likely has genotype AA or AG at position Y, and is less likely to have genotype GG, because our observed read with an A at that position would have only occurred because of a sequencing error in that case. If we saw 10 reads from individual X, and they all had the A base at position Y, however, we could be quite confident that the genotype is AA rather than AG, because if the indvidual had genotype AG, then each read should have a 50% chance of carrying an A and a 50% chance of carrying a G, and it is very unlikely, in that case, that all 10 would carry the A allele. However, imagine how your conclusions would change if someone were to tell you that 95% of the reads were PCR duplicates of another one of the reads. In that case, even if the individual has genotype AG, there is high probability that only a single, true template carrying an A came from the individual, and the remaining 9 reads that carry an A are just PCR copies of that first original template.\nPCR duplicates are copies of the same template fragments. Therefore in paired-end data if a pair of reads maps to exactly the same location as another pair of reads, it is likely that one of them is a PCR duplicate of the other. Figure 9.1 provides a simplified schematic (with merely single-end data) describing a situation with high and low numbers of PCR duplicates. The PCR duplicate fragments are denoted with gray-colored flanking sequence. In the figure, note that the length of the fragments gives information about which fragments are duplicated and which are not—if two fragments of the same length are identified, then one of them is considered a PCR duplicate. It is worth noting that it isn’t (to my knowledge) really known which fragment is the duplicate and which is the “original,” but, in order to correct the situtation, one (or more of them) will be identified as duplicates, and one of them will be identified as an “original.”\nFigure 9.1: An example of PCR duplicates. Note that in paired end data, duplicates are identified by the lengths of both reads as well as the insert length. This figure represents a simplified, toy, situation, showing just single-end data.\nPCR duplicates occur during the library prep stage, and also are only possible for fragments originating from the same individual sample. In other words, if you found two different paired-end reads that mapped to the same location in the genome, but they were from different individuals (i.e. had different sample-specific DNA barcodes), you would not suspect that they were PCR duplicates. Likewise, DNA fragments from two different library preps, even if they came from the same individual—in the case that DNA from that individual had been prepared in two separate libraries—would not be considered to be PCR duplicates. (We should pause here to note what is considered a “library prep.” Basically, if a batch of samples were all processed together in the same day with reagents from the same kit, that is considered a single “library prep.”) Accordingly, when keeping track of read groups for the purposes of identifying PCR duplicates, the most important information is which individuals the DNA came from and which library preparation it was prepared in.\nWhat we can conclude from this section is that, for downstream analyses, we need a way to quickly (and efficiently, in terms of data storage space) identify, for each read the:\nThis information can be strored in the alignment file using read group information as described below.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sequence alignment and handling BAM files</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#read-journey",
    "href": "nmfs-bioinf/sequence-alignment.html#read-journey",
    "title": "9  Sequence alignment and handling BAM files",
    "section": "",
    "text": "Sample it originated from\nThe library in which it was prepared for sequencing.\nThe flowcell that it was sequenced on.\nThe lane on that flowcell that it was sequenced on",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sequence alignment and handling BAM files</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#read-groups",
    "href": "nmfs-bioinf/sequence-alignment.html#read-groups",
    "title": "9  Sequence alignment and handling BAM files",
    "section": "9.2 Read Groups",
    "text": "9.2 Read Groups\nWe previously touched briefly on read groups, noting that the SAM file header can contain lines like this one:\n@RG ID:HTYYCBBXX.1.CH_plate1_A01    SM:CH_plate1_A01    LB:Lib-1    PU:HTYYCBBXX.1.TCAATCCG+TTCGCAGT    PL:ILLUMINA\nThe purpose of this line is to identify a read group that is named via the ID tag. In the above case the read group is named HTYYCBBXX.1.CH_plate1_A01. The attributes of this read group are stored using the remaining tags in the @RG line. Happily, those attributes mirror exactly what we might need to know to identify PCR duplicates and to perform base quality score recalibration; namely:\n\nSM : give the name of the sample, CH_plate1_A01, the read is from,\nLB : gives the name of the library prep, Lib-1, the read is from,\nPU : gives the name of the flowcell, HTYYCBBXX, and the lane, 1, the read is from.\n\nIt is important to note that this line in the SAM header is just a header line that gives information about a particular read group that is found in the file. It is not, by itself giving you information about any particular read found in the alignment file. Read group information about a particular read in the alignment file is given on the alignment line in a column (way off near the end of the line typically) tagged with RG:Z. That RG:Z column in an alignment row simply holds the ID of the read group that the read belongs to. In this way, all the information that applies to the read—i.e., the sample it came from, (SM), the library prep it was in (LB), and information about the flowcell and lane (PU)—can be assigned to the read, without writing all of that information on every line in the alignment file. Accordingly, it should be obvious why the read group IDs must be unique across all the different read groups (@RG tags) in a SAM/BAM file: the ID is used to identify all the other information that goes along with that read, and is used to differentiate those different read groups!\nYou can download the first 100 lines (without the @SQ lines) of a SAM file that has 8 different read groups in it here. Let’s do that and have a look at it.\nThe way that read group information—particularly the platform unit, or PU information—is recorded is dictated by which software will be used for downstream processing. We describe the conventions that are useful for passing the aligned data to the Genome Analysis Toolkit (GATK), an extensive and well-supported piece of software developed by the Broad Institute. Currently, the GATK conventions for read group tags are these:\n\nID : any string that uniquely identifies a read group. The user must make sure that these uniquely identify the read groups. If you have multiple samples, libraries, flowcells, and lanes, and easy way to ensure this is to just bung those all together in a format like {Sample}.{Library}.{Flowcell}.{Lane}, where the parts in curly braces should be replaced by the actual names of those items. As long as there is a PU field, however, you can make the ID anything you want, as long as it is unique for each read group.\nSM : any string (no spaces please!) that serves as an identifier for the individual the sample was taken from. This is used to assign genotypes to indvididuals, so even if you have different “samples” from the same individual (i.e. some blood and some tissue, prepped in different libraries; or an original extraction prepared in an earlier library and a later extraction prepared in a library for individuals that needed more sequening coverage after an initial sequencing run) you should still assign them all the same SM tag because they are the same individual, and you will’ want all of the DNA sequences from them to apply to calling genotypes for them.\nLB : any string which gives the name of the library prep these reads were prepared in. If you prepped 192 individuals in one day, they are all part of the same library.\nPU : GATK expects this information to be given in a specific format. You can provide it as {Flowcell}.{Lane} (for example, HTYYCBBXX.1), but with later versions of the base quality score recalibration methods in GATK, it appears that it might be useful to also add information about the particular individual/library the read came from. It is suggested that this be done by including the barcode of reads, as {Flowcell}.{Lane}.{Barcode}, like HTYYCBBXX.1.TCAATCCG+TTCGCAGT.\nPL : It is also important for base calibration to indicate what type of sequencer was used. Most of the time, currently, that will be ILLUMINA.\n\nUsually, when you get data back from the sequencing center, it will be in a series of different FASTQ files, each one containing reads from a single DNA sample well (on a plate that you sent to them), that was sequenced on a certain lane on a given flow cell. The files will often be named something like: DPCh_plate1_C07_S31_L7_R2.fq.gz where the first part gives a sample identifier, the part after the L gives the lane, and the part after the R tells whether the file holds read 1 or read 2 of paired end reads. You can get the flowcell name by looking at the sequence names inside the file (See Section @ref(illumina-ids)). You will map the reads in each file separately, and when you do so, you will attach the read group information to them (see below). Accordingly, it is a great idea to maintain a spreadsheet that links the different files to the attributes and the read group information of the reads inside them, like this:\nindex   file_prefix ID  PU  SM  PL  LB  Flowcell    Lane\n1   DPCh_plate1_A01_S1_L1_R HTYYCBBXX.1.CH_plate1_A01   HTYYCBBXX.1.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   1\n2   DPCh_plate1_A01_S1_L2_R HTYYCBBXX.2.CH_plate1_A01   HTYYCBBXX.2.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   2\n3   DPCh_plate1_A01_S1_L3_R HTYYCBBXX.3.CH_plate1_A01   HTYYCBBXX.3.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   3\n4   DPCh_plate1_A01_S1_L4_R HTYYCBBXX.4.CH_plate1_A01   HTYYCBBXX.4.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   4\n5   DPCh_plate1_A01_S1_L5_R HTYYCBBXX.5.CH_plate1_A01   HTYYCBBXX.5.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   5\n6   DPCh_plate1_A01_S1_L6_R HTYYCBBXX.6.CH_plate1_A01   HTYYCBBXX.6.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   6\n7   DPCh_plate1_A01_S1_L7_R HTYYCBBXX.7.CH_plate1_A01   HTYYCBBXX.7.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   7\n8   DPCh_plate1_A01_S1_L8_R HTYYCBBXX.8.CH_plate1_A01   HTYYCBBXX.8.TCAATCCG+TTCGCAGT   CH_plate1_A01   ILLUMINA    Lib-1   HTYYCBBXX   8\n9   DPCh_plate1_A02_S2_L1_R HTYYCBBXX.1.CH_plate1_A02   HTYYCBBXX.1.CGCTACAT+CGAGACTA   CH_plate1_A02   ILLUMINA    Lib-1   HTYYCBBXX   1\nIn the exercise we will see how to use such a file to assign read groups when mapping.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sequence alignment and handling BAM files</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#aligning-reads-with-bwa",
    "href": "nmfs-bioinf/sequence-alignment.html#aligning-reads-with-bwa",
    "title": "9  Sequence alignment and handling BAM files",
    "section": "9.3 Aligning reads with bwa",
    "text": "9.3 Aligning reads with bwa\nThere are several programs for aligning reads to a reference genome. We focus on bwa which is an industry standard aligner written by Heng Li and Richard Durbin [@liFastAccurateShort2009]. The name stands for “Burrows-Wheeler Aligner.” We won’t go deeply into the guts of this alignment algorithm, but we will briefly state that nearly all alignment methods rely on pre-processing the reference genome into a data-structure (like a suffix tree) that provides an index which makes it fast to find the place in a genome where a query sequence matches. Such indexes can take up a large amount of computer memory. The Burrows-Wheeler Transform (BWT) provides a way of decreasing the size of such indexes. The BWT is an operation that takes a sequence of characters (in this case DNA bases) and re-orders them so that similar characters tend to appear together in long runs of the same character. Sequences with long runs of the same character can be easily compressed to take up less space using run length encoding, for example. (We have already seen an example of run length encoding in the CIGAR string (Section @ref(cigar)), in which runs of the same kind of alignment characteristic (Matches, Deletions, etc) where encoded by their type and the length of the run.) The remarkable thing about the BWT is that it is invertible: if someone gives you a sequence and says that it is the result of applying the BWT to an original sequence, you can actually recover the original sequence without any other information! The program bwa uses the BWT to compress the index used for alignment so that it can easily fit into memory—even for very large genomes—on most any computer system, even a low-powered laptop produced in the last half decade.\n\n9.3.1 Indexing the genome for alignment\nAs the foregoing discussion suggests, it is necessary to index a genome before aligning to it with bwa. This is a step that only needs to be done once, since the process creates an index that is stored on the hard drive alongside the genome. So, after you download a reference genome, before you can start aligning sequences to it, you must index it using bwa. The syntax for this is very straightforward:\nbwa index path-to-genome\nFor example:\nbwa index genome/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz\nThe reference genome must be stored as a FASTA file (Section @ref(fasta)), which can be compressed using gzip. (In the above example, the .fna.gz prefix means that the file is a FASTA file of nucleotides (.fna) and has been gzipped (.gz)).\nThis process may take several hours, depending upon the size of the reference genome. When it is complete, several new files with names that are the orginal reference genome file plus several extensions will have been produced and saved in the same directory as the reference genome. In the above example, after indexing, a long listing of the directory with the genome file in it shows these files:\n% ls -hl\ntotal 3.8G\n-rw-rw-r-- 1 eanderson eanderson 704M Mar  5 00:08 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz\n-rw-rw-r-- 1 eanderson eanderson 4.4M Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.amb\n-rw-rw-r-- 1 eanderson eanderson 2.1M Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.ann\n-rw-rw-r-- 1 eanderson eanderson 2.3G Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.bwt\n-rw-rw-r-- 1 eanderson eanderson 579M Mar  5 00:47 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.pac\n-rw-rw-r-- 1 eanderson eanderson 1.2G Mar  5 00:58 GCA_002872995.1_Otsh_v1.0_genomic.fna.gz.sa\nThe first file is the original gzipped compressed reference genome, and the remaining files are all part of the index. Note that the index files occupy considerably more space than the original reference file (but not so much as they would if the BWT were not used to reduce the size of the index).\nIf you ever wonder whether you have indexed a certain reference genome with bwa, it is a simple matter to go to the directory holding the reference genome and see what other files are there. If you find files with the same name, but having the suffixes, .amb, .ann, .bwt, .pac and .sa, that means that the reference genome has already been indexed.\n\n\n9.3.2 Mapping reads with bwa mem\nOnce the reference genome has been indexed, you are ready to align reads to it. The program bwa includes several different alignment algorithms. For paired-end Illumina data, the best available bwa algorithm, today, is the mem algorithm. The syntax is very simple. While there are many options available to tune the parameters used for alignment, the default values usually give good performance. Thus, at its simplest, bwa mem simply takes three file arguments:\nbwa mem   path-to-reference-genome   path-to-read1-fastq-file   path-to-read2-fastq-file\nAn example might look like:\nbwa mem genome/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz fastqs/DPCh_plate1_A01_S1_L1_R1.fq.gz fastqs/DPCh_plate1_A01_S1_L1_R2.fq.gz\nbwa mem prints progress messages to stderr and prints its output in SAM format to stdout. The read alignments come spewing out in the order they are listed in the FASTQ files, such that each read and its paired-end mate appear in adjacent rows in the output. It’s all super simple!\n\n\n9.3.3 Hold it Right There, Buddy! What about the Read Groups?\nIndeed! It is during the mapping with bwa mem that we must include read group tags for our reads. This is straightforward if the pair of FASTQ files contains reads that are all from a single read group, we merely have to give the read group information that we want to see as the argument to the -R option of bwa mem. This argument must be a single token on the command line, so, if it does not include spaces or other whitespace it can be supplied unquoted; however, it is usually safer to quote the argument on the command line. The tabs that occur between read group tags are given as \\t in the argument to the -R option.\nThus, in the bwa mem documentation, the example invocation of the -R option shows it single quoted as '@RG\\tID:foo\\tSM:bar'. In other words, you pass it the read group header line, complete with the @RG header tag, encoding the tabs between identifiers with \\t. What this example does not make clear is that you can also use double quotes around that argument. Double quotes, you will recall, allow variable substitution (Section @ref(quotes-and-var-subs)) to occur inside them. So, in your own scripts, especially when cycling over very many different pairs of FASTQ files, it is extremely useful to be able to define the values of the read group tags, and then supply them on the command line. Coarsely this would look like:\nID=HTYYCBBXX.1.CH_plate1_A01\nSM=CH_plate1_A01\nLB=Lib-1\nPU=HTYYCBBXX.1.TCAATCCG+TTCGCAGT\nPL=ILLUMINA\n\nbwa mem -R \"@RG\\tID:$ID\\tSM:$SM\\tLB:$LB\\tPU:$PU\\tPL:$PL\"  genome.fasta  file_R1.fq file_R2.fq\nHowever, you would probably want some way of assigning the values of the shell variables ID, SM, etc., programmatically, perhaps from the spreadsheet that holds all that information. The exercise this week shows one example of that using awk and a TAB-delimited spreadsheet.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sequence alignment and handling BAM files</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/sequence-alignment.html#samtools",
    "href": "nmfs-bioinf/sequence-alignment.html#samtools",
    "title": "9  Sequence alignment and handling BAM files",
    "section": "9.4 Processing alignment output with samtools",
    "text": "9.4 Processing alignment output with samtools\nDoing the alignment with bwa mem is only the first step of getting data ready to do variant calling. Once the SAM format data comes out of bwa mem, a number of things must happen to it before it can be used for variant calling:\n\nIt must be converted to BAM format (the compressed binary companion format to SAM)\nAdditional information about mate pairs might need to be attached to sequences\nYou must sort the BAM files from the original ordering produced by bwa mem into a coordinate-ordered format, in which the reads are sorted by where they appear within the reference genome.\nIf you started with multiple pairs of FASTQ files for a single individual (i.e. from different lanes or from different sequencing runs or libraries) you might need to merge those all into a single BAM file before variant calling.\nYou might want to mark PCR duplicates as such in the BAM file.\nFinally when you have your BAM file all ready for variant calling, you typically will need to index it for rapid access by the variant caller program.\n\nPhew! That is a lot of steps! Fortunately, there is a single “go-to” program (one-stop shopping!) that can manage all of that for you. It is the program samtools, brought to you by the same people who developed the SAM/BAM formats [@liSequenceAlignmentMap2009] and the bwa aligner (and bcftools for that matter). If you want to get anywhere in bioinformatics, it is important to become good friends with samtools.\nThe program samtools includes a large number of different subcommands. We will cover just a few of them, here, that are used in our typical paired-end workflow for whole genome sequencing, and in a somewhat cursory fashion, and we will disscuss only the most commonly used options. All readers are encouraged to completely read the online manual page for samtools for a complete account of its uses.\nOr, you can read the usage guidelines for any subcommand from the command line:\nconda activate bwasam\n\n# This lists all the different subcommands for you, categorized by\n# their uses\nsamtools  \n\n# if you follow samtools by any subcommand with nothing else, it gives you information\n# about all the options of that subcommand.  For example:\nsamtools view\n\nsamtools sort\nThe usage patterns of the subcommands can broadly be categorized into two groups:\n\nSubcommands that explicitly require an argument that gives the name of an output file. These subcommands are usually described like this:\n\n\nsamtools subcommand [options] out.bam in.bam\n\nwhere an output file is explicitly called for. These commands do not write output to stdout! They must be given an output file name. It is not possible to pipe output from one of these subcommands to another program or utility.\n\nSubcommands that don’t explicitly require an argument giving an output file:\n\n\nsamtools subcommand [options] in.bam\n\nThese subcommands write their output to stdout, and that output can be piped into other programs or can be redirected into a file (some also provide a -o file option to write output to file rather than to stdout.)\nIn short, if the syntax of the subcommand explicitly calls for an output file on the command line (not as part of a -o file option), then that subcommand does not write its output to stdout and you can’t pipe it into the next part of your pipeline.\nOn the other hand, if you can write output from samtools to stdout, you might be interested in knowing how you can pipe that into the input for another samtools subcommand. Rejoice! There is a syntactically economical and lovely way to do that. Any subcommand which expects a single input file will be described like:\nsamtools subcommand [options]  in.bam\nYou would typically put the path to the input file in the place of in.bam there. However, if you put - in place of in.bam then samtools will take input for in.bam from stdin.\nSo, for example, you can do something like this:\nbwa mem genome.fna R1.fq R2.fq |\n  samtools view -u -  |   # convert the SAM output from bwa mem into BAM format\n  samtools sort -l 9  - -o output_file.bam  # take stdin as the input, sort it, and write (with the best\n                                        # compression possible: -l 9) the output to output_file.bam\nThis is HUGE! It means that you don’t even have to save the initial SAM file that bwa mem makes, you can proceed directly to the sorted BAM file you want without eating up disk space on the intermediate files.\n\n9.4.1 samtools subcommands\nWe will go through some of the samtools subcommands that are particularly important in processing whole genome sequencing data in a step-by-step fashion, so you can get some experience with them, rather than just running them as parts of long and complex pipelines.\nTo do these steps, you will need to have a SAM file that is in the location example-files/s001---1.sam relative to the current working directory. To get that, you might need to sync your fork and then pull that into main.\nAlso, you will need samtools. On Alpine you can get that with\nmodule load samtools\nfrom a compute node.\nYou should be doing this on an interactive shell.\n\n9.4.1.1 samtools view\nThe view command is a major samtools workhorse.\nIt is often used to convert between SAM and BAM format, or to view them easily. Look at the options:\nsamtools view\nTo convert from SAM to BAM format specify that the output should be BAM with the -b option:\nsamtools view -b example-files/s001---1.sam &gt; example-files/s001---1.bam\nOnce you have done that, compare the size of the SAM and BAM versions of the same data:\ndu -h example-files/*\nHow much of a reduction in storage is that?\nNote that you can’t read a BAM file as a human. Try it with head and get the universal symbols of “HEY THIS IS A BINARY FILE!!”\nhead example-files/s001---1.bam\nYou can use samtools view to look at the alignments in a bam file. By default it doesn’t show you the SAM header:\n# pipe output to head to look at the first 10 alignments\nsamtools view example-files/s001---1.bam | head\nIf you want the output to inlude the header, you use the -h option:\nsamtools view -h example-files/s001---1.bam | head\nIf all you want to print is the SAM header, then use the -H command:\n# this is a handy way to get the last N (12 in this case) lines of the\n# SAM header:\nsamtools view -H example-files/s001---1.bam | tail -n 12\nCheck out that read group line! Also, note that some PG lines have been added there\nIn general, you will almost exclusively use BAM format instead of SAM format in bioinformatics, because it is much faster to process BAM input that SAM input. But it is worth understanding that BAM format can come in uncompressed or highly compressed versions. The compressed versions take up less disk space, but could require more time to operate on.\nWhen samtools view is used for converting from the text-based SAM format to BAM format, you might not want to spend time compressing the output. This is particularly true if you are piping the output back into another command that would just have to de-compress the BAM output in order to use it. If that is the case, then BAM output using the -u option is preferred over the -b option, since -u makes BAM output, but it saves the time that would be spent compressing and then decompressing the data as it gets piped from one samtools command to the next.\nWe will see an example of that when we do sorting.\n\n\n9.4.1.2 samtools sort\nThis subcommand sorts the alignments in a BAM file in order of their placement in the reference genome. This must be done for many downstream operations, so it is something that you will often do to alignments that come out of bwa mem or any other aligner.\nWhen a BAM file is sorted by genomic coordinates (i.e., by order of the placement of the alignments in the genome), it is said to be “coordinate-sorted.”\nFirst notice that our example-files/s001---1.sam file is not coordinate-sorted. Just look at it with less -S and note that the successive sequences are on different chromosomes/scaffolds and are not in sorted order at all!\nHere we will coordinate sort the BAM file we made above. We will put it into a file called example-files/s001---1.srt.bam. First, look at the samtools sort syntax:\nsamtools sort\nNow, let’s sort the file and make sure it is well compressed:\nsamtools sort -l 9 -o example-files/s001---1.srt.bam example-files/s001---1.bam\nCheck out how big the resulting file is:\ndu -h example-files/*\nCool! Even smaller than the original BAM file!\nFor fun, check out the header for the file and see that it now has an @HD line:\nsamtools view -H example-files/s001---1.srt.bam | head\nAnd, for fun, look at the @PG lines at the bottom of the header:\nsamtools view -H example-files/s001---1.srt.bam | tail -n 10\nNote that every command that has been used to modify the file contents is there!\nsamtools sort produces intermediate files when it is sorting large BAM files. You will sometimes see them if ls-ing the contents of directories while running a sort job. You used to have to be very careful if sorting multiple files at the same time that those intermediate files didn’t overwrite one another, but that is largely fixed/taken care of automatically by versions of samtools sort available today.\nNote that, instead of making the intermediate bam file, we could have piped BAM output into sort. This is very useful:\n# remove the .bam .srt.bam files:\nrm example-files/s001---1.bam example-files/s001---1.srt.bam\n\n# now, directly make a sorted BAM file at example-files/s001---1.srt.bam\n# by piping samtools view output into samtools sort.  Note the use\n# of -u for uncompressed BAM output, and the - at the end of the\n# line, instead of a file name, to mean take\n# input from stdin instead of a file\nsamtools view -u example-files/s001---1.sam | \\\n  samtools sort -l 9 -o example-files/s001---1.srt.bam -\n\n\n9.4.1.3 samtools index\nThe samtools index command operates on a coordinate-sorted BAM file and creates a new file having the name of the original file, but with the extension .bai added. .bai stands for “bam index.” It is an index that makes it very fast to access the alignments within certain genomic regions.\nWe can index our sorted BAM file like this:\nsamtools index example-files/s001---1.srt.bam\nOnce we’ve done that, let’s see how big that .bai file is:\ndu -h example-files/*\nIt’s pretty small! It is also a binary file.\nIf you have indexed a BAM file, you can use samtools view to retrieve and print all the reads in that indexed BAM file covering one or more genomic regions as specified using genomic coordinates. This happens very fast.\nFor example, to see all the read that overlap the 100 base pairs from 1001 to 1100 on chromosome CM031202.1, you would do this.\nsamtools view example-files/s001---1.srt.bam CM031202.1:1001-1100\nIn the full dataset there are three sequences that overlap that region. In the small data set, if you directly downloaded it today, there might not be any.\n\n\n9.4.1.4 samtools merge\nIf you have multiple sorted BAM files that you wish to merge into a single BAM file, you can use samtools merge. This subcommand opens up connections to each of the files and steps through each of them, alignment-by-alignment, copying alignments from each of the original BAMs into the merged BAM file. Thought of this way, it is clear why the input BAM files must be sorted. The merged BAM in outputted in sorted order.\nThere is some arcana with this utility when you have different reference genomes represented in your different BAMs, and older versions of samtools merge didn’t always play nicely with mutliple read groups in different files. For the most part, those issues were resolved in later releases. So, if you have just mapped a bunch of fastqs into BAMs and have sorted them, samtools merge should work and behave appropriately when merging those files.\nWhy should you merge files? The reason we typically to it is to make a single file that holds all the reads that came from one individual in one library prep for finding PCR duplicates. Alternatively, you might want to merge all BAMS from a single individual into a single BAM for variant calling.\nBack in the “old days” people would sometimes merge all the BAMs from multiple individuals. This is definitely not recommended today. It is easier to have a single BAM for each sample. Some utilities, like ANGSD, even require that input for variant discovery be done with one individual per BAM. So, you should almost never take BAM-merging further than merging all the alignments for a single individual (i.e., when you are done you should have one BAM file per individual.)\nThe syntax for samtools merge is:\nsamtools merge [options] output-bam-name.bam  sorted-input-bam-1.bam sorted-input-bam-2.bam ...\nIn other words, you give it the name of the output file you want it to produce, followed by all the input file names. If you get the order of the filenames wrong—for example putting the output file last, then it will usually bark you an error because you are, in effect, asking it to overwrite an existing input file.\nWe don’t have multiple files to merge at this point, but you might see this at a later time.\n\n\n9.4.1.5 samtools flagstats\nProvide summary information about the SAM flags of the alignments. This is a great utility to assess how many reads mapped, how many didn’t, and the nature of their mapping (as relayed by their SAM flags).\nsamtools flagstats example-files/s001---1.srt.bam\n\n\n9.4.1.6 samtools idxstats\nFrom an indexed BAM file, report how many reads map to each sequence in the reference genome.\nsamtools idxstats example-files/s001---1.srt.bam | less\nThe columns that come out of this are:\n\nReference sequence name\nLength of reference sequence\nNumber of reads that properly paired-end-map to the reference sequence\nNumber of reads that align to the reference sequence, but not as part of a properly paired read\n\n\n\n9.4.1.7 samtools stats\nComprehensive summaries of features of the alignments.\nsamtools stats example-files/s001---1.srt.bam | less\nNote that, in the headers for each of the different summaries in the output from samtools stats, instructions are given on how to extract just that summary.\nPage through the output. It isn’t always easy to read in text format. MULTIQC can gobble it up and make pretty graphs of it.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sequence alignment and handling BAM files</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html",
    "href": "nmfs-bioinf/bioinf-formats.html",
    "title": "10  Bioinformatic file formats",
    "section": "",
    "text": "10.1 Sequences",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#fastq",
    "href": "nmfs-bioinf/bioinf-formats.html#fastq",
    "title": "10  Bioinformatic file formats",
    "section": "10.2 FASTQ",
    "text": "10.2 FASTQ\nThe FASTQ format is the standard format for lightly-processed data coming off an Illumina machine. If you are doing whole genome sequencing, it is typical for the Illumina processing pipeline to have separated all the reads with different barcodes into different, separate files. Typically this means that all the reads from one library prep for one sample will be in a single file. The barcodes and any associated additional adapter sequence is typically also removed from the reads. If you are processing RAD data, the reads may not be separated by barcode, because, due to the vagaries of the RAD library prep, the barcodes might appear on different ends of some reads than expected.\nA typical file extension for FASTQ files is .fq. Almost all FASTQ files you get from a sequencer should be gzipped to save space. Thus, in order to view the file you will have to uncompress it. Since you would, in most circumstances, want to visually inspect just a few lines, it is best to do that with gzcat and pipe the output to head.\nAs we have seen, paired-end sequencing produces two separate reads of a DNA fragment. Those two different reads are usually stored in two separate files named in such a manner as to transparently denote whether it contains sequences obtained from read 1 or read 2. For example bird_08_B03_R1.fq.gz and bird_08_B03_R2.fq.gz. Read 1 and Read 2 from a paired read must occupy the sames lines in their respective files, i.e., lines 10001-10004 in bird_08_B03_R1.fq.gz and lines 10001-10004 in bird_08_B03_R2.fq.gz should both pertain to the same DNA fragment that was sequenced. That is what is meant by “paired-end” sequencing: the sequences come in pairs from different ends of the same fragment.\nThe FASTQ format is very simple: information about each read occupies just four lines. This means that the number of lines in a proper FASTQ file must always be a multiple of four. Briefly, the four lines of information about each read are always in the same order as follows:\n\nAn Identifier line\nThe DNA sequence as A’s, C’s, G’s and T’s.\nA line that is almost always simply a + sign, but can optionally be followed by a repeat of the ID line.\nAn ASCII-encoded, Phred-scaled base quality score. This gives an estimated measure of certainty about each base call in the sequence.\n\nThe code block below shows three reads worth (twelve lines) of information from a FASTQ file. Take a moment to identify the four different lines for each read.\n@K00364:64:HTYYCBBXX:1:1108:4635:14133/1\nTAGAATACGCCAGGTGTAAGAATAGTAGAATACGCCAGGTGTAAGAATAGTAGAACACGCCAGGTGTAAGAATAGTAGAA\n+\nAAAFFJJJJJFFJFJJJFJJFFJFJFJJJJJJJJFFJJJFJJJFJJAJJFJJFJJFJJJ7JJJF-&lt;JAFJJ&lt;F&lt;AJAJJF\n@K00364:64:HTYYCBBXX:1:1108:5081:25527/1\nAAAACACCAAAAGAAAGATGCCCAGGGTCCCTGCTCATCTGCGTGAAAGTGACTTAGGCATGCTGCAAGGAGGCATGAGG\n+\nAAFFFJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ\n@K00364:64:HTYYCBBXX:1:1108:5852:47295/1\nAGGTGGCTCTAGAAGGTTCTCGGACCGAGAAACAGCCTCGTACATTTGCAATGATTTCAATTCATTTTGACCATTACGAA\n+\nAAFFFJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ\nLines 2 and 3 are self-explanatory, but we will expound upon lines 1 and 4 below.\n\n10.2.1 Line 1: Illumina identifier lines\nThe identifier line can be just about any string that starts with an @, but, from Illumina data you might see something like this:\n@K00364:64:HTYYCBBXX:1:1108:4635:14133/1\nThe colons (and the /) are field separators. The separate parts of the line above are interpreted something along the lines as follows (keeping in mind that Illumina occasionally changes the format and that there may be additional optional fields):\n@           : mandatory character that starts the ID line\nK00364      : Unique sequencing machine ID \n64          : Run number on instrument\nHTYYCBBXX   : Unique flow cell identifier\n1           : Lane number\n1108        : Tile number (section of the lane)\n4635        : x-coordinate of the cluster within the tile\n14133       : y-coordinate of the cluster within the tile\n1           : Whether the sequence is from read 1 or read 2 \nFor Illumina data processed with versions 1.8+ of Casava, the identifier lines are a little different, like this:\n@A00600:191:HY75HDSX2:1:2624:27480:35744 2:N:0:AAGACCGT+CAATCGAC\nin which we have:\n@           : mandatory character that starts the ID line\nA00600      : Unique sequencing machine ID \n191         : Run number on instrument\nHY75HDSX2   : Unique flow cell identifier\n1           : Lane number\n2624        : Tile number (section of the lane)\n27480       : x-coordinate of the cluster within the tile\n35744       : y-coordinate of the cluster within the tile\n\n2                 : Whether the sequence is from read 1 or read 2 \nN                 : N if the sequence was not flagged as crappy by the maching, Y otherwise\n0                 : 0 when no control bits are turned on\nAAGACCGT+CAATCGAC : Index/barcode of the sample\nQuestion: For paired reads, do you expect the x- and y-coordinates for read 1 and read 2 to be the same?\n\n\n10.2.2 Line 4: Base quality scores\nThe base quality scores give an estimate of the probability that the base call is incorrect. This comes from data the sequencer collects on the brightness and compactness of the cluster radiance, other factors, and Illumina’s own models for base call accuracy. If we let \\(p\\) be the probability that a base is called incorrectly, then \\(Q\\), the Phred-scaled base quality score, is:\n\\[\nQ = \\lfloor-10\\log_{10}p\\rfloor,\n\\] where \\(\\lfloor x \\rfloor\\) means “the largest integer smaller than \\(x\\).\nTo get the estimate of the probability that the base is called incorrectly from the Phred scaled score, you invert the above equation:\n\\[\np = \\frac{1}{10^{Q/10}}\n\\] Base quality scores from Illumina range from 0 to 40. The easiest values to use as guideposts are \\(Q = 0, 10, 20, 30, 40\\), which correspond to error probabilites of 1, 1 in 10, 1 in 100, 1 in 1,000, and 1 in 10,000, respectively.\nAll this is fine and well, but when we look at the quality score line above we see something like 7JJJF-&lt;JAFJJ&lt;F&lt;AJAJJF. What gives? Well, from a file storage and parsing perspective, it makes sense to only use a single character to store the quality score for every base. So, that is what has been done: each of those single characters represents a quality score—a number between 0 and 40, inclusive.\nThe values that have been used are the decimal representations of ASCII text characters minus 33.\nThe decimal representation or each character can be found in Figure 10.1.\n\n\n\n\n\n\nFigure 10.1: This lovely ASCII table shows the binary, hexadecimal, octal and decimal representations of ASCII characters (in the corners of each square; see the legend rectangle at bottom. Table produced from TeX code written and developed by Victor Eijkhout available at https://ctan.math.illinois.edu/info/ascii-chart/ascii.tex\n\n\n\nThe decimal representation is in the upper left of each character’s rectangle.\nFind the characters corresponding to base quality scores of 0, 10, 20, 30, and 40. Remember that the base quality score is the character’s decimal representation minus 33.\nHere is another question: why do you think the scale starts with ASCII character 33?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#fasta",
    "href": "nmfs-bioinf/bioinf-formats.html#fasta",
    "title": "10  Bioinformatic file formats",
    "section": "10.3 FASTA",
    "text": "10.3 FASTA\nThe FASTQ format, described above, is tailored for representing short DNA sequences—and their associated quality scores—that have been generated from high-throughput sequencing machines. A simpler, leaner format is used to represent longer DNA sequences that have typically been established from a lot of sequencing, and which no longer travel with their quality scores. This is the FASTA format, which you will typically see storing the DNA sequence from reference genomes. FASTA files typically use the file extensions .fa, .fasta, or .fna, the latter denoting it as a FASTA file of nucleotides.\nIn an ideal world, a reference genome would contain a single, uninterrupted sequence of DNA for every chromosome. While the resources for some well-studied species include “chromosomal-level assemblies” which have much sequence organized into chromosomes in a FASTA file, even these genome assemblies often include a large number of short fragments of sequence that are known to belong to the species, but whose location and orientation in the genome remain unknown.\nMore often, in conservation genetics, the reference genome for an organism you are working on might be the product of a recent, small-scale, assembly of a low-coverage genome. In this case, the genome may be represented by thousands, or tens of thousands, of scaffolds, only a few of which might be longer than one or a few megabases. All of these scaffolds go into the FASTA file of the reference genome.\nHere are the first 10 lines of the FASTA holding a reference genome for Chinook salmon:\n&gt;CM008994.1 Oncorhynchus tshawytscha isolate JC-2011-M1\nAGTGTAGTAGTATCTTACCTATATAGGGGACAGTGTAGTAGTATCTTACTTATTTGGGGGACAATGCTCTAGTGTAGTAG\nAATCTTACCTTTATAGGGGACAGTGCTGGAGTGCACTGGTATCTTACCTATATAGGGGACAGTGCTGGAGTGTAGTAGTG\nTCTCGGCCCACAGCCGGCAGGCCTCAGTCTTAGTTAGACTCTCCACTCCATAAGAAAGCTGGTACTCCATCTTGGACAGG\nACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA\nTGTGTGGTACATGTTTACAGAGAAGGAGtatattaaaaacagaaaactgTTTTGGttgaaatatttgtttttgtctgaAG\nCCCGAAAAACACATGAAATTCAAAAGATAATTTGACCTACGCACTAACTAGGCTTTTCAGCAGCTCAACTACTGTCCGTT\nTATTGATCTACTGTACTGCAACAACATATGTACTCACACAACAGACTATATTGGATTCAGACAGGACCTATAGGTTACCA\nTGCTTCCTCTCTACAGGACCTATAGGTTACCATGCTTCCTCTCTACAAGGTCTATAGGTTACCATGCGTCCTCTCTACAG\nGACCTATAGGTTACCATGCTTCCTCTCTACAGGGCCTATAGGTTACCATGCTTCCTCTCTACAGGACCTGTAGGTTACCA\nThe format is straightforward: any line starting with &gt; is interpreted as a line holding the identifier of the following sequence. In this case, the identifier is CM008994.1. The remainder of the line (following the first white space) are further comments about the sequence, but will not typically be carried through downstream analysis (such as aligment) pipelines. In this case CM008994.1 is the name of an assembled chromosome in this reference genome. The remaining lines give the DNA sequence of that assembled chromosome.\nIt is convention with FASTA files that lines of DNA sequence should be less than 80 characters, but this is inconsistently enforced by different analysis programs. However, most of the FASTA files you will see will have lines that are 80 characters long.\nIn the above fragment, we see DNA sequence that is either upper or lower case. A common convention is that the lowercase bases are segments of DNA that have been inferred (by, for example RepeatMasker) to include repetitive DNA. It is worth noting this if you are trying to design assays from sequence data! However, not all reference genomes have repeat-sequences denoted in this fashion.\nMost reference genomes contain gaps. Sometimes the length of these gaps can be accurately known, in which case each missing base pair is replaced by an N. Sometimes gaps of unknown length are represented by a string of N’s of some fixed length (like 100).\nFinally, it is worth reiterating that the sequence in a reference-genome FASTA file represents the sequence only one strand of a double-stranded molecule. In chromosomal-scale assemblies there is a convention to use the strand that has its 5’ end at the telomere of the short arm of the chromosome [@cartwrightMultiplePersonalitiesWatson2011]. Obviously, such a convention cannot be enforced in a low-coverage genome in thousands of pieces. In such a genome, different scaffolds will represent sequence on different strands; however the sequence in the FASTA file, whichever strand it is upon, is taken to be the reference, and that sequence is referred to as the forward strand of the reference genome.\n\n10.3.1 Genomic ranges\nAlmost every aspect of genomics or bioinformatics involves talking about the “address” or “location” of a piece of DNA in the reference genome. These locations within a reference genome can almost universally be described in terms of a “genomic range” with a format that looks like:\nSegmentName:start-stop\nFor example,\nCM008994.1:1000001-1001000\ndenotes the 1 Kb chunk of DNA starting from position 1000001 and proceeding to (and including!) position 1001000. Such nomenclature is often referred to as the genomic coordinates of a segment.\nIn most applications we will encounter, the first position in a chromosome is labeled 1. This is called a base 1 coordinate system. In some genomic applications, a base 0 coordinate system is employed; however, for the most part such a system is only employed internally in the guts of code of software that we will use, while the user interface of the software consistently uses a base 1 coordinate system.\nSometimes you will have to convert between base 0 and base1 coordinate systems. Thinking about how to do this is easy if you keep a simple picture in your head—I like to think of it as, the base-1 coordinate system counts “beads” that are the actual base pairs, while the base-0 system counts “fences” that separate the beads. In the following picture, the fences and their corresponding numbers are red, while the beads and their corresponding numbers are black.\n\n\n\nBase-0 vs Base-1 coordinate systems in terms of fences and beads\n\n\nFrom this picture, it is pretty clear that if you have a base-1 range from \\(x\\) to \\(y\\) (with \\(x \\leq y\\)), then you could refer to that by the base-0 range from \\(x-1\\) to \\(y\\) (those are the “fences” that contain the beads). Likewise, if you have a base-0 range from \\(v\\) to \\(w\\) (with \\(v&lt;w\\)), then the corresponding base-1 range would be from \\(v+1\\) to \\(w\\), as those are the beads that are contained by the fences at \\(v\\) and \\(w\\).\n\n\n10.3.2 Extracting genomic ranges from a FASTA file\nCommonly (for example, when designing primers for assays) it is necessary to pick out a precise genomic range from a reference genome. This is something that you should never try to do by hand. That is too slow and too error prone. Rather the software package samtools (which will be discussed in detail later) provides the faidx utility to index a FASTA file. It then uses that index to provide lightning fast access to specific genomic coordinates, returning them in a new FASTA file with identifiers giving the genomic ranges. Here is an example using samtools faidx to extract four DNA sequences of length 150 from within the Chinook salmon genome excerpted above:\n# assume the working directory is where the fasta file resides\n\n# create the index\nsamtools faidx GCA_002831465.1_CHI06_genomic.fna\n\n# that created the file: GCA_002831465.1_CHI06_genomic.fna.fai\n# which holds four columns that constitute the index\n\n# now we extract the four sequences:\nsamtools faidx \\\n    GCA_002831465.1_CHI06_genomic.fna \\\n    CM009007.1:3913989-3914138 \\\n    CM009011.1:2392339-2392488 \\\n    CM009013.1:11855194-11855343 \\\n    CM009019.1:1760297-1760446\n    \n# the output is like so:\n&gt;CM009007.1:3913989-3914138\nTTACCGAtggaacattttgaaaaacacaaCAATAAAGCCTTGTGTCCTATTGTTTGTATT\nTGCTTCGTGCTGTTAATGGTAgttgcacttgattcagcagccgtAGCGCCGGGAAggcag\ntgttcccattttgaaaaaTGTCATGTCTGA\n&gt;CM009011.1:2392339-2392488\ngatgcctctagcactgaggatgccttagaccgctgtgccactcgggaggccttcaGCCTA\nACTCTAACTGTAAGTAAATTGTGTGTATTTTTGGGTACATTTCGCTGGTCCCCACAAGGG\nGAAAGggctattttaggtttagggttaagg\n&gt;CM009013.1:11855194-11855343\nTGAGGTTTCTGACTTCATTTTCATTCACAGCAGTTACTGTATGCCTCGGTCAAATTGAAA\nGGAAAGTAAAGTAACCATGTGGAGCTGtatggtgtactgtactgtactgtattgtactgt\nattgtgtgGGACGTGAGGCAGGTCCAGATA\n&gt;CM009019.1:1760297-1760446\nttcccagaatctctatgttaaccaaggtgtttgcaaatgtaacatcagtaggggagagag\naggaaataaagggggaagaggtatttatgactgtcataaacctacccctcaggccaacgt\ncatgacactcccgttaatcacacagactGG\n\n\n10.3.3 Downloading reference genomes from NCBI\nI might want to write blurb here about how much nicer I find it is to use the ftp link: http://ftp.ncbi.nlm.nih.gov/genomes/genbank/ to find genomes on Genbank. Although now that the number of genomes is growing so quickly, it can take quite a while for the pages to load. But I still find it easier to navigate to exactly the files I want using this system than the actual user interface that they have.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#sambamfiles",
    "href": "nmfs-bioinf/bioinf-formats.html#sambamfiles",
    "title": "10  Bioinformatic file formats",
    "section": "10.4 Alignments",
    "text": "10.4 Alignments\nA major task in bioinformatics is aligning reads from a sequencing machine to a reference genome. We will discuss the operational features of that task in a later chapter, but here we treat the topic of the SAM, or Sequence Alignment Map, file format which is widely used to represent the results of sequence alignment. We attempt to motivate this topic by first considering a handful of the intricacies that arise during sequence alignment, before proceeding to a discussion of the various parts of the SAM file that are employed to handle the many and complex ways in which DNA alignments can occur and be represented. This will necessarily be an incomplete and relatively humane introduction to SAM files. For the adventurous a more complete—albeit astonishingly terse—description of the SAM format specification is maintained and regularly updated.\n\n10.4.1 How might I align to thee? Let me count the ways…\nWe are going to consider the alignment of very short (10 bp) paired-end reads from the ends of a short (50 bp) fragment from the fourth line of the FASTA file printed above. In other words, those 80 bp of the reference genome are:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\nAnd we will be considering double-stranded DNA occupying the middle 50 base pairs of that piece of reference genome. That piece of double stranded DNA looks like:\n5'  ACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGA  3'\n    ||||||||||||||||||||||||||||||||||||||||||||||||||\n3'  TGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACT  5'\nIf we print it alongside (underneath, really) our reference genome, we can see where it lines up:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGA  3'\n                   ||||||||||||||||||||||||||||||||||||||||||||||||||\n               3'  TGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACT  5'\nNow, remember that any template being sequenced on an Illumina machine is going to be single-stranded, and we have no control over which strand, from a double-stranded fragment of DNA, will get sequenced. Futhermore, recall that for this tiny example, we are assuming that the reads are only 10 bp long. Ergo, if everything has gone according to plan, we can expect to see two different possible templates, where I have denoted the base pairs that do not get sequenced with -’s\neither:\n               5'  ACCTGCAGGA------------------------------AACACAGTGA  3'\nor:\n               3'  TGGACGTCCT------------------------------TTGTGTCACT  5'\nIf we see the top situation, we have a situation in which the template that reached the lawn on the Illumina machine comes from the strand that is represented in the reference genome. This is called the forward strand. On the other hand, the bottom situation is one in which the template is from the reverse complement of the strand represented by the reference. This is called the reverse strand.\nNow, things start to get a little more interesting, because we don’t get to look at the the entire template as one contiguous piece of DNA in the 5’ to 3’ direction. Rather, we get to “see” one end of it by reading Read 1 in the 5’ to 3’ direction, and then we “see” the other end of it by reading Read 2, also in the 5’ to 3’ direction, but Read 2 is read off the complementary strand.\nSo, if we take the template from the top situation:\nthe original template is:\n               5'  ACCTGCAGGA------------------------------AACACAGTGA  3'\n               \nSo the resulting reads are:\n\nRead 1:        5'  ACCTGCAGGA  3'   --&gt; from 5' to 3' on the template\nRead 2:        5'  TCACTGTGTT  3'   --&gt; the reverse complement of the\n                                        read on the 3' end of the template\nAnd if we take the template from the bottom scenario:\nthe original template is:\n               3'  TGGACGTCCT------------------------------TTGTGTCACT  5'\n               \nSo the resulting reads are:\n\nRead 1:        5'  TCACTGTGTT  3'   --&gt; from 5' to 3' on the template\nRead 2:        5'  ACCTGCAGGA  3'   --&gt; the reverse complement of the\n                                        read on the 3' end of the template\nAha! Regardless of which strand of DNA the original template comes from, sequences must be read off of it in a 5’ to 3’ direction (as that is how the biochemistry works). So, there are only two possible sequences you will see, and these correspond to reads from 5’ to 3’ off of each strand. So, the only difference that happens when the template is from the forward or the reverse strand (relative to the reference), is whether Read 1 is from the forward strand and Read 2 is from the reverse strand, or whether Read 1 is from the reverse strand and Read 2 is from the forward strand. The actual pair of sequences you will end up seeing is still the same.\nSo, to repeat, with a segment of DNA that is a faithful copy of the reference genome, there are only two read sequences that you might see, and as we will show below Read 1 and Read 2 must align to opposite strands of the reference.\nWhat does a faithful segment from the reference genome look like in alignment? Well, in the top case we have:\n      Read 1:  5'  ACCTGCAGGA  3' \n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\nforward-strand\n    ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'  \n                                              Read 2:  3'  TTGTGTCACT  5'\nAnd in the bottom case we have:\n      Read 2:  5'  ACCTGCAGGA  3' \n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'  \nforward-strand\n    ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'\n                                              Read 1:  3'  TTGTGTCACT  5'\nNote that, although one of the reads always aligns to the reverse strand, the position at which it is deemed to align is still read off of the position on the forward strand. (Thank goodness for that! Just think how atrocious it would be if we counted positions of fragments mapping to the reverse strand by starting from the reverse strands’s 5’ end, on the other end of the chromosome, and counting forward from there!!)\nNote that the alignment position is one of the most important pieces of information about an alignment. It gets recorded in the POS column of an alignment. It is recorded as the first position (counting from 1 on the 5’ end of the forward reference strand) at which the alignment starts. Of course, the name of the reference sequence the read maps to is essential. In a SAM file this is called the RNAME or reference name.\nBoth of the last two alignments illustrated above involve paired end reads that align “properly,” because one read in the pair aligns to the forward strand and one read aligns to the reverse strand of the reference genome. As we saw above, that is just what we would expect if the template we were sequencing is a faithful copy (apart from a few SNPs or indels) of either the forward or the reverse strand of the reference sequence. In alignment parlance we say that each of the reads is “mapped in a proper pair.” This is obviously an important piece of information about an alignment and it is recorded in a SAM file in the so-called FLAG column for each alignment. (More on these flags later…)\nHow can a read pair not be properly mapped? There are a few possibilities:\n\nOne read of the pair gets aligned, but the other does not. For example something that in our schematic would look like this:\n\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGA  3'\n\nBoth reads of the pair map to the same strand. If our paired end reads looked like:\n\nRead 1:   5'  ACCTGCAGGA  3'\nRead 2:   5'  AACACAGTGA  5'\nthen they would both align nicely to just the forward strand of the reference genome:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGA------------------------------AACACAGTGA  3'\nAnd, as we saw above, this would indicate that the template must not conform to the reference genome in some way. This may occur if there is a rearrangement (like an inversion, etc.) in the genome being sequenced, relative to the reference genome.\n\nThe two different reads of a pair get aligned to different chromosomes/scaffolds or they get aligned so far apart on the same chromosome/scaffold that the alignment program determines the pair to be aberrant. This evaluation requires that the program have a lot of other paired end reads from which to estimate the distribution, in the sequencing library, of the template length—the length of the original template. The template length for each read pair is calculated from the mapping positions of the two reads and is stored in the TLEN column of the SAM file.\n\nWhat is another way I might align to thee? Well, one possbility is that a read pair might align to many different places in the genome (this can happen if the reads are from a repetitive element in the genome, for example). In such cases, there is typically a “best” or “most likely” alignment, which is called the primary alignment. The SAM file output might record other “less good” alignments, which are called secondary alignments and whose status as such is recorded in the FLAG column. The aligner bwa mem has an option to allow you to output all secondary alignments. Since you don’t typically output and inspect all secondary alignments (something that would be an unbearable task), most aligners provide some measure of confidence about the alignment of a read pair. The aligner, bwa, for example, looks at all possible alignments and computes a score for each. Then it evaluates confidence in the primary alignment by comparing its score to the sum of the scores of all the alignments. This provides the mapping quality score found in the MAPQ column of a SAM file. It can be interpreted, roughly, as the probability that the given alignment of the read pair is incorrect. These can be small probabilities, and are represented as Phred scaled values (using integers, not characters!) in the SAM file.\nThe last way that a read might align to a reference is by not perfectly matching every base pair in the reference. Perhaps only the first part of the read matches base pairs in the reference, or maybe the read contains an insertion or a deletion. For example, if instead of appearing like 5'  ACCTGCAGGA  3', one of our reads had an insertion of AGA, giving: 5'  ACCAGAGTGCAGGA  3', this fragment would still align to the reference, at 10 bp, and we might record that alignment, but would still want a compact way of denoting the position and length of the insertion—a task handled by the CIGAR column.\nTo express all these different ways in which an alignment can occur, each read occupies a single line in a SAM file. This row holds information about the read’s alignment to the reference genome in a number of TAB-delimited columns. There are 11 required columns in each alignment row, after which different aligners may provide additional columns of information. Table 10.1 gives a brief description of the 11 required columns (intimations of most of which occurred in ALL CAPS BOLDFACE in the preceding paragraphs. Some, like POS are relatively self-explanatory. Others, like FLAG and CIGAR benefit from further explanation as given in the subsections below.\n\n\n\n\nTable 10.1: Brief description of the 11 required columns in a SAM file.\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nField\nData Type\nDescription\n\n\n\n\n1\nQNAME\nString\nName/ID of the read (from FASTQ file)\n\n\n2\nFLAG\nInteger\nThe SAM flag\n\n\n3\nRNAME\nString\nName of scaffold/chromosome the read aligns to\n\n\n4\nPOS\nInteger\n1-based 5’-most alignment position on reference forward strand\n\n\n5\nMAPQ\nInteger\nPhred-scaled mapping quality score\n\n\n6\nCIGAR\nString\nString indicating matches, indels and clipping in alignment\n\n\n7\nRNEXT\nString\nScaffold/chromosome that the read’s mate aligns to\n\n\n8\nPNEXT\nInteger\nAlignment position of the read’s mate\n\n\n9\nTLEN\nInteger\nLength of DNA template whose ends were read (in paired-end sequencing)\n\n\n10\nSEQ\nString\nThe sequence of the read, represented in 5’ to 3’ on the reference forward strand\n\n\n11\nQUAL\nString\nBase quality scores, ordered from 5’ to 3’ on the reference forward strand\n\n\n\n\n\n\n\n\n\n\n10.4.2 Play with simple alignments\nTeam up with someone who has a Mac. They can clone the RStudio project repository on GitHub at https://github.com/eriqande/alignment-play (by opening a new RStudio project with the “From Version Control” –&gt; GitHub option, for example).\nThis has a notebook that will let us do simple alignments and familiarize ourselves with the output in SAM format. This is only available on Mac because bwa and samtools are not available on conda compiled for windows (and because I figured there wouldn’t be many folks running Linux on their laptops).\nRead through the R notebook and run the code. See how the SAM flags and or the CIGAR strings change when you make read1 and read2 different sequences, as shown in the notebook.\n\n\n10.4.3 SAM Flags\nThe FLAG column expresses the status of the alignment associated with a given read (and its mate in paired-end sequencing) in terms of a combination of 12 yes-or-no statements. The combination of all of these “yesses” and “nos” for a given aligned read is called its SAM flag. The yes-or-no status of any single one of the twelve statements is called a “bit” because it can be thought of as a single binary digit whose value can be 0 (No/False) or 1 (Yes/True). Sometimes a picture can be helpful: we can represent each statement as a circle which is shaded if it is true and open if it is false. Thus, if all 12 statements are false you would see \\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\). However, if statements 1, 2, 5, and 7 are true then you would see \\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\bullet}{\\circ}{\\bullet}~{\\circ}{\\circ}{\\bullet}{\\bullet}\\). In computer parlance we would say that bits 1, 3, 5, and 7 are “set” if they indicate Yes/True. As these are bits in a binary number, each bit is associated with a power of 2 as shown in Table 10.2, which also lists the meaning of each bit.\n\n\n\n\nTable 10.2: SAM flag bits in a nutshell. The description of these in the SAM specification is more general, but if we restrict ourselves to paired-end Illumina data, each bit can be interpreted by the meanings shown here. The “bit-grams” show a visual representation of each bit with open circles meaning 0 or False and filled circles denoting 1 or True. The bit grams are broken into three groups of four, which show the values that correspond to different place-columns in the hexadecimal representation of the bit masks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbit-#\nbit-gram\n\\(2^x\\)\ndec\nhex\nMeaning\n\n\n\n\n1\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\bullet}\\)\n\\(2^0\\)\n1\n0x1\nthe read is paired (i.e. comes from paired-end sequencing.)\n\n\n2\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\bullet}{\\circ}\\)\n\\(2^1\\)\n2\n0x2\nthe read is mapped in a proper pair\n\n\n3\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\bullet}{\\circ}{\\circ}\\)\n\\(2^2\\)\n4\n0x4\nthe read is not mapped/aligned\n\n\n4\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\bullet}{\\circ}{\\circ}{\\circ}\\)\n\\(2^3\\)\n8\n0x8\nthe read’s mate is not mapped/aligned\n\n\n5\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\bullet}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^4\\)\n16\n0x10\nthe read maps to the reverse strand\n\n\n6\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\bullet}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^5\\)\n32\n0x20\nthe read’s mate maps to the reverse strand\n\n\n7\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\bullet}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^6\\)\n64\n0x40\nthe read is read 1\n\n\n8\n\\({\\circ}{\\circ}{\\circ}{\\circ}~{\\bullet}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^7\\)\n128\n0x80\nthe read is read 2\n\n\n9\n\\({\\circ}{\\circ}{\\circ}{\\bullet}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^8\\)\n256\n0x100\nthe alignment is not primary (don’t use it!)\n\n\n10\n\\({\\circ}{\\circ}{\\bullet}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^9\\)\n512\n0x200\nthe read did not pass platform quality checks\n\n\n11\n\\({\\circ}{\\bullet}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^{10}\\)\n1024\n0x400\nthe read is a PCR (or optical) duplicate\n\n\n12\n\\({\\bullet}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}~{\\circ}{\\circ}{\\circ}{\\circ}\\)\n\\(2^{11}\\)\n2048\n0x800\nthe alignment is part of a chimeric alignment\n\n\n\n\n\n\n\n\nIf we think of the 12 bits as coming in three groups of four we can easily represent them as hexadecimal numbers. Hexadecimal numbers are numbers in base-16. They are expressed with a leading “0x” but otherwise behave like decimal numbers, except that instead of a 1’s place, 10’s place, and 100’s place, and so on, we have a 1’s place, a 16’s place, and 256’s place, and so forth. In the first group of four bits (reading from right to left) the bits correspond to 0x1, 0x2, 0x4, and 0x8, in hexadecimal. The next set of four bits correspond to 0x10, 0x20, 0x40, and 0x80, and the last set of four bits correspond to 0x100, 0x200, 0x400, 0x800. It can be worthwhile becoming comfortable with these hexadecimal names of each bit.\nIn the SAM format, the FLAG field records the decimal (integer) equivalent of the binary number that represents the yes-or-no answers to the 12 different statements. It is relatively easy to do arithmetic with the hexadecimal flags to find the decimal equivalent: add up the numbers in each of the three hexadecimal value places (the 1’s, 16’s, and 256’s places) and multiply the result by 16 raised to the number of zeros right of the “x” in the hexadecimal number. For example if the bits set on an alignment are 0x1 & 0x2 & 0x10 & 0x40, then they sum column-wise to 0x3, and 0x50, so the value listed in the FLAG field of a SAM file would be \\(3 + 5 \\cdot 16 = 83\\).\nWhile it is probably possible to get good at computing these 12-bit combinations from hexadecimal in your head, it is also quite convenient to use the Broad Institute’s wonderful SAM flag calculator.\nWe will leave our discussion of the various SAM flag values by noting that the large SAM-flag bits (0x100, 0x200, 0x400, and 0x800) all signify something “not good” about the alignment. The same goes for 0x4 and 0x8. On the other hand, when you are dealing with paired-end data, one of the reads has to be read 1 and the other read 2, and that is known from their read names and the FASTQ file that they are in. So, we expect that 0x40 and 0x80 should always be set, trivially. With paired-end data, we are always comforted to see bits 0x1 and 0x2 set, as departures from that condition indicate that the pairing of the read alignments does not make sense given the sequence in the reference genome. As we saw in our discussion of how a template can properly map to a reference, you should be able to convince yourself that, in a properly mapped alignment, exactly one of the two bits 0x10 and 0x20 should be set for one read in the pair, and the other should be set for the other. Therefore, in good, happy, properly paired reads, from a typical whole genome sequencing library preparation, we should find either:\nread 1 : 0x1 & 0x2 & 0x10 & 0x40 = 83\nread 2 : 0x1 & 0x2 & 0x20 & 0x80 = 163\nor\nread 1 : 0x1 & 0x2 & 0x20 & 0x40 = 99\nread 2 : 0x1 & 0x2 & 0x10 & 0x80 = 147\nSo, now that we know all about SAM flags and the values that they take, what should we do with them? First, investigating the distribution of SAM flags is an important way of assessing the nature and reliability of the alignments you have made (this is what samtools flagstat is for, as discussed in a later chapter). Second, you might wonder if you should do some sort of filtering of your alignments before you do variant calling. With most modern variant callers, the answer to that is, “No.” Modern variant callers take account of the information in the SAM flags to weight information from different alignments, so, leaving bad alignments in your SAM file should not have a large effect on the final results. Furthermore, filtering out your data might make it hard to follow up on interesting patterns in your data, for example, the occurrence of improperly aligning reads can be used to infer the presence of inversions. If all those improperly paired reads had been discarded, they could not be used in such an endeavor.\nNonetheless, you will want to mark (rather than remove) some aberrations, like PCR duplicates. We discuss that in a later section.\n\n\n10.4.4 The CIGAR string\nCIGAR is an acronym for Compressed Idiosyncratic Gapped Alignment Report. It provides a space-economical way of describing the manner in which a single read aligns to a reference genome. It is particularly important for recording the presence of insertions or deletions within the read, relative to the reference genome. This is done by counting up, along the alignment, the number of base pairs that: match (M) the reference; that are inserted (I) into the read and absent from the reference; and that are deleted (D) from the read, but present in the reference. To arrive at the syntax of the CIGAR string you catenate a series of Number-Letter pairs that describe the sequence of matches, insertions and deletions that describe an alignment.\nSome examples are in order. We return to our 80 base-pair reference from above and consider the alignment to it of a 10 bp read that looks like 5'  ACCTGCAGGA  3':\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACCTGCAGGA  3'\nSuch an alignment has no insertions or deletions, or other weird things, going on. So its CIGAR string would be 10M, signifying 10 matching base pairs. A very important thing to note about this is that the M refers to bases that match in position in the alignment even though they might not match the specific nucleotide types. For example, even if bases 3 and 5 in the read don’t match the exact base nucleotides in the alignment, like this:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  ACTTACAGGA  3'\nits CIGAR string will typically still be 10M. (The SAM format allows for an X to denote mismatches in the base nucleotides between a reference and a read, but I have never seen it used in practice.)\nNow, on the other hand, if our read carried a deletion of bases 3 and 4. It would look like 5'  ACGCAGGA  3' and we might represent it in an alignment like:\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  AC--GCAGGA  3'\nwhere the -’s have replaced the two deleted bases. The CIGAR string for this alignment would be 2M2D6M.\nContinuing to add onto this example, suppose that not only have bases 3 and 4 been deleted, but also a four-base insertion of ACGT occurs in the read between positions 8 and 9 (of the original read). That would appear like:\n5'  ACATAGACAGGGACCACCTGCAG----GACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'\n               5'  AC--GCAGACGTGA  3'\nwhere -’s have been added to the reference at the position of the insertion in the read. The CIGAR string for this arrangement would be 2M2D4M4I2M which can be hard to parse, visually, if your eyes are getting as old as mine, but it translates to:\n2 bp Match  \n2 bp Deletion  \n4 bp Match  \n4 bp Insert   \n2 bp Match   \nIn addition to M, D and I (and X) there are also S and H, which are typically seen with longer sequences. They refer to soft- and hard-clipping, respectively, which are situations in which a terminal piece of the read, from either near the 3’ or 5’ end, does not align to the reference, but the central part, or the other end of the read does. Hard clipping removes the clipped sequence from the read as represented in the SEQ column, while soft clipping does not remove the clipped sequence from the SEQ column representation.\nOne important thing to understand about CIGAR strings is that they always represent the alignment as it appears in the 5’ to 3’ direction. As a consequence, it is the same whether you are reading it off the read in the 5’ to 3’ direction or if you are reading it off from how the reverse complement of the read would align to the opposite strand of the reference. Another picture is in order: if we saw a situation like the following, with a deletion in Read 1 (which aligns to the reverse strand), the CIGAR string would be, from 5’ to 3’ on Read 1, 6M2D2M, which is just what we would have if we were to align the reverse complement of Read 1, called Comp R1 below to the forward strand of the reference.\n      Read 2:  5'  ACCTGCAGGA  3'            Comp R1:  5'  AA--CAGTGA  3'\n5'  ACATAGACAGGGACCACCTGCAGGACACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'  \nforward-strand\n    ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCTGTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'\n                                              Read 1:  3'  TT--GTCACT  5'\nFor the most part, it is important to have an understanding of CIGAR strings, though you will rarely end up parsing and using them yourself. That job is best left for the specialized tools that process SAM files and their compressed equivalents, BAM files. Nonetheless, it is worth pointing out that if you want to identify the nucleotide values (i.e. alleles) at different variant positions upon single reads, it is necessary to contend with CIGAR strings to do so. This is one of the things that gets taken care of (with some Perl code) in the R package microhaplot for extracting microhaplotypes from short read data (and then visualizing them).\n\n\n10.4.5 The SEQ and QUAL columns\nThese columns hold the actual reads and quality scores that came off the sequencing machine and were in the FASTQ files. (Note, if you thought that after aligning your reads, the SAM or BAM files would end up taking up less space then the ridiculously large, gzipped FASTQ files you just downloaded from the sequencing center, guess again! SAM files actually have all the information present in a FASTQ, along with extra information about the alignments.)\nThe only thing that is tricky about these columns is that, if the read aligns to the reverse strand of the reference genome, the entry in the SEQ column is the reverse complement of the read that actually appeared in the FASTQ file. Of course, when you read off the letters of DNA from a reverse complement, going left to right the way you read a book, the order in which you encounter the complement of each base from the original sequence is reversed from the way you would read the bases in the original sequence. Accordingly, the order of the base quality scores that appear in the QUAL column will be in reverse order is the mapping was to the reverse strand.\n\n\n10.4.6 SAM File Headers\nThis next section uses an example file that is in the repository at example-files/s001---1.sam. To get it, you might need to sync your fork of the repo and then pull down changes into your main branch.\nTo this point, we have talked almost exclusively about the rows in a SAM file which record the alignments of different reads. However, if you look at a SAM file (with cat or less, or in a text editor) the first thing you will see is the SAM file header: a series of lines that all start with the @ symbol followed by two capital letters. Like @SQ.\nTry this:\nless -S example-files/s001---1.sam\nKeep hitting the space bar until you get down to the alignments, then use the up-arrow key to go back up through the file and inspect all the header lines.\nThese file header lines can appear daunting at first, and, when merging or dividing SAM and BAM files can prove to the bane of your existence, so understanding both their purpose and structure is paramount to avoiding some pain down the road.\nIn a nutshell, the lines in the header provide information to programs (and people) that will be processing the data in a SAM (or BAM, see below) file. Some of the header lines give information about the reference genome that the reads were aligned to, others provide an overview of the various samples (and other information) included in the file, while still others give information about the program and data that produced the SAM file.\nIf you have ever looked at the SAM header for an alignment to a reference genome that is in thousands of pieces, you were likely overwhelmed by thousands of lines that started with @SQ. Each of these lines gives information about the names and lengths (and optionally, some other information) of sequences that were used as the reference for the alignment.\nEach header line begins with an @ followed by two capital letters, for example, @RG or @SQ. The two-letter code indicates what kind of header line it is. There are only five kinds of header lines:\n\n@HD: the “main” header line, which, if present, will be the first line of the file. It’s purpose is to reveal the version of the SAM format in use, and also information about how the alignments in the file are sorted. (You won’t find this line in results/sam/s001---1sam. It’s not there! But we will see it later after converting it to a BAM file and sorting it!)\n@SQ: typically the most abundant SAM header lines, each of these gives information about a sequence in the reference genome aligned to.\n@RG: these indicate information about read groups, which are collections of reads that, for the purposes of downstream analysis can all be assumed to be from the same individual and to have been treated the same way during the processes of library preparation and sequencing. We will discuss read groups more fully in the section on alignment.\n@PG: a line that tells about the program that was used to produced the SAM file.\n@CO: a line in which a comment can be placed.\n\nLook though the header for example-files/s001---1.sam and find the @RG and @PG lines.\nThose first three characters (i.e. @ followed to two uppercase letters) signify that a line is a header line, but, within each header line, how is information conveyed? In all cases (except for the comment lines using @CO), information within a header line is provided in TAB-delimited, colon-separated key-value pairs. This means that the type or meaning of each piece of information is tagged with a key, like ID, and its value follows that key after a colon. For example, the line\n@PG     ID:bwa  PN:bwa  VN:0.7.17-r1188 CL:bwa mem -R @RG\\tID:s001_T199967_Lib-1_HY75HDSX2_1_AAGACCGT+CAATCGAC\\tSM:T199967\\tPL:ILLUMINA\\tL\nB:Lib-1\\tPU:HY75HDSX2.1.AAGACCGT+CAATCGAC resources/genome.fasta results/trimmed/s001---1_R1.fq.gz results/trimmed/s001---1_R2.fq.gz\ntells us that the ID of the program that produced the SAM file was bwa, and its version number (VN) was 0.7.17-r1188. And it also shows the complete command line (CL) that was used to produce it.\nThe keys in the key-value pairs are always two uppercase letters. And different keys are allowed (and in some case required) in the context of each of the five different kinds of SAM header lines.\nThe most important keys for the different kinds of header lines are as follows:\n\nFor @HD\n\nVN: the version of the SAM specification in use. This is required.\nSO: the sort order of the file. The default value is unknown. More commonly, when your SAM/BAM file is prepared for variant calling, it will have been sorted in the order of the reference genome, which is denoted as SO:coordinate. For other purposes, it is important to sort the file in the order of read names, or SO:queryname. It is important to note that setting these values in a SAM file does not sort it. Rather, when you have asked a program to sort a SAM/BAM file, that program will make a note in the header about how it is sorted.\n\nFor @SQ\n\nSN is the key for the sequence name and is required.\n\nLN is the key for the length (in nucleotide bases) of the sequence, and is also required.\n\nFor @RG\n\nID is the only required key for @RG lines. HUGE NOTE: if multiple @RG lines occur in the file, each of their ID values must be different.\nSM is the key for the name of the sample from which the reads came from. This is the name used when recording genotypes of individuals when doing variant calling.\nLB denotes the particularl library in which the sample was prepared for sequencing.\nPU denotes the “Platform Unit,” of sequencing. Typically interpreted to mean the flow-cell and the lane upon which the sample was sequenced. We will talk much more about the contents of read-group header lines, and how to fill them.\nPL denotes the sequencing technology (PL is short for “platform”) used. If the reads are from an Illumina sequencer, the value would be ILLUMINA.\n\nFor @PG\n\nID is required and provides information about the program that produced the SAM output.\nVN as we saw before, this key lets you record the version of the program used to produce the SAM output.\n\n\nA complete accounting of the different possible SAM header lines and their contents is given in the SAM specification. It is given in a terse table that is quite informative and is not terribly tough sledding. It is recommended that you read the section on header lines.\n\n\n10.4.7 The BAM format\nAs you might have inferred from the foregoing, SAM files can end up being enormous text files. For our example file, try this: du -h example-files/s001---1.sam to see how big it is:\nThe two big problems with having such large files are:\n\nThey could take up a lot of hard drive space.\nIt would take you (or some program that was processing a SAM file) a lot of time to “scroll” through the file to find any particular alignment you (it) might be interested in.\n\nThe originators of the SAM format dealt with this by also specifying and creating a compressed binary (meaning “not composed of text”) format to store all the information in a SAM file. This is called a BAM (Binary Alignment Map) file. In a BAM file, each column of information is stored in its native data type (i.e., the way a computer would represent it internally if it were working on it) and then the file holding all of these “rows” is compressed into a series of small blocks in such a way that the file can be indexed, allowing rapid access (without “scrolling” through the whole file) to the alignments within a desired genomic range. As we will see in a later chapter, in order to index such a file for rapid access to alignments in a particular genomic range, the alignments must be sorted in the order of genomic coordinates in the reference sequence.\nSince BAM files are smaller than SAM files, and access into them is faster than for SAM files, you will almost always convert your SAM files to BAM files to prepare for futher bioinformatic processing. The main tool used for this purpose is the program samtools (written by the creators of the SAM and BAM formats) for that purpose. We will encounter samtools in a later chapter.\nFinally, humans cannot directly read BAM files, or even decompress them with standard Unix tools. If you want to view a BAM file, you can use samtools view to read it in SAM format.\n\n\n10.4.8 Quick self study\n\nSuppose a read is Read 2 from the FASTQ, it aligns to the reverse strand, and its mate does too. The alignment is a primary alignment, but it has been flagged as a PCR duplicate. Write down, in hexadecimal form, all the bits that will be set for this read’s alignment, then combine them to compute the SAM FLAG for it.\nGiven the alignments shown below, what do you think the CIGAR strings for Read 1 and Read 2 might look like?\n\n      Read 2:  5'  ACCT--AGGAGGACACACAC   3' \n5'  ACATAGACAGGGACCACCTGCAGGA---CACACACGCAGGTTTACTAAGGGTTTACTCAACACAGTGAACAGCATATACCAGA  3'  \nforward-strand\n    |||||||||||||||||||||||||---|||||||||||||||||||||||||||||||||||||||||||||||||||||||\nreverse-strand\n3'  TGTATCTGTCCCTGGTGGACGTCCT---GTGTGTGCGTCCAAATGATTCCCAAATGAGTTGTGTCACTTGTCGTATATGGTCT  5'\n                                      Read 1:  3' AAAAAAAAAAAATTGTGTC-CT  5'",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#variants",
    "href": "nmfs-bioinf/bioinf-formats.html#variants",
    "title": "10  Bioinformatic file formats",
    "section": "10.5 Variants",
    "text": "10.5 Variants\nMost conservation geneticists who started working in the field before the genomic revolution will recognize variant data as being most similar to what they have traditionally thought of as “genotype data” or, simply, “the data.” As the name implies, variant data consist of data only at those places in the genome where there is variation within a population or a species. This variation is the informative portion of the data for many analyses. In other words, you needn’t always carry with you information about every single base in the genome, because at most positions in the genome, every individual you are studying will carry the same base, and such fixed sites are not informative for many genetic analyses.\nOn the one hand, you can think of sites with no variation as being a little like monomorphic microsatellite loci (if that analogy is helpful for our older, more experienced, readers, who have been doing conservation genetics for a while…) Typically, such monomorphic loci were dropped for many analyses. On the other hand, when you are sampling genetic variation from an entire genome, it is also very useful to be able to keep track of how many sites in the genome are variable versus not variable. Accordingly,with genomic variant data it is important to keep track of how many variants exists within a given portion of the genome, so, we must keep track of where, within our reference genome, the variant sites are.\nFinally, decades ago, many of the different types of genetic variant data were a reflection of the methodology used to assay them. So, for example, restriction-fragment length polymorphisms and amplified fragment length polymorphisms were somewhat crude summaries that were responsive to differences in the underlying sequences of bases, but were only telling us about sequence at a very small (like 6 bp) section of the genome). Meanwhile minisatellites and microsatellites were expressed in terms of the number of repeated short motifs in a location of the genome, without telling us what the sequence there actually was. Today, however, with the resolution provided by genome sequencing, many such polymorphisms that were defined by gross or summarized features of the sequence can now be categorized and described in terms of the actual genome sequence carried by the two copies of a chromosome in a diploid individual. The variants in this context are define according to how those sequences differ from the corresponding sequence in the reference genome. The latter point bears repeating: currently, variant data are defined in terms of differences between the sequence carried by an individual and the genome sequence as it is listed in the reference genome.\n(We note that there is exciting work on developing a more “de-centralized” notion of variation data: one in which variants are not defined as departures from a “canonical” reference genome, but rather one in which all the known variation is recorded in a single “genomic variation graph.” This has exciting possibilities for reducing biases that might arise from the use of a single reference genome; however its use in conservation genetics is still likely a few (or more) years distant.)\nThe resolution of genetic data now allows us to define many different kinds of polymorphisms in terms of the sequence data itself. At the same time, just as the instruments used to gather sequence data can attach a variety of additional information (like base-quality and mapping scores) to sequence data, the methods used to infer the variants carried by an individual also may provide an abundance of additional information that should remain attached to the genotypes of individuals. Further, as is a constant theme in genomics, the scale of variant data one can obtain is huge and we will often want to access information about only certain defined subsets of the variants carried by any particular individual. And finally, as more data gets collected, we will want to be able to combine data from different sources and studies and use the combined data in a unified fashion.\nFor all the above reasons, the genomics community has recognized the need for a flexible and extensible format for storing variant data. Developing such a format and helping it to evolve as new types of genomic data allow new types of variants to be recorded, has been a major focus, driven largely by research in human genetics. For the most part, this community has converged on a format known as Variant Call Format or VCF. This format makes it possible to do all of the following:\n\nRecord variants such as SNPs, and short insertions and deletions, in a single framework, in terms of the sequences carried by individuals. (There is also now support for reording large structural variations and copy number variations; however, those have not, as yet, become central in conservation genetics).\nAttach to each particular locus or variant an arbitrary quantity of extra information, which might include confidence/quality scores or annotation information (which genes is the variant in), etc.\nAttach to each particular inferred genotype of an individual a variety of information such as confidence scores, read depth, haplotypic phase, etc.\nCompress all of the data in an efficient fashion and still allow the data to be processed (by some utilities) without uncompressing the data.\nIndex the data so that fast access to information about specific genomic regions can be extracted quickly.\n\nThe flexibility and extensibility of the VCF format comes with a small cost to conservation genomicists in that the format might look like nothing you have ever seen before! On top of that, if you want to work with VCF files, you really ought to learn to use specialized tools that have been developed for handling VCF files. This turns out to be essential in conservation genomics for the simple reason that even if you don’t end up doing most of your own bioinformatics (i.e. you leave the alignment and variant calling to a genotyping/bioinformatics service (or to a graduate student or postdoc)), the chances are good that whomever does that for you will (or, at least, should) send results to you in a VCF file. So, while you might be able to do work in conservation genomics without ever having to crack open a BAM file, or parse the lines of a FASTQ file, you will almost invariably have to bang your head against a VCF file at some point.\nSo, what I am saying, is don’t be a complete noob like I was in 2011 when I received my first data set in VCF format. I didn’t have any idea what it was, and after wracking my brain over it finally wrote back to the genotyping service that had sent it to me:\nI have looked over the file and everything makes sense save for a few \nthings.   I was hoping you could answer just a few (should be easy) \nquestions.  You went over this when we videoconferenced, but just a \nfew things are hazy. Sorry.\n\nQUAL column:  How is this scaled? There are 13990 distinct values \nranging from 6.2 to 250.64.  Is it just sort of relative?  What is \n\"good\" quality.\n\nThe INFO column:  what does this mean: \"NS=22:AN=1:DP=541\"\n\nIn other words what are NS, AN, and DP?\n\nThe FORMAT column.  Looking at the file it appears that \n\"GT:DP:GQ:EC:SG\" means that the first field is a genotype (0,1, or \n0/1, for homozygous reference, homozygous non-reference, and \nheterozygous).  And SG must be the IUPAC code for the individual's \ngenotype, but what are DP, GQ, and EC?  If I had to hazard a guess I \nwould say:\nDP = # of reads with reference base\nGQ = # of reads total whether or not they overlapped the position of \nthe base in question\nEC = # of reads with alternate base\nHah! I just put that out there to remind myself that VCF files will seem super mysterious to everyone when they first encounter them. My goal in the next section is to help you be better informed than I was when I saw my first VCF file. So, with that in mind, let’s jump into learning about the format…\n\n10.5.1 VCF Format – The Body\nVCF files, just like SAM files, have a header section and a main “body” section. We will start by describing the body section, in which each row holds information about a variant and the genotypes carried by individuals in a sample. We will discuss the header section after that.\nTo download a VCF file that you might view in a text editor you can use this link:\n\nhttps://www.dropbox.com/s/dzwixabvy3wwfdr/chinook-32-3Mb.vcf.gz?dl=1\n\nIf you want to, you can decompress that file and open it with an able text editor (like TextWrangler or BBedit on a Mac or Notepad++ on Windows). Don’t open it with MS Word! Also, on you laptop, don’t try to just double click it! The .vcf extension is often interpreted as a “Virtual Contact File”, meaning that your computer will try to open it—and possibly try to import it—using whatever program you handle your contacts’ phone numbers and addresses with.\nTo get download the “index” for that file, use this:\n\nhttps://www.dropbox.com/s/d303fi7en71p8ug/chinook-32-3Mb.vcf.gz.csi?dl=1\n\nIf you want to download these onto a remote Unix server you can use:\nwget https://www.dropbox.com/s/dzwixabvy3wwfdr/chinook-32-3Mb.vcf.gz?dl=1\nwget https://www.dropbox.com/s/d303fi7en71p8ug/chinook-32-3Mb.vcf.gz.csi?dl=1\n\n# after which you might need to rename them, if wget retained the `?dl=1`\n# in the resulting file names:\nmv chinook-32-3Mb.vcf.gz\\?dl\\=1 chinook-32-3Mb.vcf.gz\nmv chinook-32-3Mb.vcf.gz.csi\\?dl\\=1 chinook-32-3Mb.vcf.gz.csi\nAt their core, VCF files are TAB-delimited text files. Below is a fragment of a VCF file body (and the final header line) holding information about 9 variants (in 9 rows below the final header line) and the genotypes at those variants carried in four individuals (the four right-most columns).\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  DPCh_plate1_A05_S5  DPCh_plate1_A06_S6  DPCh_plate1_A11_S11 DPCh_plate1_A12_S12\nNC_037124.1 4001417 .   T   G   1528.4  .   AC=14;AF=0.245;AN=44;BaseQRankSum=2.296;ClippingRankSum=-1.168;DP=206;FS=0;MLEAC=51;MLEAF=0.236;MQ=60;MQRankSum=1.501;QD=23.88;ReadPosRankSum=1.685;SOR=0.608   GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  ./.:.:.:.:.\nNC_037124.1 4001912 .   A   G   4886.98 .   AC=29;AF=0.652;AN=48;BaseQRankSum=4.315;ClippingRankSum=-0.984;DP=220;FS=0.753;MLEAC=139;MLEAF=0.662;MQ=59.92;MQRankSum=0.311;QD=29.62;ReadPosRankSum=-1.071;SOR=0.737  GT:AD:DP:GQ:PL  0/0:1,0:1:3:0,3,46  ./.:.:.:.:. 0/1:3,1:4:33:33,0,120   1/1:0,2:2:6:87,6,0\nNC_037124.1 4004574 .   AAAGG   A   957.91  .   AC=12;AF=0.176;AN=48;BaseQRankSum=1.052;ClippingRankSum=0.887;DP=201;FS=0;MLEAC=28;MLEAF=0.149;MQ=60;MQRankSum=0.73;QD=26.61;ReadPosRankSum=-1.135;SOR=0.591    GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:2,0:2:6:0,6,119 0/0:2,0:2:6:0,6,109\nNC_037124.1 4006558 .   T   TG  3442.71 .   AC=24;AF=0.56;AN=28;BaseQRankSum=-1.068;ClippingRankSum=-1.509;DP=191;FS=5.394;MLEAC=102;MLEAF=0.554;MQ=60;MQRankSum=-0.621;QD=29.42;ReadPosRankSum=0.552;SOR=0.464 GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,2:2:6:75,6,0  1/1:0,2:2:6:82,6,0  ./.:.:.:.:.\nNC_037124.1 4006887 .   G   T   109.29  .   AC=2;AF=0.048;AN=40;BaseQRankSum=-0.411;ClippingRankSum=-0.145;DP=175;FS=0;MLEAC=6;MLEAF=0.032;MQ=60;MQRankSum=-0.718;QD=15.61;ReadPosRankSum=-0.502;SOR=1.323  GT:AD:DP:GQ:PL  0/0:2,0:2:6:0,6,90  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  0/0:1,0:1:3:0,3,42\nNC_037124.1 4007343 .   T   TGGAAGGAAGGAAACG    7696.07 .   AC=42;AF=0.985;AN=42;BaseQRankSum=2.278;ClippingRankSum=1.604;DP=213;FS=0;MLEAC=192;MLEAF=0.99;MQ=60;MQRankSum=0.139;QD=31.22;ReadPosRankSum=0.973;SOR=0.7  GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,1:1:3:34,3,0  ./.:.:.:.:. 1/1:0,1:1:9:135,9,0\nNC_037124.1 4007526 .   C   T   1040.07 .   AC=13;AF=0.17;AN=54;BaseQRankSum=1.507;ClippingRankSum=0.333;DP=227;FS=2.374;MLEAC=35;MLEAF=0.165;MQ=60;MQRankSum=2.562;QD=17.05;ReadPosRankSum=1.567;SOR=0.679 GT:AD:DP:GQ:PL  0/1:1,1:2:19:19,0,39    0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,49  0/0:2,0:2:6:0,6,90\nNC_037124.1 4007583 .   C   CAA 596.58  .   AC=3;AF=0.084;AN=54;BaseQRankSum=-0.197;ClippingRankSum=-0.445;DP=224;FS=0;MLEAC=19;MLEAF=0.089;MQ=60;MQRankSum=0.096;QD=20.57;ReadPosRankSum=-1.754;SOR=1.127  GT:AD:DP:GQ:PL  ./.:.:.:.:. 0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,90  0/0:1,0:1:3:0,3,45\nNC_037124.1 4007584 .   GCC G,CCC   3246.17 .   AC=3,25;AF=0.083,0.324;AN=54;BaseQRankSum=1.157;ClippingRankSum=-0.459;DP=227;FS=2.342;MLEAC=16,56;MLEAF=0.074,0.259;MQ=60;MQRankSum=0.412;QD=28.98;ReadPosRankSum=-0.656;SOR=0.527 GT:AD:DP:GQ:PL  ./.:.:.:.:. 2/2:0,0,1:1:3:37,39,45,3,3,0    2/2:0,0,2:2:6:82,84,90,6,6,0    2/2:0,0,1:1:3:45,45,45,3,3,0\nWow! That is really hard to look at. The column corresponding to the INFO field is quite long and obtrusive. Below we turn that column into a . in every row:\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  DPCh_plate1_A05_S5  DPCh_plate1_A06_S6  DPCh_plate1_A11_S11 DPCh_plate1_A12_S12\nNC_037124.1 4001417 .   T   G   1528.4  .   .   GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  ./.:.:.:.:.\nNC_037124.1 4001912 .   A   G   4886.98 .   .   GT:AD:DP:GQ:PL  0/0:1,0:1:3:0,3,46  ./.:.:.:.:. 0/1:3,1:4:33:33,0,120   1/1:0,2:2:6:87,6,0\nNC_037124.1 4004574 .   AAAGG   A   957.91  .   .   GT:AD:DP:GQ:PL  1/1:0,1:1:3:45,3,0  ./.:.:.:.:. 0/0:2,0:2:6:0,6,119 0/0:2,0:2:6:0,6,109\nNC_037124.1 4006558 .   T   TG  3442.71 .   .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,2:2:6:75,6,0  1/1:0,2:2:6:82,6,0  ./.:.:.:.:.\nNC_037124.1 4006887 .   G   T   109.29  .   .   GT:AD:DP:GQ:PL  0/0:2,0:2:6:0,6,90  ./.:.:.:.:. 0/0:1,0:1:3:0,3,46  0/0:1,0:1:3:0,3,42\nNC_037124.1 4007343 .   T   TGGAAGGAAGGAAACG    7696.07 .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 1/1:0,1:1:3:34,3,0  ./.:.:.:.:. 1/1:0,1:1:9:135,9,0\nNC_037124.1 4007526 .   C   T   1040.07 .   .   GT:AD:DP:GQ:PL  0/1:1,1:2:19:19,0,39    0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,49  0/0:2,0:2:6:0,6,90\nNC_037124.1 4007583 .   C   CAA 596.58  .   .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 0/0:1,0:1:3:0,3,45  0/0:2,0:2:6:0,6,90  0/0:1,0:1:3:0,3,45\nNC_037124.1 4007584 .   GCC G,CCC   3246.17 .   .   GT:AD:DP:GQ:PL  ./.:.:.:.:. 2/2:0,0,1:1:3:37,39,45,3,3,0    2/2:0,0,2:2:6:82,84,90,6,6,0    2/2:0,0,1:1:3:45,45,45,3,3,0\nPhew! That is easier on the eyes at this point. Looking at the above, notice that the first nine columns carry information that pertains to the variant itself (i.e., the SNP or the indel that is being described in this row). These nine columns are followed by four columns, each one pertaining to one of the four samples whose names are given in the top row: DPCh_plate1_A05_S5    DPCh_plate1_A06_S6  DPCh_plate1_A11_S11 DPCh_plate1_A12_S12.\nWe will start by describing the 9 standard variant-specific columns:\n\nCHROM: the name of the reference sequence upon which the variant was found. This comes from the FASTA file that was used as a reference to align the sequencing data to, orinally.\nPOS: The 1-based position within CHROM that the variant is found at. For SNPs, this is straightforward. For insertions and deletions it is also straightforward, but you must remember that the value in the POS columns give the first position in the entry in the REF column. More on that below…\nID: An identifier for a particular variant. These identifiers are used to indicate the name of the variant in different data bases, like the human dbSNP data base. For non-model organisms of conservation concern, there are typically no such data bases, so the values in this column will be “missing” which is always denoted by a single period, ., in a VCF file.\nREF the sequence of the reference—i.e. the “reference allele” at the variant. See below for more description about how this is designated.\nALT the sequence of the alternate allele(s) at the variant. If there is more than one alternate allele at the variant, the sequences for each are separated by a comma. (See more below).\nQUAL: a Phred-scaled assessment of whether the variant truly represents variation in the sample that was sequenced and genotyped, or whether it might be a spurious variant—for example, not a true polymorphism, but rather due to a sequencing error. If an alternate allele is given in ALT, then the QUAL is \\(-10\\log_{10} P(\\mathrm{there~truly~is~NOT~such~an~alternate~allele(s)})\\). A larger number means more confidence that the alternate allele is real. If there is no alternate allele given in ALT (i.e. the ALT field holds a .), then the value in QUAL is \\(-10\\log_{10} P(\\mathrm{there~actually~IS~a~variant~here})\\).\nFILTER: a semicolon-delimited list of filters that this variant tests positive for. This provides a way of creating different possible sets of variants that can be tagged in different ways, but not removed from the data set. Not used extensively in conservation genomics.\nINFO: a semicolon-delimited list of “key=value” pairs that describe properties of the variant. GATK attaches a large number of these. The meaning of different keys is given in the header (see below).\nFORMAT: this is a column that says, “In the remaining columns, information about the genotypes of different individuals will be given in the following fields separated by colons.” It is basically a key to deciphering the following columns of the genotype data of the individuals, in that row. Specifying the genotype column formats for each row, in this fashion, makes it possible to attach different types of information to different variants, as might be appopriate.\n\n\n10.5.1.1 Coding of REF and ALT\nWhen coding for insertions and deletions—or for more complex variants in which some of the insertions might carry SNPs, etc.—the convention is followed that the REF column always shows the sequence in the reference sequence starting at POS. For understanding how insertions and deletions are encoded in this system, it is helpful to view them in columnar format with positions running down the first column, and different alleles in different columns. Thus, a simple SNP, such as the first row in the above VCF example would appear as:\n# meaning of: NC_037124.1 4001417 .   T   G \n\npos         Ref  Alt\n4001417     T      G\nBy contrast, an insertion, relative to the reference genome, creates bases that don’t have a standard coordinate that they align to. Thus, if we take the second-to-last variant in the above example (at position 4007583), and we assume that positions 4007582 and 4007584 in the reference sequence are a G and a T, respectively, then the first five columns of the row indicate that the variation appears as follows:\n# meaning of: NC_037124.1 4007583 .   C   CAA\n\nPos         Ref  Alt\n4007582     G    G\n4007583     C    C\n.           -    A\n.           -    A\n4007584     T    T\nwhere the - represents an insertion relative to the reference genome.\nA deletion relative to the reference genome, like that in the third row of the above example: would appear in columnar format as:\n# meaning of: NC_037124.1 4004574 .   AAAGG   A\n\nPos         Ref  Alt\n4004573     C    C\n4004574     A    A\n4004575     A    -\n4004576     A    -\n4004577     G    -\n4004578     G    -\n4004579     T    T\nwhere we are imagining that the reference holds a C at position 4004573 and a T at position 4004579, and where the -’s indicate deletions relative to the reference.\nFinally, the multiallelic and complex example on the last row, in columnar format, if the reference held an A at position 4007583 and a T at position 4007587, would show the two possible alternate alleles like:\n# meaning of: NC_037124.1 4007584 .   GCC G,CCC\n\nPos         Ref   Alt1  Alt2\n4007583     A     A     A\n4007584     G     G     C\n4007585     C     -     C\n4007586     C     -     C\n4007587     T     T     T\n\n\n10.5.1.2 The Genotype Columns\nEach of the columns after the 9th column correspond to genotype information at a single one of the samples. The FORMAT string for each in the example is: GT:AD:DP:GQ:PL. There could be more fields than these five (and, if so, information about those fields will be stored in the header section of the VCF file); however, these five are pretty standard, and will be described here. For a concrete example, we will focus in the following on the genotype column for the first sample on the third line from the bottom (POS = 4007526):\n0/1:1,1:2:19:19,0,39\nBroken into the five fields we would see:\n\nGT = 0/1 — The GT stands for GenoType. This field gives the “called” genotype of the individual as two alleles (0 and 1, here) separated by either a / or a |. The 0 always refers to the reference allele (REF), and a number greater than 0 refers to an alternate allele (from the ALT column). If there are only two alleles, (a reference allele and one alternate allele) then the alternate allele will always be denoted by a 1. If there are two alternate alleles, the two alternate alleles will be denoted as 1 and 2, respectively.\n\nA / means that the two gene copies are not phased—in other words, it is not known which haplotype they occur on.\nA | as the separator means that the two gene copies in the individual are recorded as being phased. If an indiduals GT field in a series of variants looks like:\n::: {.cell}\n0|1\n1|0\n1|0\n1|1\n0|0\n1|0\n:::\nThen this is recording that on one of the (diploid) individual’s two chromosomes, the alleles would be strung along like: 0-1-1-1-0-1, and the other chromosome would carry the alleles strung along like: 1-0-0-1-0-1.\n\n\nAD = 1,1 — The AD stands for Allele Depth. This field gives the comma-separated depths of reads from the individual that carried the different alleles at this position. In this case there are only two alleles (a REF and an ALT), so that there is one comma separating two values. The 1 before the comma says that we saw one read with the REF allele, and the 1 after the comma says that we saw one read that had the ALT allele. If there is more than one ALT allele, then there will be more commas, each one separating a different allele read depth (i.e., REF,ALT1,ALT2,etc). These values can sometimes be extracted and used in place of the actual genotypes for making inferences (for example, doing PCAs or computing Fst values using single read sampling).\nDP = 2 — The DP denotes DePth. This field gives the total number of reads from this individual mapped on top of this position. It is typically the sum of the allele depths, but not always, owing to some reads indicating an allele at the position that the variant caller might have deemed a sequening error, etc. In general, the greater the read depth, the more confidence you can have in the genotype calls. Though it might be worthwhile to focus on the read depth itself to identify variants in repetitive regions or copy-number variation.\nGQ = 19 — The GQ stands for Genotype Quality. This field gives a Phred scaled measure of confidence in the genotype call. This value is given conditional on there actually being a variant (in the population) at this position. Thus, it says, “If we set aside the question of whether or not the variant we see here is real, its genotype quality is given by \\(-10\\log_{10} P(\\mathrm{the~genotype~call~is~wrong})\\).” So, colloquially, the probability that the genotype recorded for the individual is incorrect is \\(10^{-GQ/10}\\). So, if GQ = 19, we have (ostensibly) the probability that the genotype call is incorrect equalling \\(10^{-19/10} = 0.0126\\). These probabilities must be taken with a grain of salt. Often, the raw genotype calls from low-ish coverage sequencing data can be somewhat more error prone.\n\nPL = 19,0,39 — PL stands for\n\n\n\n\n10.5.2 VCF Format – The Header\nLet’s just get in there and take a look at it.\n\n\n10.5.3 Boneyard\nVCF. I’ve mostly used vcftools until now, but I’ve gotta admit that the interface is awful with all the –recode BS. Also, it is viciously slow. So, let’s just skip it all together and learn how to use bcftools. One nice thing about bcftools is that it works a whole lot like samtools, syntactically.\nNote that for a lot of the commands you need to have an indexed vcf.gz.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#segments",
    "href": "nmfs-bioinf/bioinf-formats.html#segments",
    "title": "10  Bioinformatic file formats",
    "section": "10.6 Segments",
    "text": "10.6 Segments\nBED",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#conversionextractions-between-different-formats",
    "href": "nmfs-bioinf/bioinf-formats.html#conversionextractions-between-different-formats",
    "title": "10  Bioinformatic file formats",
    "section": "10.7 Conversion/Extractions between different formats",
    "text": "10.7 Conversion/Extractions between different formats\n\nvcflib’s vcf2fasta takes a phased VCF file and a fasta file and spits out sequence.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/bioinf-formats.html#visualization-of-genomic-data",
    "href": "nmfs-bioinf/bioinf-formats.html#visualization-of-genomic-data",
    "title": "10  Bioinformatic file formats",
    "section": "10.8 Visualization of Genomic Data",
    "text": "10.8 Visualization of Genomic Data\nMany humans, by dint of our evolution, are exceptionally visual creatures. Being able to create visual maps for different concepts is also central facilitating understanding of those concepts and extending their utility to other realms. The same is true for bioinformatic data: being able to visualize bioinformatic data can help us understand it better and to see connections between parts of our data sets that we did not, before.\nThe text-based bioinformatic formats we have discussed so far do not, standing by themselves, offer a rich visual experience (have you ever watched a million lines of a SAM file traverse your terminal, and gotten much understanding from that?). However, sequence data, once it has been aligned to a reference genome has an “address” or “genomic coordinates” that, by analogy to street addresses and geographic coordinates suggests that aligned sequence data might be visualized like geographic data in beautiful and/or thought-provoking “maps.”\nThere are a handful of programs that do just that: they make compelling, interactive pictures of bioinformatic data. One of those programs (that I am partial to), called IGV (for Integative Genomics Viewer), was developed in the cross-platform language, Java, by researchers at the Broad Institute. It is available for free download from https://software.broadinstitute.org/software/igv/, and it should run on almost any operating system. It has been well-optimized to portray genomics data at various scales and to render an incredible amount of information in visual displays as well as text-based “tool-tip” reports that may be activated by mousing over different parts of the display.\nThere is extensive documentation that goes with IGV, but it is valuable (and more fun!) to just crack open some genomic data and start playing with it. To do so, there are just a few things that you need to know:\n\nThe placement of all data relies on the reference genome as a sort of “base map.” The reference genome serves the purpose of a latitude-longitude coordinate system that lets you make sense of spatial data on maps. Therefore, it is required for all forays with IGV. You must specify a reference genome by choosing one of the options from the “Genomes” menu. If you are working on a non-model organism of conservation concern it is likely that you will have the reference genome for that critter on your local computer, so you would “Genomes-&gt;Load Genome From File…”, to show IGV where the FASTA file (cannot be compressed) of your genome is on your hard drive. Let’s repeat that: if you are loading the FASTA for a reference genome into IGV, you must use the “Genomes” menu option. If you use “File-&gt;Load From File…” to try to load the reference genome, it won’t let you. Don’t do it! Use “Genomes-&gt;Load Genome From File”\nOnce your reference genome is known to IGV, you can add data from the bioinformatic formats described in this chapter that include positions from a reference genome. These include SAM or BAM files and VCF files (but note FASTQ files). To include data from these formats, choose “File-&gt;Load From File…”. (Note, BAMs are best sorted and indexed). The data from each file that you “Load” in this manner appears in a separate track of data in a horizontally tiled window that is keyed to the reference genome coordinates.\nYou can zoom in and out as appropriate (and in several different ways).\nRight clicking within any track gives a set of options appropriate for the type of track it is. For example, if you are viewing a BAM file, you can choose whether the reads joined together with their mates (“View as pairs”) or not, or whether the alignments should be viewed at “full scale” (“Expanded”), somewhat mashed down (“Collapsed”) or completely squashed down and small (“Squished”).\n\n\n10.8.1 Sample Data\nAbout 0.5 Gb of sample data can be downloaded from https://drive.google.com/file/d/1TMug-PjuL7FYrXRTpNikAgZ-ElrvVneH/view?usp=sharing.\nThis download includes a zip-compressed folder called tiny-genomic-data. Within that folder are two more folders:\n\nchinook-wgs-3-Mb-on-chr-32: FASTQs, BAMs, and VCFs from whole genome sequencing data along a 3 Mb span of Chinook salmon chromosome 32 (which in the reference is named NC_037124.1). The genomic region from which the data comes from is NC_037124.1:4,000,000-7,000,000. The BAM and VCF files can be viewed against the reference genome in IGV.\nmykiss-rad-and-wgs-3-Mb-chr-omy28 includes BAMs from RAD-seq data (sequenced in the study by [@princeEvolutionaryBasisPremature2017]) from multiple steelhead trout individuals merged together into two BAM files. The genomic region included in those BAM files is omy28:11,200,000-12,200,000. Also included is a VCF file from whole genome resequencing data from 125 steelhead and rainbow trout in the 3 Mb region from `omy28:10,150,000-13,150,000.\n\nBoth of the above directories include a genome directory that holds the FASTA that you must point IGV to. Note that in neither case does the FASTA hold the complete genome of the organism. I have merely included three chromosomes in each—the chromosome upon which the BAM and VCF data are located, and the chromosome on either side of that chromosome.\nExplore these data and have fun. Some things to play with (remember to right-click [cntrl-click on a Mac] each track for a menu) :\nStart with the Chinook data:\n\nLoad the FASTA for each data set first\nLoad a BAM file after that\nThen load a VCF file.\nYou will likely have to zoom pretty far into a genomic region with data (see above!) before you see anything interesting.\nTry zooming in as far as you can.\nToggle “View as Pairs” and see the result.\nPlay with “Collapsed/Expanded/Squished”\nExperiment with grouping/sorting/coloring alignments by different properties\nUse coloring to quickly find alignments with\n\nF1R2 of F2R1 orientation\nInsert size &gt; 1000 bp\n\nSort by mapping quality and find some reads with MAPQ &lt; 60\nZoom out and use the VCF to find a region with a high variant density, then zoom back in and view the alignments there? What do you notice about the number of reads aligning to those areas?\n\nFor the steelhead data additionally:\n\nDo you see where the RAD cutsite must have been and how paired-end sequencing works from either side of the cutsite?\nWhat do you notice about the orientation of Read 1 and Read 2 on either side of the cutsite?\nWhy do we see the read depth patterns we see on either side of the cutsite? (i.e., in many cases it goes up as you move away from the cutsite, and then drops off again.)\nDo you appreciate this visual representation of how sparse RAD data is compared to whole genome resequencing data?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bioinformatic file formats</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snake.html",
    "href": "nmfs-bioinf/snake.html",
    "title": "11  Snakemake Narrative and Topics",
    "section": "",
    "text": "11.1 Input Functions",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Snakemake Narrative and Topics</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snake.html#input-functions",
    "href": "nmfs-bioinf/snake.html#input-functions",
    "title": "11  Snakemake Narrative and Topics",
    "section": "",
    "text": "11.1.1 Input functions returning dicts",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Snakemake Narrative and Topics</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snake.html#using-pandas",
    "href": "nmfs-bioinf/snake.html#using-pandas",
    "title": "11  Snakemake Narrative and Topics",
    "section": "11.2 Using Pandas",
    "text": "11.2 Using Pandas",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Snakemake Narrative and Topics</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-relevant-python.html",
    "href": "nmfs-bioinf/snakemake-relevant-python.html",
    "title": "12  Snakemake-relevant Python for R Users",
    "section": "",
    "text": "12.1 Running python, interactively, in RStudio\nThe folks at Posit (the company that started as RStudio) make a great integrated development environment (IDE) for working with R; however they also recognized the need for being a great IDE for other languages, like python and julia. As I understand it, that was part of the reason that they changed their name to Posit—to be less tied to the R language. At any rate, they have tools for running python code, much like you would run R code in RStudio.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Snakemake-relevant Python for R Users</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-relevant-python.html#running-python-interactively-in-rstudio",
    "href": "nmfs-bioinf/snakemake-relevant-python.html#running-python-interactively-in-rstudio",
    "title": "12  Snakemake-relevant Python for R Users",
    "section": "",
    "text": "12.1.1 Get mamba and create a snakemake environment\nYou can do this on your laptop, rather than upon the cluster; however, if you have been working on Alpine, you should skip the rest of this section and get an RStudio session on Alpine following the directions in Chapter 1. Returning to here to get the reticulate package and proceed.\nIf you are doing this on your laptop, fortunately, mamba works on all platforms, and so does Snakemake. But you might need to first install mamba on your laptop. This is pretty straightforward. If you do not already have mamba on your laptop, find the instructions for your platform here and follow them. You should be able to just use the default location for your conda/mamba library.\nOnce you are done with that, you will want to create a snakemake environment on your laptop. Do that by following the directions that we used on our clusters, here, without bothering to be on a compute node, of course, since you are on your laptop!\nWhen both of the above are done, we can move on to working with python in RStudio.\n\n\n12.1.2 Get the reticulate package and run python from your snakemake conda environment\nThe reticulate package allows you to run python in a comfortable, interactive way within an R (and RStudio) session. Importantly, it allows you to run the exact python version that Snakemake uses by letting you load the snakemake conda environment.\nSo, Sync your fork, then open up the RStudio project for your fork of the con-gen-csu repo, make sure you are on the main branch, and then pull, to make sure that you have all the recent materials from the course.\nIf you do not already have the ‘reticulate’, R-package, get it with this command at the R console in Rstudio. If you are Alpine you must do this:\n# first do this\ninstall.packages(\"reticulate\")\n\n# then do this:\ninstall.packages(\"reticulate\", repos = \"https://cran.rstudio.com/\")\nNote that, on Alpine, it is critical to use that second line where you add the repos = \"https://cran.rstudio.com/ part in there. The RStudio Server maintained by CURC in OpenOnDemand is configured by default to draw its packages from a repository full of binary versions, and it may end up givng you versions of packages which are much older than the most recent versions. (At the time of this writing, it installed ‘reticulate’ version 1.28 instead of 1.35, and all the following steps failed…I lost a good several hours because of that!!) We do the initial install.packages(\"reticulate\") to install binary versions of some of the dependencies for the ‘reticulate’ package.\nIf you are not on Alpine, you should be able to simply do:\ninstall.packages(\"reticulate\")\nto get the latest version of reticulate.\nThen, you can load the package with:\nlibrary(reticulate)\nNow, we want to use python from a particular conda environment—our snakemake conda environment, in fact. For that there is a reticulate function called use_condaenv(). We can tell it which conda environment to use most reliably by passing the absolute path of the environment to the function. This path will be the path to your conda library plus envs plus the name of the environment. For example on my laptop that would translate into:\nuse_condaenv(condaenv = \"~/mambaforge-arm64/envs/snakemake-8.5.3\")\nHowever, if you are on Alpine, your conda library would probably be in /project/user@colostate.edu/miniforge.\nWhile on my RStudio Server session on Alpine I would do:\nuse_condaenv(condaenv = \"/projects/eriq@colostate.edu/miniforge3/envs/snakemake-8.5.3\")\nAt any rate, it should be easy to find the path to your snakemake environment using tab-completion for path names in your R console.\nOnce you have successfully given the above command, you can start a python REPL (Read Evaluate Print and Loop) session in your R console with:\nrepl_python()\nAfter you have done that, you should see something that looks like this:\n&gt; repl_python()\nPython 3.12.2 (/Users/eriq/mambaforge-arm64/envs/snakemake-8.5.3/bin/python)\nReticulate 1.35.0 REPL -- A Python interpreter in R.\nEnter 'exit' or 'quit' to exit the REPL and return to R.\n&gt;&gt;&gt; \nThe command prompt has now changed to &gt;&gt;&gt; and anything you type at it will be interpreted as python code.\nSo, you can’t just type R code into it any longer. For example, see what happens if you type this in there:\nx &lt;- 1:4\nBut you can type python code, for example:\nx = [1,2,3,4]\nThen try:\nx\nNote, to get out of the python REPL loop, you can type:\nexit\nwhich will drop you back into R.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Snakemake-relevant Python for R Users</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-relevant-python.html#r-and-python-compare-and-contrast",
    "href": "nmfs-bioinf/snakemake-relevant-python.html#r-and-python-compare-and-contrast",
    "title": "12  Snakemake-relevant Python for R Users",
    "section": "12.2 R and Python: compare and contrast",
    "text": "12.2 R and Python: compare and contrast\nYou can consider this a sort of Berlitz language course for R-speakers on how to communicate in Python. It assumes that y’all know something about R. This is really just scratching the surface of both langauges, but it will supply you with a few important things for working in Snakemake.\n\n12.2.1 Python doesn’t have native vectors\nIn R, we are all familiar with vectors. Each element of a vector is of the same type, i.e., integer, numeric, or character/string, etc. Here we make a numeric vector and a character vector in R:\n\n\n\nR:\n\n# numeric vector\nx &lt;- 1:10\n\n# character vector\ny &lt;- c(\"a\", \"b\", \"c\")\n\n\nOne nice feature of having vectors is that we can do array processing of them—i.e., many operations/functions in R are vectorized: they do the same thing to each element of the vector. For example:\n\n\n\nR:\n\nx^2\n\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\npaste(y, \"!\", sep = \"\")\n\n[1] \"a!\" \"b!\" \"c!\"\n\n\nBy contrast, vectors and vectorized operations, as such, do not exist natively in Python. Some packages provide that type of functionality, but it is not “baked into the guts of the language” the way it is in R.\n\n\n12.2.2 Python’s native array is like an R list (without any names)\nThough python does not have a native vector class, like R’s, it does have something that is very similar to an R list, and, in fact, in python these are called “lists.”\nIn R you make a list, which is a special type of vector that can include elements of different types (i.e., one element could be an integer, another a character/string, another a numeric vector, and another a list!, etc.), like this:\n\n\n\nR:\n\nL_R &lt;- list(1, 2, \"R\", \"word\", 10)\n\n\nand you could find its length with:\n\n\n\nR:\n\nlength(L_R)\n\n\n[1] 5\n\n\nThe equivalent data structure in Python is a list and it is formed with [ ]:\n\n\n\npython:\n\nL_Py = [1, 2, \"R\", \"word\", 10]\n\n\nand you can get its length with:\n\n\n\npython:\n\nlen(L_Py)\n\n\n5\n\n\nYou can access the elements of an R list using the [[ ]] operator and the index of each element. In R, lists are “base-1 indexed,” which means the first element has the index 1, and the second the index 2, and so forth. For example:\n\n\n\nR:\n\n# get the first element of the R list L_R\nL_R[[1]]\n\n\n[1] 1\n\n# get the fourth element of the R list L_R\nL_R[[4]]\n\n[1] \"word\"\n\n\nTo access elements of a python list you use [ ], but (THIS IS EXTREMELY IMPORTANT!) python lists are “base-0 indexed”— the indexes of a python list start from 0, instead of 1. So, if you want the first element of a python list, you subscript it with 0:\n\n\n\npython:\n\nL_Py[0]\n\n\n1\n\n\nWhile, if you wanted the 4-th element of a python list you would subscript it with 3:\n\n\n\npython:\n\nL_Py[3]\n\n\n'word'\n\n\n\n\n12.2.3 Double or single quotes in Python?\nWe also note in the above output that python seems to have a preference for using single quotes for strings in its output; however just as in R, you can use \" and ' somewhat interchangeably in Python. According to StackOverflow:\nIn Python there is no difference between both quotations of a string.\nAs you have noticed both result in the same value stored in a variable.\n\nThere is an important advantage of being able to use these quotation marks\ninterchangeably - you can include in the string the quotation mark that you\nare not using to enquote the string, e.g.:\n\nsingle = \"This string variable contains 'single' quotation marks.\"\ndouble = 'This string variable contains \"double\" quotation marks.'\nSo, that is sort of like R, though R makes internal substitutions and some backslash-escaping of double quotes. Here is the R version:\n\n\n\nR:\n\n# R\nheck_double &lt;- \"What the 'heck'?\"\nheck_single &lt;- 'What the \"heck\"?'\n\nheck_double\n\n\n[1] \"What the 'heck'?\"\n\nheck_single\n\n[1] \"What the \\\"heck\\\"?\"\n\n\nHere is the Python version:\n\n\n\npython:\n\n# python\nheck_double = \"What the 'heck'?\"\nheck_single = 'What the \"heck\"?'\n\nheck_double\n\n\n\"What the 'heck'?\"\n\nheck_single\n\n'What the \"heck\"?'\n\n\nThat said, every Snakefile I have seen out in the wild tends to use double quotes to enquote path names and simple command lines.\n\n\n12.2.4 Replacement of elements of a list\nIn R, you know that you can change one element of a list with a simple assignment to that element:\n\n\n\nR:\n\nL_R[[5]] &lt;- \"New Last Element\"\n\n# now we print the whole list\nL_R\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] \"R\"\n\n[[4]]\n[1] \"word\"\n\n[[5]]\n[1] \"New Last Element\"\n\n\nIn python we can do exactly the same thing:\n\n\n\npython:\n\nL_Py[4] = \"New Last Element\"\n\n# now print the whole python list\nL_Py\n\n\n[1, 2, 'R', 'word', 'New Last Element']\n\n\n\n\n12.2.5 Operating on every element of a list\nSince vectorized operations do not occur automatically over the elements of lists in R, you may be familiar with the lapply() function in R, which applies a function to every element of a list. So, if we wanted to append “Weird!” to every element of list L_R, we could do something like this:\n\n\n\nR:\n\nlapply(L_R, function(x) paste(x, \"Weird!\"))\n\n\n[[1]]\n[1] \"1 Weird!\"\n\n[[2]]\n[1] \"2 Weird!\"\n\n[[3]]\n[1] \"R Weird!\"\n\n[[4]]\n[1] \"word Weird!\"\n\n[[5]]\n[1] \"New Last Element Weird!\"\n\n\nHere, we have, on the fly, defined a new function using the function() function in R. This is called an “anonymous” function because we do not give it a name, but it returns a function in the context where R is expecting a function (i.e., the FUN argument of lapply()). We see above that lapply() returns a list.\nIn python, there is a function very similar to lapply() in R, and it is called map. Additionally, there is a notion of an “anonymous” function, but, in python, these are called “lambda” functions. The analogous syntax in python for appending “Weird!” to each element of a list would be:\n\n\n\npython:\n\nmap(lambda x: str(x) + \" Weird!\", L_Py)\n\n\n&lt;map object at 0x106c80970&gt;\n\n\nmap is like lapply() but the order of its arguments is reversed—it takes the function first and the list second—and it doesn’t automatically return a list; rather, as you can see, it returns a &lt;map object at some_location&gt; where some some_location is the address of some computer memory, like 0x292826170.\nIf you want to get the actual list back from map(lambda x: str(x) + \" Weird!\", L_Py) you have to pass it into python’s list() function:\n\n\n\npython:\n\nlist(map(lambda x: str(x) + ' Weird!', L_Py))\n\n\n['1 Weird!', '2 Weird!', 'R Weird!', 'word Weird!', 'New Last Element Weird!']\n\n\n\n\n12.2.6 Type conversion is not automatic in Python\nIn R, some type conversion is automatic if it goes from a more restricted to a more general type. In other words, if you are passing an integer or a numeric variable into a function or context that is expecting a string, R will happily convert the numeric variable to a string. Hence you can do:\n\n\n\nR:\n\npaste(\"Pi is\", 3.1415)\n\n\n[1] \"Pi is 3.1415\"\n\n\nIn the above, 3.1415 is a numeric variable, but R is content to convert it to a string to paste() it to the string “Pi is”.\nPython does not automatically convert types to the more general type. As we saw above, to concatenate two strings in python we can use the + operator:\n\n\n\npython:\n\n'string1' + 'string2'\n\n\n'string1string2'\n\n\nHowever python won’t automatically convert an numeric type, say, to a character type to get this job done. In other words, if we do this in python:\n\n\n\npython:\n\nx = \"Pi is \"\ny = 3.1415\n\nx + y\n\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\nWe get an error, rather than getting “Pi is 3.1415”.\nTherefore, when mixing types in an operation, but desiring a particular type as the output, you may need to coerce variables to that output type. For example:\n\n\n\npython:\n\nx + str(y)\n\n\n'Pi is 3.1415'\n\n\nThe five data types in python that correspond to R’s five, main, atomic data types are given in the following tables, along with the functions used for coercion to that data type.\n\n\n\nTable 12.1: Five atomic data types in R and Python\n\n\n\n\n\n\n\n(a) R Types\n\n\n\n\n\ntype\ncoercion\nexamples\n\n\n\n\nlogical\nas.logical()\nTRUE, FALSE\n\n\ninteger\nas.integer()\n3L, 16L\n\n\nnumeric\nas.numeric()\n3, 16, 4.2\n\n\ncomplex\nas.complex()\n3.2 + 4i\n\n\ncharacter\nas.character()\n\"foo\", 'bar'\n\n\n\n\n\n\n\n\n\n\n\n(b) Python Types\n\n\n\n\n\ntype\ncoercion\nexamples\n\n\n\n\nbool\nbool()\nTrue, False\n\n\nint\nint()\n3, 16\n\n\nfloat\nfloat()\n3.0, 16.0, 4.2\n\n\ncomplex\ncomplex()\n3 + 4j, 8.2 + 2.3j\n\n\nstr\nstr()\n\"foo\", 'bar'\n\n\n\n\n\n\n\n\n\n\n\nWe emphasize a few important points here:\n\nPython uses True and False for its Boolean types, while R uses TRUE and FALSE. The capitalization differences are very important. The variations don’t work across languages!\nWhile R has the class() function to tell you what class any object is, python has the type() function that tells you what data type an object is. This can be quite useful for familiarizing yourself with what is going on in your own code, or someone else’s:\n\n\n\n\nR:\n\nlist2 &lt;- list(FALSE, 3L, 3.1415, 3 + 4.2i, \"Boing!\")\nlapply(list2, class)\n\n\n[[1]]\n[1] \"logical\"\n\n[[2]]\n[1] \"integer\"\n\n[[3]]\n[1] \"numeric\"\n\n[[4]]\n[1] \"complex\"\n\n[[5]]\n[1] \"character\"\n\n\n\n\n\npython:\n\nlist2 = [False, 3, 3.1415, 3 + 4.2j, \"Boing!\"]\nlist(map(type, list2))\n\n\n[&lt;class 'bool'&gt;, &lt;class 'int'&gt;, &lt;class 'float'&gt;, &lt;class 'complex'&gt;, &lt;class 'str'&gt;]\n\n\n\nlist is itself a data type in python, and when we write list(map(type, list2)) we are essentially coercing an object of class map to be a list.\n\n\n\n12.2.7 List comprehension: Operating on every element of a list, Part 2\nPreviously, we saw the R lapply() command. We note here that you could have applied a function to every element of a list in R using a for loop. We could have done something like this:\n\n\n\nR:\n\nL_R2 &lt;- list()\nfor(i in 1:length(L_R)) L_R2[[i]] &lt;- paste(L_R[[i]], \"Weird!\")\n\n# now print it out:\nL_R2\n\n\n[[1]]\n[1] \"1 Weird!\"\n\n[[2]]\n[1] \"2 Weird!\"\n\n[[3]]\n[1] \"R Weird!\"\n\n[[4]]\n[1] \"word Weird!\"\n\n[[5]]\n[1] \"New Last Element Weird!\"\n\n\nThe above is not considered good R style, it is not much fun to look at and it will operate more slowly than lapply() in most circumstances. However, it is good to see this to motivate an approach in python for operating on every element of a list (and returning a list) that is considered good python style. This is called the syntax of “list comprehension”, and it looks like this:\n\n\n\npython, but not meant to be run:\n\n[\"do something to x\" for x in list]\n\n\nwhere “do something to x” is replaced by some operation on a variable called x (or whatever you might want to call it) and list is an existing list object.\nSo, to paste “Weird!” onto the elements of the list L_Py via list comprehension we would do:\n\n\n\npython:\n\n[str(x) + \" Weird!\" for x in L_Py]\n\n\n['1 Weird!', '2 Weird!', 'R Weird!', 'word Weird!', 'New Last Element Weird!']\n\n\nSee that it returns a list. Unlike in R, for loops, and especially those within a list comprehension, are not eschewed.\nThere is actually a further subtlety here. The for in there can cycle over the elements in a list, but in fact, it is much more general. It can cycle over all the elements in any “iterable” (i.e., able to be iterated over), object. For example a single string like \"ABCDEFG\" is iterable—python will iterate over the different letters in the string. Thus you could make a list of the letters in a string like:\n\n\n\npython:\n\n[x for x in \"ABCDEFG\"]\n\n\n['A', 'B', 'C', 'D', 'E', 'F', 'G']\n\n\nIn fact, you can go for even more economy and note that list() in python applied to any iterable object, will automatically iterate over the iterable elements in the object. Hence, this does the same as the previous line:\n\n\n\npython:\n\nlist(\"ABCDEFG\")\n\n\n['A', 'B', 'C', 'D', 'E', 'F', 'G']\n\n\nThat is good to keep in mind. If you want a list of length 1 that includes the string \"ABCDDEFG\", then, rather that list(\"ABCDEFG\"), you would want to do:\n\n\n\npython:\n\n[\"ABCDEFG\"]\n\n\n['ABCDEFG']\n\n\n\n\n12.2.8 For lists, + in python is like c()—it catenates the lists\nIn R, if you want to add one list onto the end of another, essentially catenating them, you would use c() like this:\n\n\n\nR:\n\nc(L_R, list(\"add\", \"some\", \"more\", 3))\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] \"R\"\n\n[[4]]\n[1] \"word\"\n\n[[5]]\n[1] \"New Last Element\"\n\n[[6]]\n[1] \"add\"\n\n[[7]]\n[1] \"some\"\n\n[[8]]\n[1] \"more\"\n\n[[9]]\n[1] 3\n\n\nIn python, you use + for that type of operation:\n\n\n\npython:\n\nL_Py + [\"add\", \"some\", \"more\", 3]\n\n\n[1, 2, 'R', 'word', 'New Last Element', 'add', 'some', 'more', 3]\n\n\n\n\n12.2.9 Named lists in R and dicts in python\nIn R, the separate elements of a list can be named and we can use the names to access them. For exmaple:\n\n\n\nR:\n\ndinners &lt;- list(\n  Monday = \"Arctic char\",\n  Tuesday = \"Chicken soup\",\n  Wednesday = \"Pizza\",\n  Thursday = \"Stir fry\",\n  Friday= \"Fend for yourselves, kids!\"\n)\n\n\nAnd then we can access each element of the list by its name in two ways:\n\n\n\nR:\n\n# either with the dollar sign\ndinners$Tuesday\n\n\n[1] \"Chicken soup\"\n\n# or with double brackets\ndinners[[\"Friday\"]]\n\n[1] \"Fend for yourselves, kids!\"\n\n\nAnd of course we can assign values to that list in either way:\n\n\n\nR:\n\n# make the changes\ndinners$Friday &lt;- \"Go out to brewery\"\ndinners[[\"Tuesday\"]] &lt;- \"Burritos\"\n\n# then see what that did:\ndinners\n\n\n$Monday\n[1] \"Arctic char\"\n\n$Tuesday\n[1] \"Burritos\"\n\n$Wednesday\n[1] \"Pizza\"\n\n$Thursday\n[1] \"Stir fry\"\n\n$Friday\n[1] \"Go out to brewery\"\n\n\nInterestingly, the elements of a named list in R can still be accessed by their index. We could get “Pizza” out of the dinners list in three different ways:\n\n\n\nR:\n\ndinners$Wednesday\n\n\n[1] \"Pizza\"\n\ndinners[[\"Wednesday\"]]\n\n[1] \"Pizza\"\n\ndinners[[3]]\n\n[1] \"Pizza\"\n\n\nLists in python do not take names like lists in R; however, there is another data type in python that does behave something like a named list. This data type is called a dict, which is, I believe, short for “dictionary.” We can make a dict of our dinners like this:\n\n\n\npython:\n\ndinners = {\n  \"Monday\": \"Arctic char\",\n  \"Tuesday\": \"Chicken soup\",\n  \"Wednesday\": \"Pizza\",\n  \"Thursday\":  \"Stir fry\",\n  \"Friday\": \"Fend for yourselves, kids!\"\n}\n\n\nAnd we can print it like this:\n\n\n\npython:\n\ndinners\n\n\n{'Monday': 'Arctic char', 'Tuesday': 'Chicken soup', 'Wednesday': 'Pizza', 'Thursday': 'Stir fry', 'Friday': 'Fend for yourselves, kids!'}\n\n\nwhich is not much fun to look at, and gets really ugly when the dict has multiple nested levels (which is definitely possible, and we will see examples of that when we start playing with Snakemake config files).\nAs a side note, there is a python module called pprint that will pretty-print a variety of python objects. It can be used like this:\n\n\n\npython:\n\n# import only needs to happen once per session\nimport pprint\n\npprint.pp(dinners)\n\n\n{'Monday': 'Arctic char',\n 'Tuesday': 'Chicken soup',\n 'Wednesday': 'Pizza',\n 'Thursday': 'Stir fry',\n 'Friday': 'Fend for yourselves, kids!'}\n\n\nThat is way more palatable!\nElements of a dictionary can be accessed much like elements of a named list in R, but python uses [ ] for that (rather than $ or [[ ]]) like this:\n\n\n\npython:\n\ndinners[\"Thursday\"]\n\n\n'Stir fry'\n\n\nIn R, lists can be nested with multiple levels, and the same is true with dicts in python. (In fact, it is true with python lists, as well…)\n\n\n\npython:\n\nmeals = {\n  \"Monday\": {\"breakfast\": \"eggs\", \"lunch\": \"salad\", \"dinner\": \"Arctic char\"},\n  \"Tuesday\": {\"breakfast\": \"oatmeal\", \"lunch\": \"sandwich\", \"dinner\": \"Chicken soup\"}\n}\n\n\nNow, each element at the top level (Monday or Tuesday) is itself a dict with three elements (breakfast, lunch, and dinner). If you want to get the value for Tuesday’s lunch, you just drill down into the dict, much like you would in a named R list:\n\nmeals[\"Tuesday\"][\"lunch\"]\n\n'sandwich'\n\n\nWe will come back to the dict data struture a lot when we talk about how Snakemake parses and stores the information in YAML formatted snakemake config files.\n\n\n12.2.10 Classes and objects\nR has a system called S4 that allows for very general kinds of objects to be defined. Most people don’t fiddle with that too much, so it isn’t much help by analogy. Python also has a mechanism to define certain classes of objects that can be very general. The programmer can define a class of objects, and then create an object which is an instance of a class. For our purposes working with snakemake, the important part is that instances of a class can hold attributes that can be just about anything.\nWe are discussing this because Snakemake, when it is executing, carries around values of wildcards in these sorts of objects, and you can access the wildcard values as attributes of a wildcards object (e.g. you can get their values using wildcards.sample or wildcards.chromo) and use those values in what are called input functions which we will see in the next chapter.\nHere we just present a simple example of defining a class, creating an object that is an instance of that class, and then setting and accessing attribute values in it.\n\n\n\npython:\n\n# Define class. Can be named anything. We name this Foo.\n# pass is just a placeholder.  There could/should be some code there\n# but our silly example class, here, does nothing, so we just give\n# it \"pass\" which is code that does nothing\nclass Foo(object):\n  pass\n\n# create an instance of class Foo called myfoo\nmyfoo = Foo()\n\n# assign some values to some attributes. Uses the . (dot)\nmyfoo.attrib1 = \"bing\"\nmyfoo.attrib2 = \"bong\"\nmyfoo.hello = [\"bonjour\", \"hola\", \"hallo\"]\n\n# access the value of those attributes\nmyfoo.attrib1\n\n\n'bing'\n\nmyfoo.attrib2\n\n'bong'\n\nmyfoo.hello\n\n['bonjour', 'hola', 'hallo']\n\n\nSo, we just wanted people to see this because it turns out it is helpful when writing snakemake input functions to be able to create a fake wildcards object for testing those input functions.\n\n\n12.2.11 Defining functions\nHere we will compare and contrast how functions are defined in R and python, and in the process we will see how blocks of code in python are defined by indentation, rather than by, say, curly braces, as in R.\nIf you are familiar with R functions, then you know you can define one like this:\n\n\n\nR:\n\nadd2 &lt;- function(x) {\n  x + 2\n}\n\n\nThat is a function that adds 2 to its argument. Like this:\n\n\n\nR:\n\nadd2(6)\n\n\n[1] 8\n\n\nNot very exciting, but we see that the code of the function is enclosed in curly braces, and, in R, the value produced by the last line of code gets returned by the function.\nIn python, the equivalent function would be defined this way:\n\n\n\npython:\n\ndef add2(x):\n  return x + 2\n\n\nAnd you can use it like this:\n\n\n\npython:\n\nadd2(6)\n\n\n8\n\n\nInstead of putting the code between curly braces, python uses indentation to know which code belongs to the function. Any lines indented to the same degree (here, two spaces) after the colon on the first line are understood to be part of the function definition. Importantly, unlike R, python does not automatically return the value produced by the last line of code in the function. If you want your python function to return a value, you must prepend the value or object that you want returned with the keyword return\nWe might also point out here that anywhere you might typically use curly braces in R to denote a block of code, python accomplishes the same thing with a colon and indentation—for loops, if and else statements, etc.\nThe parameters of a python function can be any valid python objects and you can have multiple parameters and they can have default values, if desired. For example\n\n\n\npython:\n\n# here is a function that adds text (\"Fun!\", by default) to the elements of a list\n# and returns a new list:\ndef fun(list_of_things, text=\"Fun!\"):\n  return [ x + \", \" + text for x in list_of_things]\n\n\nIf we call it with no value for the text parameter we get the default value for text.\n\n\n\npython:\n\nfun(list_of_things = [\"dogs\", \"kittens\", \"frisbees\"])\n\n\n['dogs, Fun!', 'kittens, Fun!', 'frisbees, Fun!']\n\n\nBut we could add the text argument to change it:\n\n\n\npython:\n\nfun(list_of_things = [\"snakes\", \"lizards\", \"squid\"], text = \"Slithery!\")\n\n\n['snakes, Slithery!', 'lizards, Slithery!', 'squid, Slithery!']\n\n\n\n\n12.2.12 R packages and python modules\nIn R, when you want to include a bunch of useful functions from an R package, you can use the library command:\n\n\n\nR:\n\nlibrary(tidyverse)\n\n\nIn python, you can do something similar by importing “modules.” We have already seen that with the pprint module for pretty printing things—we used:\n\n\n\npython:\n\nimport pprint\n\n\nThat statement imports a pprint object, and once that is done we can use all the functions from the pprint module by naming them as attributes of the pprint object:\n\n\n\npython:\n\npprint.pp(meals)\n\n\n{'Monday': {'breakfast': 'eggs', 'lunch': 'salad', 'dinner': 'Arctic char'},\n 'Tuesday': {'breakfast': 'oatmeal',\n             'lunch': 'sandwich',\n             'dinner': 'Chicken soup'}}\n\n\nIn python, we can also be more fine-grained about what we import from different modules. At its most fine-grainedness, we could import just a single function from a module so that we can call that function by name, without calling it as an attribute of an object. For example, if we wanted to play with the multiext function from snakemake (which happens to reside in a module named snakemake.io) we can import it like this:\n\n\n\npython:\n\nfrom snakemake.io import multiext\n\n\nNow we can use the multiext function to see how it works:\n\n\n\npython:\n\nmultiext(\"path/dir/file\", \".pdf\", \".jpg\")\n\n\n['path/dir/file.pdf', 'path/dir/file.jpg']\n\n\nNote that this works because we are using python in the conda environment that includes snakemake. (Remember the line like use_condaenv(condaenv = \"/projects/eriq@colostate.edu/miniforge3/envs/snakemake-8.5.3\")?) Otherwise, python typically would not find the snakemake module.\nOne can also import a module, but give its namespace a shorter name. For example, to prepare for the next section, we will import the ‘pandas’ module, but we will name it pd for brevity.\n\n\n\npython:\n\nimport pandas as pd\n\n\n\n\n12.2.13 Tabular data: pandas in python is sort of like dplyr in R\nMost R users are now familiar with the ‘tidyverse’ and its packages like ‘readr’ and ‘dplyr’. We give an example here, reading a small tab-separated file with information about the samples in our example Chinook salmon data set, which is in the repo at: data/config/units.tsv. The easiest way to peruse it is to view it on GitHub here.\nIf we wanted to read it into R using the tidyverse, that would be pretty familiar (at least if we are tidyverse aficionados):\n\n\n\nR:\n\nlibrary(tidyverse)\n\nunits &lt;- read_tsv(\"./data/config/units.tsv\")\n\n# and then we print it to see the familiar tibble format\nunits\n\n\n# A tibble: 16 × 12\n   sample     unit library flowcell platform  lane sample_id barcode fq1   fq2  \n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n 1 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 2 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 3 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 4 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 5 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 6 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 7 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 8 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n 9 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n10 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n11 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n12 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n13 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n14 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n15 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n16 DPCh_pla…     1 plate1  HTYYCBB… ILLUMINA     1 DPCh_pla… DPCh_p… data… data…\n# ℹ 2 more variables: kb1 &lt;dbl&gt;, kb2 &lt;dbl&gt;\n\n\nThe python world has a similar package, called pandas. We imported it above, using import pandas as pd, so now we can use it to do a comparable read operation on our tab-separated file using the pandas function read_table(). We will also force each column to be stored as a string, too (which is often helpful if we are going to use these values in snakemake later…) Pandas also offers indexing of data frame for faster access, so, in this case we will index each row by the sample name\n\n\n\npython:\n\nunits = pd.read_table(\"./data/config/units.tsv\", dtype=str).set_index(\n    \"sample\", drop=False\n)\n\n# and then we can print it out:\nunits\n\n\n                                  sample unit  ...        kb1        kb2\nsample                                         ...                      \nDPCh_plate1_B10_S22  DPCh_plate1_B10_S22    1  ...  10822.571  12438.261\nDPCh_plate1_B11_S23  DPCh_plate1_B11_S23    1  ...  10838.002  12026.601\nDPCh_plate1_B12_S24  DPCh_plate1_B12_S24    1  ...  11437.975  12892.735\nDPCh_plate1_C10_S34  DPCh_plate1_C10_S34    1  ...   14139.54  15767.478\nDPCh_plate1_C11_S35  DPCh_plate1_C11_S35    1  ...   9705.477   11108.45\nDPCh_plate1_C12_S36  DPCh_plate1_C12_S36    1  ...  10467.881  12064.275\nDPCh_plate1_D09_S45  DPCh_plate1_D09_S45    1  ...  12167.214  13628.883\nDPCh_plate1_D11_S47  DPCh_plate1_D11_S47    1  ...  12337.821  13929.613\nDPCh_plate1_F10_S70  DPCh_plate1_F10_S70    1  ...  10805.055  11871.849\nDPCh_plate1_F11_S71  DPCh_plate1_F11_S71    1  ...  12583.866  14308.931\nDPCh_plate1_F12_S72  DPCh_plate1_F12_S72    1  ...   14848.51  16313.417\nDPCh_plate1_G09_S81  DPCh_plate1_G09_S81    1  ...  20816.204  22987.841\nDPCh_plate1_G10_S82  DPCh_plate1_G10_S82    1  ...  24775.084  27340.307\nDPCh_plate1_G12_S84  DPCh_plate1_G12_S84    1  ...  18082.229  20757.831\nDPCh_plate1_H09_S93  DPCh_plate1_H09_S93    1  ...   9702.925  10835.209\nDPCh_plate1_H10_S94  DPCh_plate1_H10_S94    1  ...  11179.913  12277.416\n\n[16 rows x 12 columns]\n\n\nThat looks pretty similar. The first column in that output shows that sample is the index column (which we set with .set_index(), after reading it) of this pandas data frame.\nNow, in R, if we wanted to pull out the values in some of the columns that correspond to a particular sample, we could do like this:\n\n\n\nR:\n\n# get fq1 value for DPCh_plate1_C12_S36\nunits %&gt;%\n  filter(sample == \"DPCh_plate1_C12_S36\") %&gt;%\n  pull(fq1)\n\n\n[1] \"data/fastqs/DPCh_plate1_C12_S36_R1.fq.gz\"\n\n# get kb2 value for DPCh_plate1_C12_S36\nunits %&gt;%\n  filter(sample == \"DPCh_plate1_C12_S36\") %&gt;%\n  pull(kb2)\n\n[1] 12064.27\n\n\nIn pandas you can do a similar thing with the .loc method (short for “location”) of units, leveraging the fact that we are using sample as the index. The syntax is:\ndata_frame.loc( value of index,  Column )\nSo, analagous to the above we could do, in python:\n\n\n\npython:\n\n# get fq1 value for DPCh_plate1_C12_S36\nunits.loc[ \"DPCh_plate1_C12_S36\", \"fq1\"]\n\n\n'data/fastqs/DPCh_plate1_C12_S36_R1.fq.gz'\n\n# get kb2 value for DPCh_plate1_C12_S36\nunits.loc[ \"DPCh_plate1_C12_S36\", \"kb2\"]\n\n'12064.275'\n\n\nNote that if you want to extract values based on multiple values (both the samples and unit column, for example) you could do\n\n\n\npython:\n\nunits2 = units.set_index([\"sample\", \"unit\"], drop = False) \n\nunits2.loc[ (\"DPCh_plate1_C12_S36\", \"1\"), \"kb2\" ]\n\n\n'12064.275'\n\n\nThese are handy ways of populating parameters of rules with values that are taken from tabular data, for example, getting read group information for different samples.\nWe should point out that we can also use the .loc method to test more general conditions in the pandas data frame. For example, if we wanted a list of all the samples which have an F in their name we could put that condition test into place where the index went before:\n\n\n\npython:\n\nunits.loc[ units[\"sample\"].str.contains(\"F\"), \"sample\" ]\n\n\nsample\nDPCh_plate1_F10_S70    DPCh_plate1_F10_S70\nDPCh_plate1_F11_S71    DPCh_plate1_F11_S71\nDPCh_plate1_F12_S72    DPCh_plate1_F12_S72\nName: sample, dtype: object\n\n\nWe see that this sort of construct returns the sample column in the correct rows, but it is still a pandas object. If we want to just get a list of those sample names, we use .tolist() on the end:\n\n\n\npython:\n\nunits.loc[ units[\"sample\"].str.contains(\"F\"), \"sample\" ].tolist()\n\n\n['DPCh_plate1_F10_S70', 'DPCh_plate1_F11_S71', 'DPCh_plate1_F12_S72']\n\n\nThose things come in handy when writing more complex workflows.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Snakemake-relevant Python for R Users</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-embellishments.html",
    "href": "nmfs-bioinf/snakemake-embellishments.html",
    "title": "13  Important Snakemake Embellishments",
    "section": "",
    "text": "13.1 Running Snakemake on a Cluster: resources, profiles, and benchmarks",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Important Snakemake Embellishments</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-embellishments.html#running-snakemake-on-a-cluster-resources-profiles-and-benchmarks",
    "href": "nmfs-bioinf/snakemake-embellishments.html#running-snakemake-on-a-cluster-resources-profiles-and-benchmarks",
    "title": "13  Important Snakemake Embellishments",
    "section": "",
    "text": "13.1.1 Specify the resources required for each rule\nRecall from our discussion of SLURM that the main axes upon which you will need to think about allocating computing resources for your bioinformatic jobs are:\n\nMemory: what is the maximum amount of RAM the job will need while running?\nTime: How long must the job run?\nCores: how many CPUs do you want to allocate to the job?\n\nAn additional resource that Snakemake might be concerned about is tmpdir the location of a directory for writing temporary files. Typically these files get written to a directory called /tmp on the hard drive that is local to the node that the job is running on. However, at least last summer, we found that we were often exceeding the amount of space that we were offered in /tmp on Alpine, so we will show you how to instruct snakemake to write temporary files into a directory in the results directory on scratch.\nWithin your Snakefile, you can specify the memory and/or time resources for each rule in a resources block for that rule. For example, if we wanted 6 GB of memory and 10 hours for our map_reads rule, then we would add a resources block so that it looks like:\n\nrule map_reads:\n  input:\n    r1=\"results/trimmed/{sample}_R1.fastq.gz\",\n    r2=\"results/trimmed/{sample}_R2.fastq.gz\",\n    genome=\"resources/genome.fasta\",\n    idx=multiext(\"resources/genome.fasta\", \".0123\", \".amb\", \".ann\", \".bwt.2bit.64\", \".pac\")\n  output:\n    \"results/bam/{sample}.bam\"\n  conda:\n    \"envs/bwa2sam.yaml\"\n  resources:\n    mem_mb=6000,\n    time=\"08:00:00\"\n  log:\n    \"results/logs/map_reads/{sample}.log\"\n  params:\n    RG=\"-R '@RG\\\\tID:{sample}\\\\tSM:{sample}\\\\tPL:ILLUMINA' \"\n  shell:\n    \" (bwa-mem2 mem {params.RG} {input.genome} {input.r1} {input.r2} | \"\n    \" samtools view -u | \"\n    \" samtools sort - &gt; {output}) 2&gt; {log} \"\n\nIn the above, we have named and specified our resources the way SLURM would.\nAdditionally, if for most of our rules we simply wanted a default time of 4 hours and a default amount of 3.74 Gb of memory, we don’t have to write a resources block for each of those rules. Rather, we could just specify that on the command line with the --default-resources option, like this:\n--default-resources  mem_mb=3.74  time=\"04:00:00\"\nThat is handy if most of your rules require the same resources.\nWe have not yet discussed how to request a certain number of cores/CPUs for each job of a rule. Remember, there is no point of requesting more cores/CPUs than the number of threads that can be used by the programs running the job. Consequently, you want the number of cores/CPUs to typically be set to the number of threads that the program(s) in your job are using. As a consequence, CPUs are typically set according to the threads specifier in the Snakemake rule. We haven’t talked about this yet, but it is simple—you set an integer value in a threads block, then you can use {threads} in the shell block to tell the program how many threads to use.\nWe will again illustrate this with our map_reads rule. bwa-mem2 mem uses the -t option to specify the number of threads to use, so, to make bwa-mem2 mem use 4 threads we could write:\n\nrule map_reads:\n  input:\n    r1=\"results/trimmed/{sample}_R1.fastq.gz\",\n    r2=\"results/trimmed/{sample}_R2.fastq.gz\",\n    genome=\"resources/genome.fasta\",\n    idx=multiext(\"resources/genome.fasta\", \".0123\", \".amb\", \".ann\", \".bwt.2bit.64\", \".pac\")\n  output:\n    \"results/bam/{sample}.bam\"\n  conda:\n    \"envs/bwa2sam.yaml\"\n  threads: 4\n  resources:\n    mem_mb=3740 * 4,\n    time=\"08:00:00\"\n  log:\n    \"results/logs/map_reads/{sample}.log\"\n  params:\n    RG=\"-R '@RG\\\\tID:{sample}\\\\tSM:{sample}\\\\tPL:ILLUMINA' \"\n  shell:\n    \" (bwa-mem2 mem -t {threads} {params.RG} {input.genome} {input.r1} {input.r2} | \"\n    \" samtools view -u | \"\n    \" samtools sort - &gt; {output}) 2&gt; {log} \"\n\nNote we have specified 4 threads, and we also bumped up the memory, because if we are asking for 4 cores, we might as well get the default amount of memory (3.74 Gb, on Alpine) that we would get with each core.\nAll of that is pretty straightforward; however, doing this does not automatically cause the rule to be dispatched by SLURM and granted those resources. Rather, Snakemake must be told to send jobs to SLURM and also to use the resources and threads to appropriately request the right amount of resources from SLURM. Snakemake versions &lt; 8.0 had an older “officially sanctioned/supported” way of sending jobs to SLURM that can be found here. However, that approach is quite complex—it is implemented in a bunch of python scripts that I find hard to read, and I find it difficult to understand what all these scripts are doing.\nSnakemake version 8.0 brought big changes, and snakemake versions &gt;= 8.0 include an --executor option that uses snakemake-executor-plugins. These offer a relatively painless way of running Snakemake with SLURM. One of the advantages of this is that you should be able to set your workflow up to run in SLURM with the SLURM snakemake executor plugin, and then, with very few changes to your workflow you should be able to also run it using Google Batch or Amazon’s Cloud compute infrastructure using a different snakemake executor plugin. That has some big advantges, but it comes at the cost of flexibility. For example, when running the official Snakemake &gt;= 8.0 SLURM executor plugin, the job names are completely meaningless and not useful at all.\nBy contrast, a different approach to having Snakemake send jobs to SLURM is implemented in the wonderfully readable and easy-to-understand approach taken by John D. Blischak, (find it here on GitHub) John was a postdoc with Matthew Stephens and has worked extensively in improving research reproducibility. His method is implemented almost entirely by using command line options (plus one little script to help check the status of jobs in the SLURM queue). However, to avoid having to type all these options at the command line every time you launch snakemake to use SLURM, these command line options can be saved and read from a snakemake profile. We discuss profiles in the next section and show how they can be used to allow snakemake to submit jobs to SLURM. This works on both Snakemake versions &lt; 8.0 and &gt;= 8.0, but the profiles are slightly different.\n\n\n13.1.2 Snakemake profiles\nWe begin this section by quickly perusing all the different command line options that can be used when launching snakemake. If we type snakemake --help we get a listing of all the options. It starts with a synopsis of all the options, which is followed by longer explanations of each individual option. The synopsis looks like this:\nusage: snakemake [-h] [--dry-run] [--profile PROFILE] [--workflow-profile WORKFLOW_PROFILE] [--cache [RULE ...]] [--snakefile FILE] [--cores N]\n                 [--jobs N] [--local-cores N] [--resources NAME=INT [NAME=INT ...]] [--set-threads RULE=THREADS [RULE=THREADS ...]]\n                 [--max-threads MAX_THREADS] [--set-resources RULE:RESOURCE=VALUE [RULE:RESOURCE=VALUE ...]]\n                 [--set-scatter NAME=SCATTERITEMS [NAME=SCATTERITEMS ...]] [--set-resource-scopes RESOURCE=[global|local] [RESOURCE=[global|local]\n                 ...]] [--default-resources [NAME=INT ...]] [--preemptible-rules [PREEMPTIBLE_RULES ...]]\n                 [--preemptible-retries PREEMPTIBLE_RETRIES] [--config [KEY=VALUE ...]] [--configfile FILE [FILE ...]]\n                 [--envvars VARNAME [VARNAME ...]] [--directory DIR] [--touch] [--keep-going]\n                 [--rerun-triggers {code,input,mtime,params,software-env} [{code,input,mtime,params,software-env} ...]] [--force]\n                 [--executor {local,dryrun,touch}] [--forceall] [--forcerun [TARGET ...]] [--prioritize TARGET [TARGET ...]]\n                 [--batch RULE=BATCH/BATCHES] [--until TARGET [TARGET ...]] [--omit-from TARGET [TARGET ...]] [--rerun-incomplete]\n                 [--shadow-prefix DIR] [--scheduler [{ilp,greedy}]] [--wms-monitor [WMS_MONITOR]] [--wms-monitor-arg [NAME=VALUE ...]]\n                 [--scheduler-ilp-solver {PULP_CBC_CMD,COIN_CMD}] [--scheduler-solver-path SCHEDULER_SOLVER_PATH]\n                 [--conda-base-path CONDA_BASE_PATH] [--no-subworkflows] [--precommand PRECOMMAND] [--groups GROUPS [GROUPS ...]]\n                 [--group-components GROUP_COMPONENTS [GROUP_COMPONENTS ...]] [--report [FILE]] [--report-stylesheet CSSFILE] [--reporter PLUGIN]\n                 [--draft-notebook TARGET] [--edit-notebook TARGET] [--notebook-listen IP:PORT] [--lint [{text,json}]]\n                 [--generate-unit-tests [TESTPATH]] [--containerize] [--export-cwl FILE] [--list-rules] [--list-target-rules] [--dag] [--rulegraph]\n                 [--filegraph] [--d3dag] [--summary] [--detailed-summary] [--archive FILE] [--cleanup-metadata FILE [FILE ...]] [--cleanup-shadow]\n                 [--skip-script-cleanup] [--unlock] [--list-changes {params,code,input}] [--list-input-changes] [--list-params-changes]\n                 [--list-untracked] [--delete-all-output | --delete-temp-output] [--keep-incomplete] [--drop-metadata]\n                 [--deploy-sources QUERY CHECKSUM] [--version] [--printshellcmds] [--debug-dag] [--nocolor] [--quiet [{all,progress,rules} ...]]\n                 [--print-compilation] [--verbose] [--force-use-threads] [--allow-ambiguity] [--nolock] [--ignore-incomplete]\n                 [--max-inventory-time SECONDS] [--latency-wait SECONDS] [--wait-for-files [FILE ...]] [--wait-for-files-file FILE]\n                 [--queue-input-wait-time SECONDS] [--notemp] [--all-temp] [--unneeded-temp-files FILE [FILE ...]] [--keep-storage-local-copies]\n                 [--target-files-omit-workdir-adjustment] [--allowed-rules ALLOWED_RULES [ALLOWED_RULES ...]]\n                 [--target-jobs TARGET_JOBS [TARGET_JOBS ...]] [--local-groupid LOCAL_GROUPID] [--max-jobs-per-second MAX_JOBS_PER_SECOND]\n                 [--max-status-checks-per-second MAX_STATUS_CHECKS_PER_SECOND] [--seconds-between-status-checks SECONDS_BETWEEN_STATUS_CHECKS]\n                 [--retries RETRIES] [--attempt ATTEMPT] [--wrapper-prefix WRAPPER_PREFIX] [--default-storage-provider DEFAULT_STORAGE_PROVIDER]\n                 [--default-storage-prefix DEFAULT_STORAGE_PREFIX] [--local-storage-prefix LOCAL_STORAGE_PREFIX]\n                 [--shared-fs-usage {input-output,persistence,software-deployment,source-cache,sources,storage-local-copies,none} [{input-output,persistence,software-deployment,source-cache,sources,storage-local-copies,none} ...]]\n                 [--scheduler-greediness SCHEDULER_GREEDINESS] [--no-hooks] [--debug] [--runtime-profile FILE] [--mode {default,remote,subprocess}]\n                 [--show-failed-logs] [--log-handler-script FILE] [--log-service {none,slack,wms}] [--job-deploy-sources] [--container-image IMAGE]\n                 [--immediate-submit] [--jobscript SCRIPT] [--jobname NAME] [--flux]\n                 [--software-deployment-method {apptainer,conda,env-modules} [{apptainer,conda,env-modules} ...]] [--container-cleanup-images]\n                 [--use-conda] [--conda-not-block-search-path-envvars] [--list-conda-envs] [--conda-prefix DIR] [--conda-cleanup-envs]\n                 [--conda-cleanup-pkgs [{tarballs,cache}]] [--conda-create-envs-only] [--conda-frontend {conda,mamba}] [--use-apptainer]\n                 [--apptainer-prefix DIR] [--apptainer-args ARGS] [--use-envmodules] [--report-html-path VALUE]\n                 [--report-html-stylesheet-path VALUE]\n                 [targets ...]\nIn this synopsis, command line options that are truly optional (which is basically all of them!) are wrapped in [ ]. You will notice that some of these options, like --dry-run (look at the top line of the synopsis) do not take any arguments (there is nothing after --dry-run in the [ ]), while other options, like --profile take an argument, in the above listing, of PROFILE. Clearly if you use even a small fraction of these optional command-line options to snakemake, the stuff you type on the command line could grow long, typing it in on the command line could become quite ponderous, and the chance to make a mistake would grow, the longer the command line got. Not only that, but if you were just putting these options on the command line, it would be easy to forget exactly which options you used!\nThis is where snakemake profiles come in. A snakemake profile is a collection of files in a directory, one of which is named config.yaml. So, if I have directory ./my-profile and within that a file ./my-profile/config.yaml, then, I can store the command line options I want to use in YAML format in ./my-profile/config.yaml and those options will be added to the command line if I invoke snakemake with:\nsnakemake --profile ./my-profile\nNote that it is good practice to add the . in front of your profile name, because otherwise Snakemake might search for system-wide or user-wide profiles.\n\n\n\n\n\n\nProfiles for versions &lt; 8.0 and &gt;= 8.0\n\n\n\nThe changes introduced in Snakemake version 8.0 were large enough that older profiles might not run on versions &gt;= 8.0 and profiles meant for versions &gt;= 8.0 might not run on versions &lt; 8.0. To handle this, Snakemake’s developers allow for there to be two config files withing the profile directory. In our repo, for example:\nSnakemake-Example/slurm/jdb\n├── alpine\n│   ├── config.v8+.yaml\n│   ├── config.yaml\n│   └── status-sacct-robust.sh\n└── sedna\n    ├── config.v8+.yaml\n    ├── config.yaml\n    └── status-sacct-robust.sh\nThere is a config.v8+.yaml and a config.yaml in these profile directories for alpine and sedna. Snakemake Version &gt;= 8.0 will preferentially use the config.v8+.yaml file, and versions of Snakemake &lt; 8.0 will use config.yaml because they don’t even recognize config.v8+.yaml.\n\n\nSo, how do you write command line options in the YAML-formatted Snakemake profile config.yaml? It is easy:\n\nYou can specify any command line option that is preceded by two dashes, like --option, but you cannot specify options that you might specify with a single dash, like -n or -p (or -np as we often type the two together). This is OK, because every single-dash option has a two-dash name. For example:\n\n-n is the same as --dry-run\n-p is the same as --printshellcmds\n-v is the same as --version and so forth.\n\nIf the option does not take an argument, for example --dry-run or --printshellcmds, then you enter them into the config.yaml file by writing the option name, without the dashes, following it with a colon and then a space and then True, like:\n\ndry-run: True\nprintshellcmds: True\n\nIf the option does take an argument, for example --cores 20 or --rerun-triggers mtime, then you just include the argument after the colon (and a space), like:\n\ncores: 20\nrerun-triggers: mtime\n\n\nAnd that is all there is to it!\n\n\n13.1.3 An Alpine Slurm Profile for Snakemake\nIf you are using Snakemake &gt;= 8.0, you have to do one step to get the JDB’s simple-slurm-smk config to work:\npip install snakemake-executor-plugin-cluster-generic\nDo not do that if you are using Snakemake versions &lt; 8.0.\nWe have a profile for sending jobs to Alpine in the directory Snakemake-Example/slurm/jdb/alpine. The config.v8+.yaml file within that looks like this:\n\n\n\nSnakemake-Example/slurm/jdb/alpine/config.v8+.yaml\n\n\nexecutor: cluster-generic\ncluster-generic-submit-cmd:\n  mkdir -p results/slurm_logs/{rule} &&\n  sbatch\n    --partition=amilan,csu\n    --cpus-per-task={threads}\n    --mem={resources.mem_mb}\n    --time={resources.time}\n    --job-name=smk-{rule}-{wildcards}\n    --output=results/slurm_logs/{rule}/{rule}-%j-{wildcards}.out\n    --error=results/slurm_logs/{rule}/{rule}-%j-{wildcards}.err\n    --parsable\ncluster-generic-status-cmd: status-sacct-robust.sh\ncluster-generic-cancel-cmd: scancel\ncluster-generic-cancel-nargs: 400\n# warning, time here is very small.  It works for the small\n# example data set, but should be increased for production jobs\ndefault-resources:\n  - time=\"00:30:00\"\n  - mem_mb=3740\n  - tmpdir=\"results/snake-tmp\"\nrestart-times: 0\nmax-jobs-per-second: 10\nmax-status-checks-per-second: 25\nlocal-cores: 1\nlatency-wait: 60\ncores: 2400\njobs: 950\nkeep-going: True\nrerun-incomplete: True\nprintshellcmds: True\nsoftware-deployment-method: conda\nrerun-trigger: mtime\n\n\nWe see here that the “heavy-lifting” is done by snakemake’s --cluster-generic-submit-cmd option. This option takes as an argument a command line that should be used to send off individual jobs for execution. As you can see, the command it uses does two main things:\n\nFirst it creates a directory in which to place the error and output files that sbatch will write.\nIf that first command is successful, it then runs sbatch with a variety of options.\n\nVarious values known to snakemake can be substituted into that sbatch command line by enclosing them within curly braces, thereby passing them as arguments to various sbatch options. In particular:\n\n{rule} expands into the rule name\n{threads} expands into the number of threads to be used for the rule\n{wildcards} expands into a comma-separate string with the values of the wildcards in effect for this job, like sample=A,chromo=NC103567.1\n{resources.mem_mb} and {resources.time} print the values given for mem_mb and time in the resources: block of the rule.\n\nEach of these values are used as arguments to different sbatch options in a plainly readable way. The last option given is --parsable which tells sbatch to return the job number of the submitted job in an easily parsable fashion.\nSnakemake takes this sbatch command and uses it to submit a job script. Snakemake itself writes that job script (and stores it in the .snakemake directory for as long as it is needed) according to the code that it finds in the shell block of the rule. This is why you never need to write another sbatch script in your life if you use snakemake—Snakemake writes those scripts for you and submits them via sbatch.\nYou will also notice that the profile defines the --default-resources option that we described above. Note that the default memory is 3740, which is what you get with one core on Alpine, and the default time is 8 hours.\nSome of the remaining options in the profile are not really specific to SLURM, but many are. One of those,\ncluster-generic-status-cmd: status-sacct-robust.sh\ngives the name of a shell script in the profile directory that snakemake uses to check on the status of jobs that have been submitted to the cluster. This is a script that queries SLURM using sacct to ask about the status of running jobs according to their job number. For each job, the script tells Snakemake either:\n\nsuccess: the job finished successfully.\nrunning: the job is still running or waiting in the queue\nfailed: the job failed\n\nWhen your jobs are running, Snakemake is continually checking on their status, but won’t check more than max-status-checks-per-second. (In the case of this profile that is 25 checks per second.)\nYou can unfold the following code block to see the shell code for status-sacct-robust.sh.\n\n\n\nClick the triangle to see the code\n\nSnakemake-Example/slurm/alpine/status-sacct-robust.sh\n\n#!/usr/bin/env bash\n\n# Check status of Slurm job. More robust because runs `sacct` multiple times if\n# needed\n\njobid=\"$1\"\n\nif [[ \"$jobid\" == Submitted ]]\nthen\n  echo smk-simple-slurm: Invalid job ID: \"$jobid\" &gt;&2\n  echo smk-simple-slurm: Did you remember to add the flag --parsable to your sbatch call? &gt;&2\n  exit 1\nfi\n\nfunction get_status(){\n  sacct -j \"$1\" --format State --noheader | head -n 1 | awk '{print $1}'\n}\n\nfor i in {1..20}\ndo\n  output=`get_status \"$jobid\"`\n  if [[ ! -z $output ]]\n  then\n    break\n  else\n    sleep 3\n  fi\ndone\n\nif [[ -z $output ]]\nthen\n  echo sacct failed to return the status for jobid \"$jobid\" &gt;&2\n  echo Maybe you need to use scontrol instead? &gt;&2\n  exit 1\nfi\n\nif [[ $output =~ ^(COMPLETED).* ]]\nthen\n  echo success\nelif [[ $output =~ ^(RUNNING|PENDING|COMPLETING|CONFIGURING|SUSPENDED).* ]]\nthen\n  echo running\nelse\n  echo failed\nfi\n\n\n\nMuch of the code is there to make sure the script tries at least 20 times to get the status of the job, because sometimes SLURM gets busy and fails to return a result for a job, which causes snakemake to do bad things…\nOther SLURM-relevant options in this profile are:\n\n--cluster-generic-cancel-cmd: if the Snakemake run is terminated (with &lt;cntrl&gt;-c for example) then kill all jobs running under SLURM using the scancel command.\n--cluster-generic-cancel-nargs: the maximum number of arguments to pass to scancel when cancelling jobs. This should be high enough to kill all jobs currently running or queued up in SLURM.\n\n--jobs: don’t submit so many jobs that you have more than this number of jobs running or waiting in the SLURM queue. This is quite important in Alpine, because SLURM on Alpine doesn’t let you have more the 1000 jobs running or queued up.\n\n--max-jobs-per-second: don’t submit more than this jobs each second to SLURM\n\nSo, in summary, if you are using snakemake and you had the alpine directory containing config.yaml and status-sacct-robust.sh in the directory you were running snakemake from, you could get snakemake to submit jobs to the cluster using SLURM by doing:\nsnakemake  [some options or targets here] --profile ./alpine  [other options or targets here]\nAlso, we will end this section by noting that you can override options in the profile by adding them actually on the command line, just like with sbatch and its #SBATCH directives.\n\n\n13.1.4 Simple-slurm-smk profile for Snakemake versions &lt; 8.0\nIf you are running snakemake &lt; 8.0, you don’t have the executor option, and some of the command line options are a little shorter:\n\n\n\nClick the triangle to see the code with changes from v8+ highlighted\n\nSnakemake-Example/slurm/jdb/alpine/config.yaml\n\ncluster:\n  mkdir -p results/slurm_logs/{rule} &&\n  sbatch\n    --partition=amilan,csu\n    --cpus-per-task={threads}\n    --mem={resources.mem_mb}\n    --time={resources.time}\n    --job-name=smk-{rule}-{wildcards}\n    --output=results/slurm_logs/{rule}/{rule}-%j-{wildcards}.out\n    --error=results/slurm_logs/{rule}/{rule}-%j-{wildcards}.err\n    --parsable\ncluster-status: status-sacct-robust.sh\ncluster-cancel: scancel\ncluster-cancel-nargs: 400\n# warning, time here is very small.  It works for the small\n# example data set, but should be increased for production jobs\ndefault-resources:\n  - time=\"00:30:00\"\n  - mem_mb=3740\n  - tmpdir=\"results/snake-tmp\"\nrestart-times: 0\nmax-jobs-per-second: 10\nmax-status-checks-per-second: 25\nlocal-cores: 1\nlatency-wait: 60\ncores: 2400\njobs: 950\nkeep-going: True\nrerun-incomplete: True\nprintshellcmds: True\nuse-conda: True\nrerun-trigger: mtime\n\n\n\n\n\n13.1.5 The officially supported slurm executor approach for Snakemake &gt;= 8.0\nOne must first download a snakemake-executor-plugin like this:\npip install snakemake-executor-plugin-slurm\nAfter that, you can use a profile that looks like this:\nexecutor: slurm\ndefault-resources:\n  - runtime=20\n  - mem_mb=3740\n  - tmpdir=\"results/snake-tmp\"\nrestart-times: 0\nmax-jobs-per-second: 10\nmax-status-checks-per-second: 50\nlocal-cores: 1\nlatency-wait: 60\ncores: 2400\njobs: 950\nkeep-going: True\nrerun-incomplete: True\nprintshellcmds: True\nuse-conda: True\nrerun-trigger: mtime\n\n\n13.1.6 A note on local rules\nYou can tell Snakemake to execute rules locally by putting them in a localrules: directive near the top of the Snakefile, like:\nlocalrules: all, genome_faidx, genome_dict",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Important Snakemake Embellishments</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-embellishments.html#snakemake-config-files",
    "href": "nmfs-bioinf/snakemake-embellishments.html#snakemake-config-files",
    "title": "13  Important Snakemake Embellishments",
    "section": "13.2 Snakemake config files",
    "text": "13.2 Snakemake config files\nWe have already seen that we might add some variables to our Snakefile to run a workflow on certain samples, etc.\n\n13.2.1 YAML configuration\nHelpful snippet for testing:\nfrom snakemake.common.configfile import _load_configfile\n\n\n13.2.2 Tabular configuration via pandas",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Important Snakemake Embellishments</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-embellishments.html#snakemake-benchmarks",
    "href": "nmfs-bioinf/snakemake-embellishments.html#snakemake-benchmarks",
    "title": "13  Important Snakemake Embellishments",
    "section": "13.3 Snakemake benchmarks",
    "text": "13.3 Snakemake benchmarks\nCompared the previous two topics, this is really simple.\nRecommended convention, put them in:\nresults/benchmarks/&lt;rule_name&gt;/&lt;first_wildcard&gt;/.../&lt;last_wildcard&gt;.bmk",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Important Snakemake Embellishments</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/snakemake-embellishments.html#some-additional-snakemake-topics",
    "href": "nmfs-bioinf/snakemake-embellishments.html#some-additional-snakemake-topics",
    "title": "13  Important Snakemake Embellishments",
    "section": "13.4 Some additional snakemake topics",
    "text": "13.4 Some additional snakemake topics",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Important Snakemake Embellishments</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/variant-calling.html",
    "href": "nmfs-bioinf/variant-calling.html",
    "title": "14  Variant calling",
    "section": "",
    "text": "14.1 Genotype Likelihoods",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/variant-calling.html#genotype-likelihoods",
    "href": "nmfs-bioinf/variant-calling.html#genotype-likelihoods",
    "title": "14  Variant calling",
    "section": "",
    "text": "14.1.1 Basic Sketch of Genotype Likelihood Calculations\nThere are several genotype likelihood models in use, but they all share a number of properties and assumptions. Perhaps the easiest way to come to understand how genotype likelihoods are calculated is to work through a simple example, like that show in Figure @ref(fig:genolike).\n\nThe figure shows a situation where we know that there is variation (a SNP that has two possible alleles: T and C) at a certain position along a given chromosome. A diploid individual is represented by the two homologous chromosomes that s/he carries, and the alleles carried on those chromosomes are represented by ?’s because we don’t know (without collecting and analyzing data) what the genotype of that individual is.\nThe data that we will use to determine the genotype of this individual are the 4 reads from the individual that cover the position we are interested in. Specifically, from each read we observe:\n\nThe reported base at the position\nThe reported base quality score at the position. Recall from Section @ref(bqscores) that the Phred-scaled base quality score is interpreted as \\(\\lfloor -10\\log_{10}\\epsilon\\rfloor\\), where \\(\\epsilon\\) is the estimated probability that the reported base at the position is incorrect.\n\nWe can condense the data down to the base calls, \\(B\\), and \\(\\epsilon\\)’s at each of the four reads. To do so we convert the Phred score \\(Q\\) to \\(\\epsilon\\) using \\(\\epsilon = 10^{-Q/10}\\)\n\n\\(B_1 = C\\) and \\(\\epsilon_1 = 10^{-32/10} = 0.00063\\)\n\\(B_2 = C\\) and \\(\\epsilon_2 = 10^{-37/10} = 0.00019\\)\n\\(B_3 = T\\) and \\(\\epsilon_3 = 10^{-35/10} = 0.00031\\)\n\\(B_4 = C\\) and \\(\\epsilon_4 = 10^{-33/10} = 0.00050\\)\n\nThose are the raw data that go into our calculations of how likely it is that the true genotype of the individual is either a homozygote, \\(CC\\), or a homozygote, \\(TT\\), or a heterozygote, \\(CT\\) or \\(TC\\) (referred to simply as \\(CT\\) from now on).\nThere are a few different ways that one might go about this task. One of the simplest would be the “method of the eyeball,” by which you would just look at the reads and say, “That individual is probably a heterozygote, \\(CT\\).” This is actually a reasonable assessment in this situation; however it is not highly principled and it is hard to instruct a computer to employ the “method of the eyeball.”\nBy contrast, the method of likelihood provides a principled approach to evaluating how much evidence a given set of data, \\(D\\), provides to distinguish between several different hypotheses. In our case, the three different hypotheses are that the true, underlying genotype, \\(G\\) is either \\(CC\\), \\(CT\\), or \\(TT\\). It is also relatively easy to tell a computer how to compute it.\nLet’s be explicit about our terms. We have three different hypotheses:\n\n\\(H_1:~~~G = CC\\)\n\\(H_2:~~~G = CT\\)\n\\(H_3:~~~G = TT\\)\n\nIn our situation, as we have defined it, those are the three possibilities. And we want to calculate the evidence in our data \\(D\\) (which in this case is \\(B_i\\) and \\(\\epsilon_i\\), for \\(i = 1,2,3,4\\)) in support of those different hypotheses.\nThe method of likelihood states that the evidence in the data \\(D\\) supporting a hypothesis \\(H_i\\), can be quantified as being proportional to the probability of the data given the hypothesis. Hence we write: \\[\nL(H_i~|~D) \\propto P(D~|~H)\n\\] Thus, to compute \\(L(H_1~|~D) = L(G = CC~|~D)\\) we must calculate the probability of observing the four reads, C, C, T, C, given that the true, underlying genotype is \\(CC\\). To do so requires a conceptual model of how reads arrive to us from the different chromosomes in an organism. Forming such a model requires that we make some assumption. Two assumptions that are shared by most genotyping likelihood models are:\n\nReads are sampled independently from the two homologous chromosomes in an individual, each with probability \\(1/2\\).\nGiven the true sequence on the chromosome from which the read is a sample, the base at each position is recorded as the true base on the chromosome with probability \\(1 - \\epsilon\\), and, with probability \\(\\epsilon\\) the base is recorded incorrectly.\n\nWith these two assumptions, it is straightforward to calculate the probability of the observed base on a single read given each of the three different possible true genotypes. Let’s do that with the first read, which has \\(B_1 = C\\) and \\(\\epsilon_1 = 0.00063\\),for the three different possible true genotypes:\n\nIf \\(G=CC\\) then, with probability \\(1/2\\) the read is from the first chromosome or with probability \\(1/2\\) the read is from the second chromosome; however, in either case that read is from a chromosome that carries a \\(C\\). So with probability 1 the read is from a chromosome with a \\(C\\). Hence with probability \\(1-\\epsilon_1\\) the read carries a \\(C\\). So, \\[\nP(B_1=C~|~G=CC, \\epsilon_1) = 1 - \\epsilon_1\n\\]\nIf \\(G=CT\\) then, with probability \\(1/2\\), the read is from the chromosome with a \\(C\\), in which case the probability of observing a \\(C\\) on the read is \\(1 - \\epsilon\\). On the other hand, with probability \\(1/2\\) the read is from the chromosome carrying a \\(T\\), in which case, recording a \\(C\\) requires that a sequencing error occurred. Therefore: \\[\nP(B_1=C~|~G=CT, \\epsilon_1) = \\frac{1}{2}(1 - \\epsilon) + \\frac{1}{2}\\epsilon_1 = \\frac{1}{2}\n\\] Notice that this is less than \\(P(B_1=C~|~G=CC, \\epsilon_1)\\) by a factor of about 2.\nFinally, if \\(G=TT\\), then, with probability 1, the read will come from a chromosome carrying a \\(T\\), and in that case, the only way that we could have recorded a \\(C\\) from the read would be if a sequencing error occurred. Hence: \\[\nP(B_1=C~|~G=TT, \\epsilon_1) = \\epsilon_1\n\\] So, summarizing the information from the first read we have: \\[\n\\begin{aligned}\nL(G=CC | B_1 = C) &=   P(B_1 = C| G=CC, \\epsilon_1) &  &=  1 - \\epsilon_1 & &= 0.99937 \\\\\nL(G=CT | B_1 = C) &=   P(B_1 = C| G=CT, \\epsilon_1) & &=  \\frac{1}{2} & &= 0.5 \\\\\nL(G=TT | B_1 = C) &=   P(B_1 = C| G=TT, \\epsilon_1) & &=  \\epsilon_1 & &= 0.00063\n\\end{aligned}\n\\] A higher likelihood implies more support for a hypothesis. So, with that one read we have good evidence that the true genotype is not \\(G=TT\\), and twice the likelihood that \\(G=CC\\) compared to \\(G=CT\\).\n\nNow, more quickly, we can consider how much evidence read 3, \\(B_3 = T\\) with \\(\\epsilon_3 = 0.00031\\) offers: \\[\n\\begin{aligned}\nL(G=CC | B_3 = T) &=   P(B_3 = T| G=CC, \\epsilon_3) &  &=  \\epsilon_3 & &= 0.00031 \\\\\nL(G=CT | B_3 = T) &=   P(B_3 = T| G=CT, \\epsilon_3) & &=  \\frac{1}{2} & &= 0.5 \\\\\nL(G=TT | B_3 = T) &=   P(B_3 = T| G=TT, \\epsilon_3) & &=  1 - \\epsilon_3 & &= 0.99969\n\\end{aligned}\n\\] OK, that tells us there is very little support for \\(G=CC\\), and twice as much support for \\(G=TT\\) as there is for \\(G=CT\\).\nHow do we combine the likelihoods from the different reads? Well, we compute the probability of observing both of those reads. Since we assumed that reads are sampled independently from the pair of homologous chromosomes, the joint probability of both reads is merely the product of probabilities for each read. So, we have: \\[\n\\small\n\\begin{aligned}\nL(G=CC | B_1 = C, B_3 = T) &=   P(B_1 = C| G=CC, \\epsilon_1) \\times P(B_3 = T| G=CC, \\epsilon_3) & &= 0.99937 \\times 0.00031\\\\\nL(G=CT | B_1 = C, B_3 = T) &=   P(B_1 = C| G=CT, \\epsilon_1) \\times P(B_3 = T| G=CT, \\epsilon_3) & &=  0.5 \\times 0.5 \\\\\nL(G=TT | B_1 = C, B_3 = T) &=  P(B_1 = C| G=CT, \\epsilon_1) \\times P(B_3 = T| G=TT, \\epsilon_3) & &=  0.00063 \\times 0.99969\n\\end{aligned}\n\\] I couldn’t fit the actual products of each of those up above, so we put them down here: \\[\n\\begin{aligned}\nL(G=CC | B_1 = C, B_3 = T)  &= 0.00031\\\\\nL(G=CT | B_1 = C, B_3 = T)  &= 0.25\\\\\nL(G=TT | B_1 = C, B_3 = T)  &= 0.00063\n\\end{aligned}\n\\] So, with only two reads, one of each allele, the likelihood of a heterozygote can be quite high.\nAdding the data from the two remaining reads does not change the likelihood much, apart from factors of 1/2 on the heterozygote category.\nWhat have we learned from this exercise? I’d say there are two very important take-home messages:\n\nIf you have only a single read, the likelihood that the genotype is a homozygote is always higher than the likelihood that it is a heterozygote.\nIf you only see reads of a single allele, the likelihood that the genotype is a heterozygote drops by a factor of two for each new read. (And the likelihood that the genotype is homozygous for the allele you have not seen drops by a factor of the estimated genotyping error each time).\nThe values of the likelihoods for the homozygous hypotheses are highly dependent on the base quality scores but not so much for the likelihood of the heterozygous hypothesis.\n\nBefore we leave this section, we want to stress to the reader that genotype likelihoods are only half of the equation when it comes to determining what an individuals genotype is. We will talk about that more in the section on posterior probabilities. Suffice it to say, at this point, that assigning genotypes according to the maximum likelihood (which is common in programs like GATK) is a terrible idea with low read-depth data, because individuals that are truly heterozygotes will never be called as heterozygotes, unless both alleles have been observed in the read data.\n\n\n14.1.2 Specifics of different genotype likelihoods\nWill (eventually) write out the exact details of samtools’s old and GATK old likelihoods. Also bcftools new likelihood and SOAP.\n\n\n14.1.3 A Directed Acyclic Graph For Genotype Likelihoods\nThe assumptions about independence between reads and the formulation of the genotype likelihood model can be captured in an acyclic directed graph (called a DAG, for short) like that in Figure @ref(fig:single-geno-like-dag). We expand the notation established above, but subscript each variable by an additional \\(i\\) to indicate the data are from the \\(i\\)-th individual, and, rather than referring to the genotype as \\(G\\) we specifically indicate the allelic type of each gene copy within individual \\(i\\) with the variable \\(Y\\).\nThus, \\(B_{i,1}\\) is the base covering the SNP at the first read from indivdiual \\(i\\), \\(\\epsilon_{i,1}\\) is the base quality score (as a probability of a sequencing error) at that SNP on the first read from individual \\(i\\), and \\(Y_{i,1} = C\\) and \\(Y_{i,2} = T\\) denote that the genotype of individual \\(i\\) is heterozygous, \\(CT\\).\n\n\n\n\n\nOf course, we may will typically have sequencing data from multiple individuals. So let us imagine that we have data from \\(N\\) individuals (\\(i=1,\\ldots, N\\)). Additionally, each individual will have a variable number of reads covering the SNP we are focused on. We denote that number of reads by \\(R_i\\) for the \\(i\\)-th individual, and subscript each read by \\(j\\). Hence, in the \\(i\\)-th individual, \\(j=1,\\ldots,R_i\\). Then, our DAG can be expanded to what we find in Figure @ref(fig:plated-single-geno-like).\n\n\n\n\n\nAcyclic directed graphs can be very useful for gleaning the underlying structure of statistical models and also for developing intuition for what is involved in the process of inference.\nI haven’t quite figured out how/where to incorporate a discussion of these topics into this handbook, but in the meantime I want to cherry-pick a few topics about probability and statistics from: https://eriqande.github.io/con-gen-2018/bayes-mcmc-gtyperr-narrative.nb.html\nThere, we will discuss inference and directed acyclic graphs from the perspective of inferring allele frequencies from genotype data. After that discussion, we will understand that inference is the process of learning about unobserved “parent” nodes in a DAG from the data that has been observed in the daughter nodes of such a DAG.\nKnowing that, we can go back to @ref(fig:plated-single-geno-like) to see that making inference about the actual alleles carried in each individual’s genotype requires some sort of “prior” for those genotypes. Such a model is, graphically, what we were just talking about in terms of estimating allele frequencies. Which leads us to a full DAG for making inference about genotypes within individuals (Figure @ref(fig:full-geno-like-dag))\n\nThis is really how genotype inference should be done! And, it is extremely important to understand that many genotype callers provide a “called genotype” in a VCF file that do not infer genotypes using this sort of model. Rather, they may often simply report the genotype with the highest likelihood (but not necessarily the highest posterior probability.)\nAs a consequence, especially with low coverage sequencing data, these genotypes provided by default in a VCF will have far fewer heterozygotes than there should be.\nNow we will jump to some slides from a talk I gave a couple years ago about this topic as it relates to RADseq data: gbs-miscall.pdf\nAnd, point out that the very same problem occurs in low-coverage whole genome sequencing data if you just try to use the genotypes reported in a VCF like that produced by GATK.\nFor an illustration of that, here is a VCF file with 5.16 Mb of Chromosome 28 from 48 fall/late-fall-run Chinook salmon from the Central Valley of California. These fish are from different populations, but the populations are so similar, genetically, that you don’t expect a Wahlund effect by mixing them into a single sample.\nraw-fall-late-fall-geno-calls.vcf.gz.\nDownload that to your laptop, then use R within RStudio to get the ‘whoa’ package from CRAN and then make a plot:\n\ninstall.packages(\"whoa\")\nlibrary(whoa)\nlibrary(vcfR)\n\n# you might have to change the file location, depending on where\n# files get downloaded to on your laptop\nv &lt;- read.vcfR(\"~/Downloads/raw-fall-late-fall-geno-calls.vcf.gz\")\n\ngfreqs &lt;- exp_and_obs_geno_freqs(v)\n\ngeno_freqs_scatter(gfreqs, alpha = 0.04, max_plot_loci = 1e15)\n\nWhoa!",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "nmfs-bioinf/variant-calling.html#gatks-gvcf-workflow",
    "href": "nmfs-bioinf/variant-calling.html#gatks-gvcf-workflow",
    "title": "14  Variant calling",
    "section": "14.2 GATK’s gVCF Workflow",
    "text": "14.2 GATK’s gVCF Workflow\nThe Genome Analysis Toolkit, or GATK, includes a program called HaplotypeCaller which is well respected for calling variants and computing genotype likelihoods from alignments (i.e., from BAM files).\nA key feature of HaplotypeCaller is a local reassembly step in which all the reads in a small “active” region of the genome are inspected and reassembled amongst themselves in a way that can provide a clearer picture of the presence of short structural variations like insertions and deletions (collectively called “indels,” for short). This local reassembly step helps to improve the HaplotypeCaller’s accuracy when calling indels.\nHere is a reminder of what insertions and deletions look like.\nAGTGGACCTGACTA  &lt;- Reference Genome\nAGTGG--CTGACTA  &lt;- Aligned read with a deletion\n\nAGTGGAC---CTGACTA  &lt;- Reference Genome expanded to show insertion site\nAGTGGACAAGCTGACTA  &lt;- Aligned read with an insertion\n\nBecause HaplotypeCaller has good performance for small structural variants, and because GATK tools are quite well supported (the Broad Institute has an active team of representatives and developers that reply to user issues quite quickly when there is a problem with their software), HaplotypeCaller, and other associated GATK tools, are frequently used in bioinformatics.\nWhen HaplotypeCaller first became available, a standard analysis with it involved running it with all the samples you had, together at the same time, so that variant calling could be done jointly, directly producing a VCF file for all the individuals. In other words, it was important to have all of your samples in a single run so that if HaplotypeCaller found a variant in one individual it could score that variant in all the other individuals of the sample. We will call this the “direct-to-VCF” mode of operation for HaplotypeCaller. In the early 2000s, when sequencing was considerably more costly than it is today, this was a reasonable strategy, because data sets at the time didn’t include a whole lot of individual samples. However, today, when hundreds of individuals can be sequenced throughout their genomes for only a few thousand dollars, the limitations of the direct-to-VCF approach have become more apparent.\nThe first problem is that run times for HaplotypeCaller in direct-to-VCF mode do not scale linearly with the number of samples. In other words, if instead of a sample of size 50 birds, you run HaplotypeCaller in direct-to-VCF mode on a sample of 200 birds, the program will not simply require 4 times more computation. Rather, it might require 8 times more computing time. As an example of this, I performed a small experiment with alignments from 240 Chinook salmon. I randomly sampled (without replacement) a subsample of size 4, 8, 16, 24, 36, 48, 72,…, 240 fish and ran a 100 Kb section of their alignments though HaplotypeCaller in direct-to-VCF mode using 4 cores. I repeated this with 10 different random subsamples for each of the sample sizes, and recorded how much CPU time HaplotypeCaller used in each case. The results are plotted in the figure below.\n\nThe blue line shows the average CPU time for the 10 replicates at each value of sample size. The blue dots show the CPU time for each individual replicate, and the dashed black line shows what we would expect the times to be—based on the time for 48 samples—if HaplotypeCaller’s run time increased linearly with the number of samples. The blue line clearly looks like a parabola, which means that as your sample sizes get larger and larger, the penalty that you pay having to load all of your samples into HaplotypeCaller at the same time increases faster than the number of samples—it looks like it increases quadratically.\nThe second problem with having to run HaplotypeCaller on all of your samples at the same time is encountered when you want to add more samples. If you started your sequencing project with 120 fish, and then after you had created a VCF file using HaplotypeCaller in direct-to-VCF mode, one of your collaborators surprises you with 20 more samples to add to the study, you would have to run all 140 fish, total, through HaplotypeCaller in direct-to-VCF mode, which is somewhat redundant, since you had already run 120 of them through it just a little while before. This problem is called the \\(N+1\\) problem by the GATK developers, presumably because it irked them to no end that they had to run HaplotypeCaller on \\(N+1\\) samples, just to add one more sample to a VCF file that already had \\(N\\) samples in it.\nTo solve both the quadratic HaplotypeCaller runtime problem and the \\(N+1\\) problem, the scientists and the developers at the Broad Institute developed what is called the “gVCF workflow.” Although there are several different flavors of this workflow, we will be covering the latest flavor (at the time of writing) which proceeds in these three steps:\n\nHaplotypeCaller is run, individually, on each single sample, creating a special type of VCF file called a gVCF file—one gVCF file for each individual sample. This gVCF file encodes information not just about sites that are clearly variable in the individual, but it also records how much evidence there is for the individual having a variable site in between the clearly variable sites.\nThe GATK tool GenomicsDBImport loads the gVCF files from all individuals into a special type of “genomics data base” that is optimized for retrieval of information across individuals at each possible location on each chromosome (or on a collection of scaffolds).\nThe GATK tool GenotypeGVCFs is then used to extract information from the genomics data base about all the individuals, at each position on a chromosome (or in a group of scaffolds), and this is used to jointly call the variants and genotypes of all the individuals, producing, in the end, a proper VCF file for each chromosome (or group of scaffolds).\n\nHaving these multiple steps involved in the process of creating a VCF allows for some flexibility in optimizing the desired properties of the workflow. Namely, in step 1, HaplotypeCaller is still used to do a local reassembly of all the reads within a single individual, enhancing the identification of indels, but, since that local realignment is not done over all individuals at once, it doesn’t suffer quadratic scaling with number of samples. Further, Step 3 still gets to use information from all the individuals jointly when calling variants, but it is done within the context of an efficient data base structure that allows that joint process to scale linearly in the number of samples. The hope is that this gVCF workflow provides the same accuracy as joint calling all samples simultaneously in a single run of HaplotypeCaller, while being much faster for larger numbers of samples. Details can be found in @poplin2017scaling.\nAdditionally, by saving the gVCF files produced for each individual, or, even better, the genomic databases including all the individuals, adding additional samples can be done relatively quickly by updating the genomic database with information from the new individuals. Thus, this workflow improves upon the \\(N+1\\) problem, being able to use many of the previously computed results (the gVCF files or the genomic data base) from the first \\(N\\) individuals.\nHere, it is worthwhile to reflect upon how this gVCF workflow allows the end goal of obtaining a single VCF file with all your variants and all your samples to be broken down into separate, independent tasks (as this is a huge theme in bioinformatics). The direct-to-VCF workflow could be parallelized by breaking the genome into a lot of small regions and then running HaplotypeCaller on all the samples across just one small region at a time. The gVCF workflow, on the other hand, lends itself to parallelizing over individuals in step 1 (the HaplotypeCaller step) and then parallelizing steps 2 and 3 (genomics data base importing and genotyping) over regions of the genome. The regions focused upon for the genomics data base importing step are typically chromosomes, but you can also, as we shall see, combine many small scaffolds together into a single genomics data base. Step 3 can be further broken down over sub-regions of chromosomes, if desired.\n\n14.2.1 Step 1. Running HaplotypeCaller and getting one gVCF for each sample\nWe will discuss this in the context or our example workflow in Section @ref(make-gvcfs).\n\n\n14.2.2 gVCF files\nSo, you may be wondering what the difference between a VCF file a gVCF file is. It turns out that a gVCF is a special type of VCF file. (One of the nice things about the way VCF files [and SAM files, for that matter], is that the definition of various fields in the file can be made in the header, or in the FORMAT column, which makes it fairly easy to extend the files to novel situations in a format-compliant way.)\nA comprehensive overview of gVCF files can be found at here, and it is recommended reading. In this section we just summarize the information in that document.\nLet’s start by looking at the first few lines for a chromosome at an individual with low fairly low read depth. We see a lot of lines that look like this:\n#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  s001\nCM031199.1      1       .       A       &lt;NON_REF&gt;       .       .       END=71  GT:DP:GQ:MIN_DP:PL      0/0:1:3:1:0,3,15\nCM031199.1      72      .       T       &lt;NON_REF&gt;       .       .       END=72  GT:DP:GQ:MIN_DP:PL      0/0:1:0:1:0,0,0\nCM031199.1      73      .       A       &lt;NON_REF&gt;       .       .       END=110 GT:DP:GQ:MIN_DP:PL      0/0:1:3:1:0,3,15\nCM031199.1      111     .       C       &lt;NON_REF&gt;       .       .       END=138 GT:DP:GQ:MIN_DP:PL      0/0:1:0:1:0,0,0\nCM031199.1      139     .       T       &lt;NON_REF&gt;       .       .       END=139 GT:DP:GQ:MIN_DP:PL      0/0:2:3:2:0,3,45\nCM031199.1      140     .       A       &lt;NON_REF&gt;       .       .       END=141 GT:DP:GQ:MIN_DP:PL      0/0:2:0:2:0,0,0\nCM031199.1      142     .       T       &lt;NON_REF&gt;       .       .       END=144 GT:DP:GQ:MIN_DP:PL      0/0:2:3:2:0,3,45\nCM031199.1      145     .       A       &lt;NON_REF&gt;       .       .       END=146 GT:DP:GQ:MIN_DP:PL      0/0:2:0:2:0,0,0\nCM031199.1      147     .       G       &lt;NON_REF&gt;       .       .       END=147 GT:DP:GQ:MIN_DP:PL      0/0:2:3:2:0,3,45\nCM031199.1      148     .       G       &lt;NON_REF&gt;       .       .       END=149 GT:DP:GQ:MIN_DP:PL      0/0:2:0:2:0,0,0\nCM031199.1      150     .       A       &lt;NON_REF&gt;       .       .       END=151 GT:DP:GQ:MIN_DP:PL      0/0:2:3:2:0,3,45\nCM031199.1      152     .       A       &lt;NON_REF&gt;       .       .       END=155 GT:DP:GQ:MIN_DP:PL      0/0:2:0:2:0,0,0\nCM031199.1      156     .       A       &lt;NON_REF&gt;       .       .       END=156 GT:DP:GQ:MIN_DP:PL      0/0:2:3:2:0,3,45\nCM031199.1      157     .       G       &lt;NON_REF&gt;       .       .       END=159 GT:DP:GQ:MIN_DP:PL      0/0:2:0:2:0,0,0\nCM031199.1      160     .       G       &lt;NON_REF&gt;       .       .       END=160 GT:DP:GQ:MIN_DP:PL      0/0:2:3:2:0,3,45\nAha! This is interesting. The gVCF file has records for sites that are not confident variants. Interesting. Each of the rows here gives an indication of how much information the reads from the individual give us about whether there might be a variant site at the individual.\nBreaking it down, starting with the first row, we see that the ALT allele is listed as &lt;NON_REF&gt;. This is simply a code that means “Not the reference allele.” In other words, it is saying, “There might be a variant here, and if there is, then the alternate allele would be something that is not the reference allele.”\nThe other interesting thing on that line is the END=71 in the INFO field. That is telling us that all of the sites from position 1 (the value in the POS field) to position 71 are described by this single row. Using this type of mechanism, it is possible to describe every position in the genome without having a single row for each position. In other words, this system provides some compression.\nIf we look at the genotype column in the first row, we see 0/0:1:3:1:0,3,15. The fields between the colons in that are described by the value in the FORMAT column: GT:DP:GQ:MIN_DP:PL. The GQ field (the third one) is 3. That is a Phred scaled probability that the genotype call (0/0 or “reference homozygous”) is incorrect in this case. So, this is saying there is a \\(1 / 10^{3/10} \\approx 0.5\\) chance that this individual is not homozygous for the reference allele at these sites (from POS = 1 to 71). This is not terribly surprising, since the read depth (in the DP field) at these sites is 1.\nIf we go deeper in the file, we find sites with higher read depth, and hence more confidence that the individual is homozygous for the reference allele. For example:\nCM031199.1      1951    .       C       &lt;NON_REF&gt;       .       .       END=1964        GT:DP:GQ:MIN_DP:PL      0/0:8:21:7:0,21,224\nHere, the sites from 1951 to 1964 all have a genotype quality of 21, indicating a chance of \\(1/10^{21/10} \\approx 0.0079\\) that the individual is not homozygous for the reference allele at these sites.\nOn the other hand, when we come to a site where it is pretty clear from the information in a single individual that there is a variant, like:\nCM031199.1      1903    .       G       T,&lt;NON_REF&gt;     210.02  .       DP=7;ExcessHet=0.0000;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQandDP=25200,7     GT:AD:DP:GQ:PL:SB   1/1:0,7,0:7:21:224,21,0,224,21,224:0,0,4,3\nthat information is recorded. At the site above (POS = 1903), this is saying that there is at least one alternate allele, which is a T, and the gVCF format leaves open the possibility that there is another alternate allele (coded as &lt;NON_REF&gt;) so that the possibility of that can also be computed and recorded. In this case, the individual has 7 reads total, and they are all for the T allele, which leaves us with a high genotype quality (21). The genotype likelihoods for all the different possible genotypes, ordered as:\nGG,GT,TT,GN,TN,NN\nwhere N means &lt;NON_REF&gt; are also recorded in the PL field that looks like:\n21:224,21,0,224,21,224\nThis turns out to be all the information needed to eventually come back to this information in light of what was found at all the other individuals, and then jointly call variants using information from all the individuals.\n\n\n14.2.3 The Genomics Data Base\nThe use of a genomics data base in order to speed up joint calling of variants and genotypes across all samples is still fairly new and seems to still be evolving, somewhat, as a part of the GATK ecosystem.\nOne thing that is worth understanding is “Why do we have to load all this stuff into a database? Why can’t we just zip through all the gVCF files to do joint variant calling?” The answer to that question is that it is fairly inefficient to have a program open multiple files (for example 100s of gVCF files) and then inch one’s way through each of them, constantly stopping and circling back when a variant position is found in one of the files which must be inspected in all the other files.\nThe old way of solving this problem was to combine all the gVCF files (one for each sample) into a single gVCF file that included all the individuals—one column for each. This can be done with GATK’s CombineGVCFs tool. That tool still sees some use in situations where you might only want to focus on a few intervals on different chromosomes that you know of ahead of time. However, when you want to call variants across the whole genome, it is preferable to load the information in the gVCFs into a specialized data base that makes it quick and efficient to garner information at a single position in the genome across all the individuals.\nThis is achieved in GATK by using a specialized variant of the TileDB system. TileDB is a data base that was designed to work well with sparse data—meaning that there are lots of possible values that the data can be associated with, but only a small number of those values have different values for the data. In this case, those values are the base positions in the genome, and the data are information from the different individuals (available in the gVCFs for each sample) about whether there is a variant at that position or not. One of the nice features of the TileDB system is that it is computationally economical to update the data base with new information, even if it is necessary to assign new information to a base position that is between two base positions that already have information, and that information can still be accessed quickly. This structure, once it is set up, makes it efficient to work one’s way through the genome base by base and combine information about possible variation from all the individuals, without having to laboriously go through each separate file.\nThe drawback of using this genomics data base is that it is something of a black box: it’s a little mysterious and it is hard to look at anything inside of it unless you are a computer scientist that specializes in these types of data bases—in fact, the GenomicsDB format and associated tools are the product of a collaboration between the computer scientists at Intel and the bioinformaticians at the Broad Institute.\nAs of this writing, there are two important documents on the GATK website for understanding how to use the GenomicsDBImport tool to import gVCF files into this genomics data base. The first is here and the second one, that goes more into performance considerations is here.\nFrom the perspective of conservations genomicists who are working on non-model organisms, the most important thing to understand is that GenomicsDBImport is designed parallelize well by working on a single chromosome at a time—so you would typically launch multiple jobs, one for each chromosome, specified with the --interval option. For example using --interval CM031199.1 would make GenomicsDBImport create a data base that included only the chromosome CM031199.1. In the end, after doing that you get a completely separate data base (that occupies its own separate directory) for each chromosome. This is all fine and well if you are using a well-assembled human genome with all the DNA in chromosomes. However, in non-model organisms, an appreciable fraction of the reference genome might be recorded in thousands of short scaffolds. There is so much overhead involved in setting up these genomic data bases, that it does not make sense to set up a separate data base for each of the thousands of scaffolds that might only be 5 to 10 kb long (or even shorter!). Fortunately, there is an option to mash all these short scaffolds together into a single data base that treats all these scaffolds catenated together as if they were a single chromosome. This uses the --merge-contigs-into-num-partitions option.\nMy approach to using the --merge-contigs-into-num-partitions is to break the scaffolds down into scaffold_groups such that each of those scaffold groups has total length (the sum of the lengths of the scaffolds) is about 1/3 to 1/2 the average length of a chromosome. Then each separate scaffold group can be fed into GenomicsDBImport as a separate job, using --merge-contigs-into-num-partitions 1 to make sure that all of those scaffolds get merged into a single “pseudo-chromosome”. After these data bases are made, when you use them to call variants and create VCF files, they will produce a sinble VCF file that properly indicates which scaffold each variant was from—it doesn’t rename the scaffolds with some sort of pseudo-chromosome name.\nThe other particularly germane option to GenomicsDBImport is --genomicsdb-shared-posixfs-optimizations. This is recommended any time that your computer uses a shared POSIX filesystem, which, I believe, is going to almost always be the case when you are using an HPCC. So, if you are working on a cluster, you should always use this option.\nIf you have hundreds of samples, GenomicsDBImport can start to bog down if it opens connections to hundreds of gVCF files at the same time. In this case it is recommended that you tell GenomicsDBImport to import the gVCF files in batches of 50 using the option --batch-size 50. This imports the first 50 samples, then closes all their files, and then updates the data base by importing the next 50, and so on.\nThe program GenomicsDBImport can use multiple threads while reading the files; however, in my experience, there is little advantage above two threads. So, it can be useful to set the option --reader-threads 2.\nBy default, GenomicsDBImport will\nFinally, since this is a GATK program written in Java, we can allocate memory to the Java Virtual Machine. However, with GenomicsDBImport we have to be a little careful, because much of the work in loading the data base is done using calls to compiled C++ libraries from the Java program. These libraries, themselves, can consume memory as well. So, you want to be careful not to give more than 80% or so of the available memory (i.e., the memory that you reqested from SLURM) to the Java Virtual Machine.\nWe can bring all of these together by running GenomicsDBImport in our example workflow. See Section @ref(use-genomicsdbimport).\n\n\n14.2.4 Updating the GenomicsDatabase\nOne of the advantages of using a sparse data base format like TileDB is that it is possible to expand the data base without completely rearranging the existing content and without creating a new data base in which access to all the elements has been slowed considerably.\nIn fact, this is one of the big selling points of the GenomicsDB approach, especially as concerns the “\\(N+1\\)” problem: if you have kept your genomics data bases, then adding one or a few individuals to them can be quite quick. Of course, as we will see, the following step of creating a VCF file from the genomics data base still has to be done, and that can take a long time; however, at least when everything is stored in the genomics data base, that step should scale almost linearly with the number of samples, rather than quadratically (as we say above with the “direct-to-vcf” approach).\nIn principle, updating a GenomicsDB by adding information from new samples should be as easy as running GenomicsDBImport with a --genomicsdb-update-workspace-path path_to_dir option rather than the --genomicsdb-workspace-path path_do_dir option (where path_to_dir is simply the path to the existing genomics database directory, in the first case, and it is the path where you want the genomics database directory to be created, in the second case). Interestingly, when you run GenomicsDBImport to update an existing data base, you don’t supply the “–intervals` option, because the program will simply use the contents of the existing data base (that is being updated) to figure out which genomic intervals (i.e., which chromosome or which scaffold group) to import. This makes sense: you wouldn’t want to import the new samples at a different interval within the genomics data base—that simply wouldn’t work.\nOne thing that is important to note about GenomicsDBImport, as it stands today, is that you can only ever import a sample into it once. Thus, if, after creating a genomics data base with data from 50 individuals, you decide to do more sequencing on 10 of those individuals that didn’t have enough sequence depth, then you can’t just make new VCFs for those 10 individuals from the new sequencing and add it into the data base. Rather, you would have to merge the new data at the BAM file stage and then make new gVCFs and then import those 10 new gVCFs and the 40 existing ones into a new genomics data base.\nFinally, a word of caution: the documentation for GenomicsDBImport is replete with warnings about how you should backup your genomics data bases before trying to update them. Apparently there is a good chance that any little glitch during the updating process could corrupt the entire data base and render it useless. Joy!\nWith that as a preamble, it does seem that we should try creating some data bases and then adding samples to them.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  }
]
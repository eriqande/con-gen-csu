[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "con-gen-csu",
    "section": "",
    "text": "Welcome!\nWelcome to the website for the National Marine Fisheries Service Linux, Slurm, and Bioinformatics training to be held virtually over three days:"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "con-gen-csu",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course will be using the Sedna high-performance computing cluster located at the Northwest Fisheries Science Center. This cluster (and, hence, this course) is only available to NMFS employees and affiliates. If you are a NMFS employee and you are interested in this course, please see here for information about how to get an account on the cluster.\nThis course is intended for people who have already had some exposure to Unix or Linux. You should be reasonably comfortable navigating around the Unix filesystem using the command line. For a refresher, please read this chapter from my online bioinformatics book.\nMy goal is to:\n\nteach the shell programming constructs and the text processing tricks that I find myself using all the time in my day-to-day work\nprovide an introduction to how to use SLURM to do cluster computing\nOn the last day, show how Snakemake works, and how it can be used on the Sedna cluster to simplify your bioinformatics life."
  },
  {
    "objectID": "index.html#course-topics-and-sessions",
    "href": "index.html#course-topics-and-sessions",
    "title": "con-gen-csu",
    "section": "Course Topics and Sessions",
    "text": "Course Topics and Sessions\n\nDay 1: Intro, Unix-review, shell programming, awk\n\nIntroduction to the Sedna cluster (15 minutes Krista and Giles)\n\nCluster infrastructure and configuration.\nScientific software and the installation requests\n\nQuick Unix Review (25 minutes)\nShell Programming (50 Minutes)\nA Brief awk Intro Processing text files with awk (30 minutes)\n\nDay 2: A little bash stuff, then Sedna and SLURM\n\nBash scripts and functions (20 minutes)\nSedna and SLURM intro (40 minutes)\nSubmitting jobs with sbatch (40 minutes)\nSlurm Job Arrays (40 minutes)\n\nDay 3: Job Arrays, then an introduction to Snakemake\n\nSlurm Job Arrays (25 minutes)\nSnakemake Tutorial Introduction (90 minutes)"
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#what-is-bash",
    "href": "nmfs-bioinf/quick-unix-review.html#what-is-bash",
    "title": "1  Quick Unix Review",
    "section": "1.1 What is bash?",
    "text": "1.1 What is bash?\nWe start by acknowledging that there are many different flavors of Unix and Linux. I will refer to them all simply as Unix or unix.\nAlso, there are a number of different shells for Unix. The shell is the part that interprets commands.\nWe will be talking about the bash shell. This is the default shell on Sedna, and it is also the most popular shell for bioinformatics.\nBash stands for “Bourne-again shell”. It is an update to an earlier shell called the Bourne shell."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#setting-up-our-workspace",
    "href": "nmfs-bioinf/quick-unix-review.html#setting-up-our-workspace",
    "title": "1  Quick Unix Review",
    "section": "1.2 Setting up our workspace",
    "text": "1.2 Setting up our workspace\n\nI have prepared a repository with a few different example data files that we be using.\nIt also contains all these notes.\nI want everyone to download it to their home directory and then cd into its playground directory, where we will be playing today and tomorrow.\n\nAfter logging onto Sedna:\n\n\nPaste this into your shell\n\ncd ~\ngit clone https://github.com/eriqande/nmfs-bioinf-2022.git\ncd nmfs-bioinf-2022/playground\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen we say “Paste this into your shell” or “Type this at your command prompt” we also implicitly mean “Hit RETURN afterward.”\n\n\n\nThis is where our working directory will be for the next two days.\nUse the tree utility to see the files that we have to play with within this playground:\n\n\n\nType this command at your prompt\n\ntree\n\nThe data directory has a few things that we will be using for examples.\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ncd: change directories\ngit: run git subcommands, like clone with this. In the above case it clones the repository that is found at the GitHub URL.\ntree: Super cool “text-graphical” directory listing"
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#a-motivating-example",
    "href": "nmfs-bioinf/quick-unix-review.html#a-motivating-example",
    "title": "1  Quick Unix Review",
    "section": "1.3 A motivating example",
    "text": "1.3 A motivating example\n\nThe data/samtools_stats directory has gzipped output from running the samtools stats program on 30 different samples.\nThis provides information about reads that have been mapped to a reference genome in a BAM file.\n\nTo see what those files look like:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\n\n\nHit the SPACE-bar to go down a screenful, the b key to go back up\nMost terminal emulators let you use up-arrow and down-arrow to go one line at a time, too.\nHit the q key to quit out of the less viewer.\n\nTo see it without lines wrapping all over the place try this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\n\nNow you can use the left and right arrows to see different parts of lines that are not wrapped on the screen.\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ngzip -cd: decompress gzipped file to stdout (this is what zcat does, but zcat is not portable).\nless: page view. Great to pipe output into. (SPACE-bar, b, q, down-arrow, up-arrow)\n\nless -S: option to not wrap lines. (left-arrow, right-arrow)\n\n\n\n\n\n\n1.3.1 (One of) Our Missions…\nIt is pretty typical that Bioinformatic outputs will be spread as small bits of information across multiple files.\nOne motivating example is summarizing the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped, in all 30 samples, in a table."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#the-anatomy-of-a-unix-command",
    "href": "nmfs-bioinf/quick-unix-review.html#the-anatomy-of-a-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.4 The anatomy of a Unix command",
    "text": "1.4 The anatomy of a Unix command\nNearly every line in a bash script, or every line you type when banging away at the Unix terminal is a command that has this structure:\ncommand options arguments\n\nThe command is the name of the command itself (like cd or less).\nThe options are often given:\n\nwith a dash plus a single character, like -l or -S or -a, -v, -z.\n\nIn most commands that are part of Unix, these single options can be combined, so -cd, is the same as -c -d.\n\nwith two dashes and a word, like --long or --version\nSometimes options take arguments, like --cores 20, but sometimes, they stand alone.\n\nWhen they stand alone they are sometimes called flags.\n\n\nThe arguments are typically file or paths.\n\n\n\n\n\n\n\nSelf-study question\n\n\n\nIdentify the command, options, and arguments in:\ntree -d ..\n\n# and\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz\n\n\n\n\n\n\n\n\nSelf-study answer\n\n\n\n\n\nFirst case:\n\ntree is the command\n-d is the option (print only directory names, not files)\n.. is the argument (one directory level up)\n\nSecond case:\n\ngzip is the command\nThe options are -c and -d, contracted into -cd\ndata/samtools_stats/s001_stats.tsv.gz is the argument\n\n\n\n\n\n1.4.1 What do all these options mean?\nEverything you need to know about any Unix command will typically be found with the man command. For example:\n\n\ntype this at your terminal\n\nman tree\n\n\nThat gives you more information than you will ever want to know.\nIt starts with a synopsis of the syntax, which can feel very intimidating.\n\n\n\n\n\n\n\nBonus Tips:\n\n\n\n\n\n\nMan uses the less viewer for presenting contents of the man pages.\nWhen you are viewing man pages, you can scroll down with SPACE-bar and up with b, and get out with q, just like in less\nTo search for patterns in the manual pages, you can type / then the string you want and then RETURN.\n\nWhen in pattern-searching mode, use n to go to the next occurrence, and N to the previous.\nIf searching for a single letter option try searching with [, ] afterward.\nFor example, to search for the -d flag you would type /, then -d[, ], then hit RETURN. Try it on the tree man page.\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nLearn about gzip\n\nUse man to read information about the gzip command\nFind information about the -c and the -d options.\n\nMaybe even search for those using the “slash-pattern” bonus tip from above.\n\n\nLearn about the ls command\n\nUse man to see information about the ls command, which lists directories and their contents\nFind out what the -R option does. Maybe even look for it using the Bonus Tip above.\nDo the same for the -Q option.\nLook at what those do by doing ls -RQ on your terminal.\n\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\n\n\nman gzip\nTo search for -c, type /-c + return. You might have better results with /-c[, ].\n\nuse n or N to go forward or backward through the occurrences of -c.\n\n\n\nYou would do man ls\nTo search for -R in the man pages, a good way to do it would be to type /-Q + RETURN, or maybe /-Q[, ] + RETURN."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#streams-and-redirection",
    "href": "nmfs-bioinf/quick-unix-review.html#streams-and-redirection",
    "title": "1  Quick Unix Review",
    "section": "1.5 Streams and redirection",
    "text": "1.5 Streams and redirection\n\nWhen you’ve executed the unix commands above, they have typically responded by writing text or data to the terminal screen.\nThe command is actually writing to a stream that is called stdout, which is short for “standard output.”\nIt turns out that, by default, the stdout stream gets written to the terminal.\n\nAha! But here is where it gets fun:\n\nYou can redirect the stdout stream to a file by using &gt; or &gt;&gt; after the command, options, and arguments.\n\nFor example:\n\n\nPaste this into your terminal\n\nmkdir outputs\ntree -d .. &gt; outputs/repo-tree.txt\n\nNow, you can use the less viewer to see what got into the file outputs/repo-tree.txt:\n\n\nType this at the terminal\n\nless outputs/repo-tree.txt\n\nAha! Instead of writing the output to the screen, it just puts it in the file outputs/repo-tree.txt, as we told it to.\n\n\n\n\n\n\nDanger!\n\n\n\nIf you redirect stdout into a file that already exists, the contents of that file will get erased!!!\nFor example, if you now do:\n\n\nPaste this into the shell\n\necho \"New content coming through...\" &gt; outputs/repo-tree.txt\n\nThen you will no longer have the output of the tree command in the file outputs/repo-tree.txt. Check it out with the less command.\n\n\nIf you want to merely append stdout to an existing file, you can use &gt;&gt;. For example:\n\n\nPaste this into your terminal\n\necho \"Add this line\" &gt;&gt; outputs/repo-tree.txt\necho \"And then add another line\" &gt;&gt; outputs/repo-tree.txt\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\nmkdir: make a new directory\n\n(check out the -p option, which means “make any necessary parent directories and don’t complain if the directory already exists.”)\n\necho: print the argument (usually a string) to stdout."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "href": "nmfs-bioinf/quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.6 Pipes: redirecting into another Unix command",
    "text": "1.6 Pipes: redirecting into another Unix command\nAs we have said, many Unix utilities take files as their arguments, and they operate on the contents of that file. They can also receive input from streams, and almost all Unix utilities are set up to accept input from the stream called stdin, which is short for standard input.\n\nThe most important way to pass the stdin stream to a Unix command is by piping the stdout from one command in as the stdin to the next command.\nThis uses the | which is called the “pipe”.\n\nWe have already used the pipe when we did:\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\nPipe syntax is pretty simple:\ncommand1 | command2\nmeans pipe the stdout output of command1 in as stdin input for command2."
  },
  {
    "objectID": "nmfs-bioinf/quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "href": "nmfs-bioinf/quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "title": "1  Quick Unix Review",
    "section": "1.7 stderr: The stream Unix uses to yell at you",
    "text": "1.7 stderr: The stream Unix uses to yell at you\n\nIf a Unix command fails, typically the program/command will bark at you to tell you why it failed. This can be very useful.\nThe stream it writes this information to is called stderr, which is short for standard error.\nSome bioinformatics programs write progess and log output to stderr, in addition to actual error messages.\n\nIf you are running a program non-interactively, it is extremely valuable and important to redirect stderr to a file, so you can come back later to see what went wrong, if your job failed.\n\nstderr is redirected with 2&gt;.\nThink of the 2 as meaning that stderr is the second-most important stream, after stdout.\n\n\n\n\n\n\n\nBonus side comment:\n\n\n\n\n\nAs you might imagine, you could redirect stdout by using 1&gt; instead of &gt;, since stdout is stream #1.\n\n\n\nFor example, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. &gt; outputs/repo-tree.txt\n\n\nAha! We get a warning note printed on the screen,\nBecause, stderr gets printed to the terminal by default.\nAlso outputs/repo-tree.txt has been overwritten and is now a file with nothing in it.\n\nSo, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. &gt; outputs/repo-tree.txt 2&gt;outputs/error.txt\n\nNow, look at the contents of both outputs/error.txt and outputs/repo-tree.txt:\n\n\nPaste this into your shell\n\nhead outputs/repo-tree.txt outputs/error.txt\n\n\n\n\n\n\n\nStream operators and commands that we just saw:\n\n\n\n\n\n\n&gt; path/to/file: redirect stdout to file at path/to/file. This overwrites any file already at path/to/file.\n&gt;&gt; path/to/file: redirect stdout to append to file at path/to/file. If path/to/file does not exist, it creates it and then adds the contents of stdout to it.\n2&gt; path/to/file: redirect stderr to the file at path/to/file.\n|: the uber-useful Unix pipe. (Just as an aside, when R finally got a similar construct—the %&gt;% from the ‘magrittr’ package—it became much easier for Unixy people to enjoy coding in R).\nhead: print the first ten lines of a file to stdout. If multiple file arguments are given, they are separated by little ==&gt; filename &lt;== lines, which is super convenient if you want to look at the top of a lot of files.\n\nhead -n XX: print the first XX lines (instead of 10).\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nDecompress data/samtools_stats/s001_stats.tsv.gz onto stdout using the gzip -cd command and pipe the output into wc to count how many lines words, and characters are in the file.\nDo the same that you did above, but redirect the stdout to a file so.txt and stderr to a file se.txt in the current working directory.\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nThese could be done like this:\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc &gt; so.txt 2&gt;se.txt\n\nAs an interesting side note, this will only redirect stderr for the wc command into se.txt. If the first command fails, its stderr will to to the screen. Try this:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc &gt; so.txt 2&gt;se.txt\n\nAn interesting fact is that you can redirect stderr from the first command before the pipe. So, to redirect stderr for the gzip command into a file called ze.txt, we could do:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz 2&gt;ze.txt | wc &gt; so.txt 2&gt;se.txt\n\nHave a look at the contents of ze.txt.\n\n\n\n\n\n\n\n\n\nPro-tip: Redirect stdout and stderr to the same place\n\n\n\n\n\nThe astute reader might note that if you redirect stdout to a file, and then redirect stderr to the same file, you might end up overwriting the contents of stdout with stderr.\nIf you want to redirect stdout and stderr to the same place then you first redirect stdout to a file, and then after that, you say “redirect stderr to wherever stdout has been redirected to,” by using 2&gt;&1.\nSo it looks like this:\ncommand &gt; file.out 2&gt;&1"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variables",
    "href": "nmfs-bioinf/shell-prog.html#variables",
    "title": "2  Shell Programming",
    "section": "2.1 Variables",
    "text": "2.1 Variables\nThe bash shell can store strings into variables. These variables can be accessed later.\n\n2.1.1 Assigning variable values\n\nbash is remarkably picky about how to assign a value to a variable.\nUse the equals sign with no spaces around it!\n\nHere, we can assign a value to a variable called PHONE:\n\n\nPaste this into your shell\n\nPHONE=974-222-4444\n\nIn this case, the digits and dashes 974-222-4444 are treated as just a string of characters and assigned to the variable PHONE.\n\n\n\n\n\n\nLet’s make some mistakes\n\n\n\nPaste these commands into the shell and see what happens:\n\n\nPaste these mistakes into the shell and think about what is happening.\n\nPHONE = 974-222-4444\nPHONE =974-222-4444\nPHONE= 974-222-4444\n\nWhat does this tell us about how bash interprets commands with an equals sign?\n\n\n\n\n2.1.2 Accessing variable values\n\nThe process of accessing the values stored in the variable is called “variable substitution.\nIt means: “Substitute the value for the variable where it appears on the command line.”\nIn many programming languages, you can just write a variable’s name and know that its value will be accessed, like in R:\n\nVariable &lt;- 16\nsqrt(Variable)\n\nHowever, in bash, variable substitution is achieved by prepending $ to the variable’s name.\n\nWitness:\n\n\nPaste this into your shell\n\necho The value of PHONE is: $PHONE\n\nCool!\n\nRemember: if you make a substitution to the menu at a fancy restaurant, it is going to cost you some dollars. Same way when you make a variable substitution in bash: it costs you a dollar sign and you have to pay up front.\n\n\n\n2.1.3 Valid bash variable names\nThe bash shell demands that the names of variables:\n\nStart with _ (an underscore) or a letter\nThen include only _, letters, or numbers\n\n# good variable names\nMY_JOBS\n_Now\nSTRING\ni\ni_2\ni2\n\n# cannot be variable names\n1_node\n4THIS\nBIG-VAR\nfile-name\nSh**t!\n\n\n\n\n\n\nSelf-study\n\n\n\nChoose one of the good variable names from the list above and assign the value Good to it.\nChoose of the bad variable names from the list above and try to assign the value Bad to it.\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nHere is an example. {.sh filename = \"This works\"} _Now=Good\nHere is an example. {.sh filename = \"This does not work\"} BIG-VAR=Bad"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#strings-with-spaces-etc-quoting.",
    "href": "nmfs-bioinf/shell-prog.html#strings-with-spaces-etc-quoting.",
    "title": "2  Shell Programming",
    "section": "2.2 Strings with spaces, etc: Quoting.",
    "text": "2.2 Strings with spaces, etc: Quoting.\n\nIf you want to assign a string to a variable that has spaces in it you can quote the string, which holds it together as one “unit.”\n\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWhen the bash shell is interpreting a line of input, it breaks it into chunks called tokens which are separated by white space (spaces and TABS). If you wrap a series of words in quotation marks, it turns them all into a single token.\n\n\n\nFor example:\n\n\nPaste this into your shell\n\nMandela_Quote=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\necho $Mandela_Quote"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variable-substitution-and-vs",
    "href": "nmfs-bioinf/shell-prog.html#variable-substitution-and-vs",
    "title": "2  Shell Programming",
    "section": "2.3 Variable substitution and \" vs '",
    "text": "2.3 Variable substitution and \" vs '\nWe have two types of quotes:\n\nsingle quotes, like '\ndouble quotes, like \"\n\nThey both chunk their contents into a single unit, but they behave very differently with respect to variable substition.\n\nSingle Quotes: surly and strict, you can’t substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho 'Dessert tonight is $DESSERT'\n\n\nDouble Quotes: soft and friendly, you CAN substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho \"Dessert tonight is $DESSERT\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nAssign values to three shell variables, NAME, FOOD, and ACTIVITY, so that when you run the following command, it makes sense:\n\n\nAfter assigning values to the three variables, run this command\n\necho \"My name is $NAME. I like to eat $FOOD, and I enjoy $ACTIVITY.\" \n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nFor my case, I could put:\nNAME=Eric\nFOOD=\"steamed broccoli\"\nACTIVITY=\"inline skating long distances\""
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#variables-are-not-just-to-be-echoed",
    "href": "nmfs-bioinf/shell-prog.html#variables-are-not-just-to-be-echoed",
    "title": "2  Shell Programming",
    "section": "2.4 Variables are not just to be echoed!",
    "text": "2.4 Variables are not just to be echoed!\nInvariably, when learning how to use shell variables, all the examples have you using echo to print the value of the variable. How boring and misleading.\nIt is important to understand that after a value gets substituted onto the command line, the shell goes right ahead and evaluates the resulting command line.\nSo, you can record shell variables that are command lines that do something, themselves, once they are run as a command line.\nFor example, here we make a variable whose value is the command to decompress a file to stdout:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz\"\n\nAnd now if you just substitute that variable onto the command line\n\n\nType this at the command line\n\n$MyComm\n\nthe uncompressed contents of the file data/samtools_stats/s001_stats.tsv.gz go zooming by on your screen.\n\n2.4.1 Some subtlety about evaluation of substituted values\nIf the value of the variable that is being evaluated includes pipes, redirections, or variable assignment statements, then if you just substitute it into the command line, it won’t properly be evaluated as a command line in full. For example, if MyComm was trying to decompress the file and pipe it to less, it doesn’t work as expected:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\"\n\nAnd now if you just substitute that variable onto the command line the shell gets confused, because it doesn’t recognize the pipe as a pipe!\n\n\nType this at the command line\n\n$MyComm\n\nHowever, you can use the eval keyword before $MyComm to ensure that the shell recognizes that you intend for it to evaluate pipes, redirects, shell variable assignment, etc. in the substituted variable value as it normally would:\n\n\nType this at the command line\n\neval $MyComm\n\nWe will end up using this later.\nRemember, you can hit q to get out of the less page viewer."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#multiple-commands-on-one-line-with",
    "href": "nmfs-bioinf/shell-prog.html#multiple-commands-on-one-line-with",
    "title": "2  Shell Programming",
    "section": "2.5 Multiple commands on one line with ;",
    "text": "2.5 Multiple commands on one line with ;\n\nYou can put a ; after a command, and it will behave like a line ending—the shell will run that command, and then go to the next.\n\nExample:\necho \"Let us do this command and 2 others\"; echo \"here is number 2\"; echo \"and the third\"\nThis comes in handy.\n\n\n\n\n\n\nMore about line endings like ;\n\n\n\nThere are two other things you might find at the end of a line: & and &&\n\n& at the end of the line means “run the command, but don’t wait for it to finish.\n\nThis runs the command “in the background” in some sense.\nThis is not used very often when doing bioinformatics in a SLURM-driving system like that on Sedna\n\n&& at the end of a command means “only run the next command if the previous one did not fail.\n\nThis is very useful for making sure that you don’t keep running later commands if an earlier one failed.\nThere is also a || that is useful in this context, which is all about “exit status” of Unix commands, which is beyond our purview today.\n\n\nExamples:\n\n\nWith just semicolons...Paste it into your shell\n\necho \"Yawp before it fails.\"; ls --not-option data ; echo \"Yawp after it fails.\"\n\n\n\nWith the &&'s after each line...Paste it into your shell\n\necho \"Yawp before it fails.\" && ls --not-option data && echo \"Yawp after it fails.\""
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#repetition",
    "href": "nmfs-bioinf/shell-prog.html#repetition",
    "title": "2  Shell Programming",
    "section": "2.6 Repetition",
    "text": "2.6 Repetition\nLet’s face it, bioinformatics, or any sort of data analysis or processing often involves doing the same thing to a number of different inputs.\nMost unix utilities are designed so that if you give it multiple inputs it will do the same thing to each and report the results in a way that is easy to understand.\nFor example, to see how many lines, words, and characters are in each Quarto (the successor to RMarkdown) document that I used to make this website, we can use wc on all the files with a .qmd extension that are one directory level above where we are currently:\n\n\nPaste this into your shell\n\nwc ../*.qmd\n\nThis is nice."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#for-loops",
    "href": "nmfs-bioinf/shell-prog.html#for-loops",
    "title": "2  Shell Programming",
    "section": "2.7 For loops",
    "text": "2.7 For loops\nSometimes, however, we have more complex operations to do, so we can’t just provide multiple files to a single Unix utility.\nFor example, let’s say we want to know how many lines are in each of the samtools stats files in the data/samtools_stats directory. We can’t use wc directly, because these files are gzipped, and the result we get won’t be equal to the number of lines, words, and characters in each file.\nFor repetition in these cases, bash has a for loop. Its syntax looks like this:\nfor VAR in thing1 thing2 ... thingN; do\n  one or more commands where the value of VAR is set to each of the N things in turn\ndone\nThe important “structural” parts of that are:\n\nthe for\nthe in\nthe semicolon after all the things\nthe do\nthe done\n\nHere is an example:\n\n\nPaste this into your terminal\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do \n  echo \"I like $LIKE.\"\ndone\n\nNote that this is written over multiple lines, but we can substitute ; for the ends of statements and put it all on one line. (Useful if we are just hacking away on the command line…)\n\n\nThis does the same as the above one\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do echo \"I like $LIKE.\"; done\n\nNote! Don’t put a semicolon after do.\n\n\n\n\n\n\nSelf study\n\n\n\nWe want to give the number of text lines, words, and characters from all the samtools_stats files.\nPrep: Here is a command that prints the name of each file.\n\n\nPaste this into the terminal\n\nfor FILE in data/samtools_stats/*.gz; do echo $FILE; done\n\nTask: I have added the -n option to the echo command which makes it not print a line ending. Your task is to replace YOUR_STUFF_HERE with an appropriate shell command to decompress each file and then print the number of lines, words, and characters in it:\n\n\nPaste this, edit YOUR_STUFF_HERE, and run it\n\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; YOUR_STUFF_HERE; done\n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nYour edited command line should look like this:\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; gzip -cd $FILE | wc; done"
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#redirect-stdout-from-the-done",
    "href": "nmfs-bioinf/shell-prog.html#redirect-stdout-from-the-done",
    "title": "2  Shell Programming",
    "section": "2.8 Redirect stdout from the done",
    "text": "2.8 Redirect stdout from the done\nHere is something that is not always obvious: you can redirect or pipe the stdout of the whole for loop by using &gt; or | immediately after the done keyword.\nUsing the example from the self study above:\n\n\nPaste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do \n  echo -n $FILE; gzip -cd $FILE | wc; \ndone &gt; word_counts.txt\n\nNow look at what is in word_counts.txt."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#a-few-useful-topics-rapidly",
    "href": "nmfs-bioinf/shell-prog.html#a-few-useful-topics-rapidly",
    "title": "2  Shell Programming",
    "section": "2.9 A few useful topics, rapidly",
    "text": "2.9 A few useful topics, rapidly\n\n2.9.1 basename\n\nIf you have a path name to a file this_dir/that_dir/my_file3.txt, but you want to have the string for just the file name, my_file3.txt, you can use the basename command:\n\n\n\nTry these\n\nbasename this_dir/that_dir/my_file3.txt\nbasename ~/Documents/git-repos/CKMRpop/R/plot_conn_comps.R\n\n\n\n2.9.2 Capture stdout into a token to put on the command line\nThis is a pretty cool one, and is really nice if you want to capture a bit of output for use at a later time.\nBasically, if you run a command inside parentheses that are immediately preceded by a $, like $(command), then the stdout output of command gets put onto the command line as a single token.\nObserve:\n\n\nPaste this into your terminal\n\nSTART_TIME=$(date)\nsleep 3\nSTOP_TIME=$(date)\necho \"We started at $START_TIME, and finished at $STOP_TIME, and it is now $(date)\"\n\nOr even:\n\n\nPaste this into your terminal\n\nWCOUT=$(gzip -cd data/samtools_stats/s016_stats.tsv.gz | wc)\necho $WCOUT\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ndate: prints the current time and date to stdout.\nsleep: causes the shell to pause for however many seconds you tell it to, like sleep 3 for three seconds, sleep 180 for three minutes."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#fancier-variable-substitution",
    "href": "nmfs-bioinf/shell-prog.html#fancier-variable-substitution",
    "title": "2  Shell Programming",
    "section": "2.10 Fancier variable substitution",
    "text": "2.10 Fancier variable substitution\n\n2.10.1 Wrap it in curly braces ${VAR}\nEspecially if you want to substitute a variable into a string adjacent to a letter or number or underscore, you can wrap it in curly braces.\n\n\nTry this\n\nsample=001\n# this works:\necho \"The sequences are in the file called ${sample}_seqs.fq.gz\"\n\n# this does not work the way you want it to\necho \"The sequences are in the file called $sample_seqs.fq.gz\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nWhy do you think the second echo line above produced the output that it did?\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nSince _ is a valid character for a variable name, $sample_seqs.fq.gz gets broken up by the shell as $sample_seqs plus .fq.gz, and there is no variable named sample_seqs, so the shell just substitutes an empty string into $sample_seqs.\n\n\n\n\n\n2.10.2 Variable modification while substituting it\nbash has a whole lot of tricky syntaxes for manipulating variable values when substituting them onto the command line.\nThe one I use more than any other is ${VAR/pattern/replacement}. This looks for a text pattern pattern in the variable VAR and replaces it with the text replacement.\nHere are some examples.\n\n\nFancy substitution fun. Paste into your terminal.\n\nfile=myfile.eps\necho ${file/eps/pdf}\n\n# or maybe you want to get the sample name, s007\n# out of a file path like data/samtools_stats/s007_stats.tsv.gz\npath=data/samtools_stats/s007_stats.tsv.gz\necho $(basename ${path/_stats.tsv.gz/})\n\nWhoa! On that last one we nested a ${//} inside a $()!\n\n\n\n\n\n\nHot tip!\n\n\n\nYou can use * the way that you might when globbing filenames on the command line in the pattern for variable substitution with ${var/pattern/replacement}:\n\n\nIf we want to extract just the s001 part...\n\nSTRING=\"A-whole-lot-of-junk-before-then_s001_and-a-whole-lot_of-other-garbage.63713973\"\n\n# remove all the garbage in the beginning\nN1=${STRING/*then_/}\n\n# see what we have at this point\necho $N1\n\n# remove the remaining junk off the end\nN2=${N1/_and-*}\n\n# see what we ended up with\necho $N2\n\n\n\n\n\n2.10.3 Grouping multiple commands with (...)\n\nSometimes it is convenient lump a number of commands together into a group.\nThe main reason I do this is to capture stdout or stderr from all of them into a single file with one redirect (as opposed to redirecting (&gt;) output from the first command, and then redirect-appending (&gt;&gt;) output from successive commands to the same place).\nWhen you wrap a series of commands in a pair of parentheses, they get executed as a group and you can redirect that from the right side of the last parenthesis:\n\n\n\nPaste this into your terminal\n\n(\n  echo \"This\"\n  echo \"that\"\n  echo \"and the\"\n  echo \"other\"\n) &gt; group_it.txt\n\nCheck out the result with cat group_it.txt.\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWith grouping parentheses, you can also redirect stderr to a single place even if there are pipes involved. Comparing to the Self-Study answer at the end of the Quick Unix Review session:\n\n\nA stderr example\n\n(gzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc) &gt; so.txt 2&gt;se.txt\n\nEven though the error happened with the gzip command, the error message gets relayed through to be redirected into se.txt."
  },
  {
    "objectID": "nmfs-bioinf/shell-prog.html#leaving-here-for-now",
    "href": "nmfs-bioinf/shell-prog.html#leaving-here-for-now",
    "title": "2  Shell Programming",
    "section": "2.11 Leaving here for now…",
    "text": "2.11 Leaving here for now…\nYou might be thinking, “Wow, could I use a for loop or something like it in bash to cycle over the lines of a text file to process each line in turn?”\nThe answer, is, “You can, but bash is not always the best tool for processing text files…especially if they are large.”\nThere is a Unix utility called awk that is much better for that.\nThat is where we are heading next."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#awks-philosophy-and-basic-syntax",
    "href": "nmfs-bioinf/awk-intro.html#awks-philosophy-and-basic-syntax",
    "title": "3  A Brief awk Intro",
    "section": "3.1 awk’s philosophy and basic syntax",
    "text": "3.1 awk’s philosophy and basic syntax\nawk is a utility that:\n\nTakes text input from a file or from stdin\nIt automatically goes line-by-line. Treating each line of text as a separate unit.\nWhen it is focused on a line of text, it breaks it into fields which you can think of as columns.\nBy default, fields, are broken up on whitespace (spaces and TABs), with multiple spaces or TABs being treated as a single delimiter.\nYou can specify the delimiter.\n\nLet’s look through a file together. I will do:\n\n\nYou can do this if you want\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\nand we will discuss how awk sees such a file.\n\n3.1.1 Naming fields\nOnce awk has read a line of text into memory, and split it into fields, you can access the value of those fields with special names:\n\n$1 refers to the first field (column)\n$2 refers to the second column.\n\nIf you are beyond the ninth column, the number has to be wrapped in parentheses:\n\n$(13) refers to the thirteenth column\n\n\n\n\n\n\n\nImportant\n\n\n\nThese variables within an awk script are not related to substituted variables in bash. They just happen to share a preceding $. But they are being interpreted by different programming languages (one by bash the other by awk).\n\n\n\n\n3.1.2 So what?\nSo far, this doesn’t seem very exciting, but now we learn that…\n\nYou can tell awk the sorts of lines you want it to pause at and then do some action upon it.\n\nYou tell awk which lines you want to perform an action on by matching them with a logical statement called a pattern in awk parlance.\n\nThis turns out to be incredibly powerful.\n\n\n3.1.3 awk syntax on the command line\nThe syntax for running awk with a script on the command line is like this:\n\n\nDon't try running this. It is just for explanation.\n\nawk '\n  pattern1  {action1}\n  pattern2  {action2}\n' file\n\nWhere file is the name of the file you want to process.\nOr, if you are piping text into awk from some command named cmd it would look like this:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk '\n  pattern1  {action1}\n  pattern2  {action2}\n'\n\nNote that you can pass options (like -v or -F) to awk. They go right after the awk and before the first single quotation mark.\nAlso, the carriage returns inside the single quotation marks are only there for easy reading. You could also write the last example as:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk 'pattern1  {action1} pattern2  {action2}'\n\n…which is great if you are hacking on the command line, but once you have a lot of pattern-action pairs, it gets harder to read.\n\n\n\n\n\n\nSelf-study\n\n\n\nThinking back to the previous session when we talked about the difference between grouping strings with ' vs with \", why do you think it is important that the awk script is grouped with '?\n\n\n\n\n\n\n\n\nBrief answer\n\n\n\n\n\nAs we saw, we will be referring to different fields like $8 within the awk script. If we used \" to group the script, the shell might try to do variable substitution on any $’s in there."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#enough-talking-lets-start-doing",
    "href": "nmfs-bioinf/awk-intro.html#enough-talking-lets-start-doing",
    "title": "3  A Brief awk Intro",
    "section": "3.2 Enough talking, let’s start doing",
    "text": "3.2 Enough talking, let’s start doing\nAll of this will make more sense with a few examples.\n\n3.2.1 Print all the lines in which the first field is SN\nFor our first foray, let’s just pick out and print a subset of lines from our samtools stats file:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '$1==\"SN\" {print}'\n\nThat is cool.\n\n\n\n\n\n\nSelf-study\n\n\n\nMake sure that you can identify the pattern and the action in the above awk script.\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nThe pattern is $1==\"SN\"\nThe action is print\n\nThis brings up the important point that the “is equals” operator in awk is == (two consecutive equals signs…just like in R)\n\n\n\n\n\n3.2.2 Printing the lines we are interested in\nHow about if we wanted to pick out just a few particular lines from there?\nWell, we can also match lines by regular expression (which you can think of as a very fancy form of Unix word-searching.)\nLet’s say that we want information on the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped using the (cigar) criterion.\nWe can see those lines in there. And we can target them by matching strings associated with them. The awk syntax puts these regular expressions in the pattern between forward slashes.\nSo, we want to match lines that have the first field equal to SN and also match other strings. We do that like this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print}\n  $1==\"SN\" && /reads properly paired:/ {print}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print}\n'\n\n\n\n\n\n\n\nBig Note:\n\n\n\nRegular expressions are lovely and wonderful, but occasionally frustrating. In awk’s case, parentheses have special meanings in the regular expressions, so we have to precede each one in the pattern with a backslash.\nRegular expressions are a bit beyond the scope of what we will be talking about today (entire books are devoted to the topic) but I encourage everyone to learn about them.\nThey are incredibly useful and they are used in multiple programming languages (R, python, perl, etc.)\n\n\n\n\n3.2.3 Printing just the values we are interested in\nThat is nice, but remember, we really just want to put those three values we are interested in into a table of sorts.\nSo, how do we print just the values?\nUse the fields! Count columns for each line and then print just that field:\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print $4}\n  $1==\"SN\" && /reads properly paired:/ {print $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print $5}\n'\n\n\n\n3.2.4 Use variables inside awk\nWe can also assign values to variables inside awk.\nThis lets us store values and then print them all on one line at the end. The special pattern END gives us a block to put actions we want to do at the very end.\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print rm, rpp, bmc}\n'\n\n\n\n\n\n\n\nFor the programmers out there\n\n\n\nVariables in awk are untyped and do not be declared. Basically you can put them anywhere. If code calls for the value from a variable that has not has anything assigned to it yet, the variable returns a 0 in a numerical context, and an empty string in a string context.\n\n\n\n\n3.2.5 That’s great. Can we add the sample name in there?\nYes! We can pass variables from the command line to inside awk with a -v var=value syntax.\nTo do this, we use some shell code that we learned earlier!\n\n\nStudy this, then paste this into your terminal\n\nFILE=data/samtools_stats/s001_stats.tsv.gz\ngzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print samp, rm, rpp, bmc}\n'\n\n\n\n3.2.6 OMG! Do you seen where we are going with this?\nWe can now take that whole thing and imbed it within a bash for loop cycling over values of FILE and get the table talked about wanting in our Motivating Example when we started.\n\n\nStudy this, then paste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nYowzers! That is pretty quick, and it sure beats opening each file, copying the values we want, and then pasting them into a spreadsheet."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "href": "nmfs-bioinf/awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "title": "3  A Brief awk Intro",
    "section": "3.3 Another example: the distribution of mapping qualities",
    "text": "3.3 Another example: the distribution of mapping qualities\nHere is a fun awk example that just came up a couple of days ago for me.\n\nA colleague was telling me that she has started filtering her whole-genome sequencing BAM files so that she does not use any reads that map with a mapping quality less than 30.\nThe hope is that this will lessen batch effects.\nQuestions:\n\nWhat is the distribution of mapping quality scores in my own data\nIf we imposed such a filter, how many reads would we discard?\n\n\nIt turns out that none of the samtools programs stat, idxstats, or flagstats provide that distribution.\nThere are some other more obscure software packages that provide it, but also a lot of convoluted python code on the BioStars website for doing it.\nHa! It’s quick and easy with awk! And a great demonstration of awk’s associative arrays.\n\n3.3.1 Let’s look at an example bam file\nWe have an example bam file in the repository at data/bam/s0001.bam.\nIt only has only about 25,000 read in it so that it isn’t too large.\nLet’s have a look at it with:\n\n\nPaste this into your terminal\n\nmodule load bio/samtools\nsamtools view data/bam/s001.bam | less -S\n\nThe module load bio/samtools line gives us access to the samtools program, which we need for turning BAM files into text-based SAM files that we can use. Once we have given it in our shell, we have that access until we close the shell. Much more on that tomorrow!\n\n\n\n\n\n\nIf that failed you might need to define your MODULEPATH\n\n\n\n\n\nHere we check to see if the bioinformatics paths on in the MODULEPATH:\n\n\nPaste this in your shell\n\necho $MODULEPATH | grep bioinformatics\n\nIf that command did not return anything to stdout, then you need to add a command to your ~/.bashrc that will add /opt/bioinformatics/modulefiles to your MODULEPATH. You can do that (as detailed in the Sedna Google Doc) like this:\n\n\nIf you need to, paste this into the shell\n\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' &gt;&gt; ~/.bashrc\n\nAnd when you are done with that, reload your .bashrc to get the change to take effect:\n\n\nIf you need to, paste this into the shell\n\nsource ~/.bashrc\n\n\n\n\nThe mapping quality is in the 5th field. It is a number that ranges from 0 to 60. We can count up how many times each of those numbers occurs using awk.\n\n\nHere it is all on one line as I wrote it\n\nsamtools view data/bam/s001.bam | awk 'BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} {n[$5]++} END {for(i in n) tot+=n[i];  for(i=60;i&gt;=0;i--) {sum+=n[i]; print i, n[i], sum/tot;}}'\n\nAnd\n\n\nHere it is broken across lines. Paste that in your shell.\n\nsamtools view data/bam/s001.bam | awk '\n  BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} \n  {n[$5]++} \n  END {for(i in n) tot+=n[i];  for(i=60;i&gt;=0;i--) {sum+=n[i]; print i,n[i],sum/tot;}}\n'\n\nIt’s really compact and requires very little memory to do this. (You couldn’t read a whole BAM file into R and hope to deal with it).\n\n\n\n\n\n\nAll arrays in awk are associative arrays\n\n\n\nIf you come from an R programming background, you will typically think of arrays as vectors that are indexed from 1 to n, where n is the length of the vector.\nThis is not how arrays are implemented in awk. Rather all arrays are associative arrays, which are also called hash arrays, or, in Python dictionaries. Or, if you are familiar with R, you can think of an associative array as an array that has elements that can only be accessed via their names attribute, rather than by indexing them with a number.\nSo, in awk, if we write:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[\"this\"] = \"Boing!\"\n\nThis will create an array called var (if one does not already exist) and then it will set the value of element in var that is associated with the string \"this\" to the string \"Boing!\".\nAt the same time, if you do this:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[30] = 67\n\nthen we are not assigning the value of 67 to the 30-th element of var. Rather, we are assigning the value 67 to the element of var that is associated with the string \"30\".\nIt can take a little getting used to, but it is very useful for counting things."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#wrap-up",
    "href": "nmfs-bioinf/awk-intro.html#wrap-up",
    "title": "3  A Brief awk Intro",
    "section": "3.4 Wrap-Up",
    "text": "3.4 Wrap-Up\nThat was just a brief whirlwind tour of how one can use bash and awk together to automate tasks that come up on an everyday basis when doing bioinformatics."
  },
  {
    "objectID": "nmfs-bioinf/awk-intro.html#looking-toward-tomorrow",
    "href": "nmfs-bioinf/awk-intro.html#looking-toward-tomorrow",
    "title": "3  A Brief awk Intro",
    "section": "3.5 Looking toward tomorrow",
    "text": "3.5 Looking toward tomorrow\nThe bulk of day #2 is going to be focused on working within a cluster environment, and specifically on using SLURM for launching jobs on the Sedna cluster.\nTo prepare for tomorrow, please be sure to read Chapter 8 from beginning and up to and including section 8.2. This is just a small bit to read, but it should set you up for an understanding of why and how computing clusters work differently than your desktop machine when it comes to allocating resources for computation."
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#prepare-for-this",
    "href": "nmfs-bioinf/scripts-and-functions.html#prepare-for-this",
    "title": "4  Bash scripts and functions",
    "section": "4.1 Prepare for this",
    "text": "4.1 Prepare for this\nSync your fork of the repository. Then, make sure that you have all the latest updates from the repository by pulling them down with git. use git to pull down any new changes.\n\n\nUse something like this in to be sure you have the most up-to-date resources\n\ncd YOUR-CLONE-OF-YOUR-FORK-OF-THE-REPO\ngit pull origin main"
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#bash-shell-scripts",
    "href": "nmfs-bioinf/scripts-and-functions.html#bash-shell-scripts",
    "title": "4  Bash scripts and functions",
    "section": "4.2 Bash shell scripts",
    "text": "4.2 Bash shell scripts\nWe have been doing all of our bash scripting by writing commands on the command line.\nUseful bash code can be stored in a text file called a script, and then run like a normal Unix utility.\nTo illustrate this, we will copy our samtools-stats-processing commands from before into a file using the nano text editor.\nAt your command line, type this:\n\n\nType this at the command line\n\nnano sam-stats.sh\n\nThis opens a file called sam-stats.sh with a text editor called nano.\nThe convention with bash shell scripts is to give them a .sh extension, but this is not required.\nNow we copy our commands into nano.\n\n\nCopy this onto your clipboard and paste it into nano in your terminal\n\n#!/bin/bash\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nThen:\n\ndo cntrl-X\nAnswer Y when asked if you want to save the file\nHit return to save the file as sam-stats.sh\n\nVoila! That should exit the nano editor, and now you have a script containing that bash code.\nCheck out its contents with:\n\n\nType this at the command line\n\ncat sam-stats.sh\n\n\n\n\n\n\n\nWhat’s this #! at the top of the file?\n\n\n\nThat is colloquially referred to as the shebang line.\nA # usually tells the shell to “ignore everything to the right of the # on this line”\n# is used to precede comments in your code.\nHowever, in this case, at the top of the file and followed by a ! it tells the computer what language to use to interpret this file.\nIn our case /bin/bash is where the bash shell interpreter typically is found on most Unix or Linux system.\n(If bash is the default shell on your system, you may not always need to have the shebang line, but it is good practice to do so.)\n\n\n\n4.2.1 Script files must be executable\nThe Unix operating system distinguishes between files that just hold data, and files that can the run or be “executed” by the computer.\nFor your bash commands in a script to run on the computer, it must be of an executable type.\nWe can make the file executable using the chmod command, like this:\n\n\nPaste this into your shell\n\nchmod u+x sam-stats.sh\n\nIf we then use ls -l to list the file in long format like this:\n\n\nType this in\n\nls -l sam-stats.sh\n\nwe see:\n-rwxrw-r-- 1 eanderson eanderson 317 Oct 14 13:40 sam-stats.sh\nThe x in the first field of that line indicates that the file is executable by the user.\n\n\n4.2.2 Running a script\nTo run a script that is executable, you type the path to it.\nOn some clusters, by default, the current working directory is not a place the computer looks for executable scripts, so we have to prepend ./ to its path:\n\n\nType this on the command line and hit RETURN.\n\n./sam-stats.sh\n\nThat runs our script.\n\n\n4.2.3 Scripts are more useful if you can specify the inputs\nSo, that runs our script and produces results, but that is not so useful. We already had those results, in a sense.\nShell scripts become much more useful when you can change the inputs that go to them.\nOne way to do so involves using positional parameters\n\n\n4.2.4 Arguments following a script can be accessed within the script\nIf you put arguments after a script on the command line, for example like:\n\n\nDon't paste this in anywhere\n\nscript.sh arg1 some_other_arg  And_another_arg\n\nthen in the script itself:\n\nthe value of the first argument (arg1) is accessible as $1\nthe value of the second argument (some_other_arg1) is accessible as $2\nthe value of the third argument (And_another_arg) is accessible as $3\n\n…and so forth for as many arguments as you want.\nSo, we can rewrite our script as:\n\n\nJust give this a read\n\n\n#!/bin/bash\n\nfor FILE in $1; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nSee that we have replaced data/samtools_stats/*.gz with $1.\nEdit sam-stats.sh to look like the above. Now we can use it like this:\n\n\nPaste this into the shell\n\n./sam-stats.sh \"data/samtools_stats/s0*.gz\"\n\nHere, we have passed in \"data/samtools_stats/s0*.gz\" as the first argument to the script, and, since we have a $1 in the script where that goes, it does what it did before.\n\n\n4.2.5 Now we can use it on other samtools stats files\nIt is not very exciting to see it just run again on the same set of files.\nBut, now we could direct the script to operate on a different set of files, just by changing the argument that we pass to the script.\nI have put a much larger set of samtools stats files within subdirectory of the /share directory on Sedna. You should be able to list them all with:\n\n\nPaste this into your shell. This should work...\n\n ls /share/all/eriq/big_sam_stats/s*.gz\n\nWhoa! 275 files. (They are here on Sedna, in the shared folder, because I didn’t want to put them all on GitHub.)\nBut, now, we can summarize them all just by pointing our script to them:\n\n\nThis should work on Sedna, Paste it into your shell\n\n./scripts/sam-stat-pp.sh \"/share/all/eriq/big_sam_stats/s*.gz\"\n\nThat is fast.\nIf we wanted to redirect that into a file we could do so\n\n\n\n\n\n\nFun Tip! – Record the positional parameters passed to a script.\n\n\n\n\n\nSometimes it is nice to know and record the values of all the arguments passed to a script that you have written. This can be done by adding something like this to your script:\necho \"Script $0 started at $(date) with arguments $* \" &gt; /dev/stderr\nIn that:\n\n$0 is the path to the script\n$(date) puts the current date and time in the line\n$* expands to a single string with all the arguments passed to the script\n&gt; /dev/stderr redirects stdout from echo to stderr\n\nSo, for the last script, that might look like:\n#!/bin/bash\n\necho \"Script $0 started at $(date) with arguments $* \" &gt; /dev/stderr\n\nfor FILE in $1; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone"
  },
  {
    "objectID": "nmfs-bioinf/scripts-and-functions.html#bash-functions",
    "href": "nmfs-bioinf/scripts-and-functions.html#bash-functions",
    "title": "4  Bash scripts and functions",
    "section": "4.3 Bash functions",
    "text": "4.3 Bash functions\nLike most programming languages, you can define functions in bash.\nThe syntax for defining a function named MyFunc is:\n\n\nJust read this\n\nfunction MyFunc {\n  ...code that MyFunc executes...\n}\n\nIn this regard, it has a similar syntax to R.\nOnce defined, you can use the name of the function (in the above case, MyFunc) as if it were just another Unix command.\n\n4.3.1 Example: congratulate yourself on making it through your day\nHere is a gratuitous example: we make a bash function called congrats that tells us what time it is and encourages us to keep getting through our day:\n\n\nPaste this into your terminal\n\nfunction congrats { echo \"It is now $(date).  Congrats on making it this far in your day.\"; }\n\n\n\n\n\n\n\nWarning\n\n\n\nCurly braces in bash are extremely finicky. They don’t like to be near other characters. In the above, the space after the { is critical, as is the ; before the }. (The last } needs to have a ; before it if it does not have a line ending before it).\n\n\nNow, you can just type congrats at the command line:\n\n\nType this on the command line\n\ncongrats\n\n\n\n4.3.2 Bash functions take positional parameters too\nWe can rewrite our function as congrats2 so that it can use two arguments, the first a name, and the second an adjective:\n\n\nPaste this into your terminal\n\nfunction congrats2 { \n  echo \"It is now $(date).  Congrats, $1, you are $2.\"\n}\n\nNow, you can use that function and supply it with whatever names and adjectives you would like:\n\n\nPaste these in and see what it does.\n\ncongrats2 Fred splendid\ncongrats2 Eric tired\ncongrats2 Amy amazing\n\nWe will write a few functions, later, to simplify our life in SLURM."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#why-do-we-need-slurm",
    "href": "nmfs-bioinf/slurm.html#why-do-we-need-slurm",
    "title": "5  Sedna and SLURM intro",
    "section": "5.1 Why do we need SLURM?",
    "text": "5.1 Why do we need SLURM?\n\nThe fundamental problem of cluster computing.\nA cluster does not operate like your laptop.\nMost compute-intensive jobs run most efficiently on a dedicated processor or processors."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#hpcc-architecture-in-a-nutshell",
    "href": "nmfs-bioinf/slurm.html#hpcc-architecture-in-a-nutshell",
    "title": "5  Sedna and SLURM intro",
    "section": "5.2 HPCC Architecture in a Nutshell",
    "text": "5.2 HPCC Architecture in a Nutshell\n\n\nNodes: the closest thing to what you think of as a computer. (“Pizza-boxes” with no displays).\n\nEach node is attached via a fast connection to centralized attached storage (A big set of hard drives attached via “Infiniband.”)\nWithin each node are some numbers of Cores or CPUs\n\nCores/CPUs are the actual processing units within a node. (Usually 20 to 24)\n\n\nSedna, like almost all other HPCCs has a login node\n\nThe login node is dedicated to allowing people to communicate with the HPCC.\nDO NOT do computationally intensive, or input/output-intensive jobs on the login node\nNot surprisingly, when you login to Sedna you are on the login node.\n\n\n\n\n\n\n\n\nHot tip!\n\n\n\nIn the default configuration on Sedna, your command prompt at the shell tells you which node you are logged into. The default command prompt looks like:\n[username@computer directory]\nSo, for example, mine at the moment is:\n[eanderson@sedna playground]$\nWhich tells me that I am user eanderson and I am logged in to sedna in the directory whose basename is playground.\nThe login node for Sedna is named sedna. So, this is telling me that I am logged into the login node of Sedna.\nIf you don’t have such an informative prompt…\nOn Alpine, I am not sure what the default command prompt is, but you can always change yours by setting the PS1 variable. Try doing:\nexport PS1='[\\h: \\W]--% '\nThat will give you a command prompt with the hostname (\\h) and the working directory basename (\\W), which is helpful. Mine, right now, looks like:\n[login11: con-gen-csu]--%\nOf course, you can put the export PS1='[\\h: \\W]--% ' into your ~/.bashrc file and then always have an informative prompt.\n\n\n\n\n\n\n\n\nSelf study\n\n\n\nInvestigate your own command prompt and make sure you understand what the different parts in it mean."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#say-it-again-dont-do-heavy-computing-on-the-login-nodes",
    "href": "nmfs-bioinf/slurm.html#say-it-again-dont-do-heavy-computing-on-the-login-nodes",
    "title": "5  Sedna and SLURM intro",
    "section": "5.3 Say it again: Don’t do heavy computing on the login nodes!",
    "text": "5.3 Say it again: Don’t do heavy computing on the login nodes!\nIf you run a big job that is computationally intensive on the login node, it can interfere with other people being able to access the cluster.\nDon’t do it!!\nThat is all fine and well, but how do we avoid computing on the login nodes?"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#do-your-computation-on-the-compute-nodes",
    "href": "nmfs-bioinf/slurm.html#do-your-computation-on-the-compute-nodes",
    "title": "5  Sedna and SLURM intro",
    "section": "5.4 Do your computation on the compute nodes",
    "text": "5.4 Do your computation on the compute nodes\n\nTo run jobs on the compute node you need to ask SLURM to give you “compute resources” to run the job.\nThe basic axes of “compute resources” are:\n\nThe number of cores to use.\nThe maximum amount of RAM memory you might need.\nThe amount of time that you will need those resources for.\n\nTo prepare for the next part of the course on SEDNA, each one of us is going to “check out” 2 cores for 3 hours for interactive work.\n\nInteractive here means that we will have a Unix shell that has access to compute power on the cores that we checked out.\nHere is the command to checkout 2 cores for 3 hours for interactive use:\n\n\n\nPaste this into your shell on SEDNA\n\nsrun -c 2 -t 03:00:00 --pty /bin/bash\n\nIf you are working on ALPINE then you can get on a compute node differently:\n\nLoad the slurm/alpine module\nRequest a shell on the oversubscribed atesting partition.\nThe commands for that look like:\n\n\n\nPaste this into your shell on ALPINE\n\nmodule load slurm/alpine\nsrun --partition atesting --pty /bin/bash\n\n\n\n\n\n\n\n\nSelf study:\n\n\n\nHave a look at your command prompt now. If you are on SEDNA, it should show that you are logged in to nodeXX where XX is a number like 04 or 33. If you are on Alpine, then the node you are on will be named something more cryptic, like c3cpu-a5-u34-3."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#what-does-sedna-have-under-the-hood",
    "href": "nmfs-bioinf/slurm.html#what-does-sedna-have-under-the-hood",
    "title": "5  Sedna and SLURM intro",
    "section": "5.5 What does Sedna have under the hood?",
    "text": "5.5 What does Sedna have under the hood?\nSedna is not an outrageously large cluster, but it is well-maintained and provides a lot of computing power.\nThe nodes in Sedna are broken into three different partitions.\nA partition is a collection of nodes that tend to be similar.\n\nnode partition: This is a collection of 36 nodes:\n\nnode01 – node28: “standard compute nodes”\nnode29 – node36: “standard compute nodes with twice as much memory”\n\nhimem partition: Four nodes with a boatload of memory for high-memory jobs\n\nhimem01 – himem04\n\nbionode partition: legacy machines from an older NWFSC cluster\n\nI’m not sure if all of us have access to this.\nWhen I check, it seems like a lot of the nodes are down, occasionally.\nBut I have been able to checkout resources on it.\nMight be an option, but it doesn’t perform at the same level as the node partition"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#what-does-alpine-have-under-the-hood",
    "href": "nmfs-bioinf/slurm.html#what-does-alpine-have-under-the-hood",
    "title": "5  Sedna and SLURM intro",
    "section": "5.6 What does ALPINE have under the hood?",
    "text": "5.6 What does ALPINE have under the hood?\nAlpine is a pretty darn large cluster, with a boatload of different machines, many of them with 64 cores. It also has some GPU machines, etc.\nWe will have a look at it with the commands, like sinfo described next."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#ask-slurm-about-what-nodes-are-available-and-how-busy-they-are",
    "href": "nmfs-bioinf/slurm.html#ask-slurm-about-what-nodes-are-available-and-how-busy-they-are",
    "title": "5  Sedna and SLURM intro",
    "section": "5.7 Ask SLURM about what nodes are available, and how busy they are",
    "text": "5.7 Ask SLURM about what nodes are available, and how busy they are\nWhen I login to Sedna, before launching and jobs or working on anything, I always like to get a summary of how hard Sedna is currently working.\nFor this we have the sinfo command:\n\n\nType this at your command prompt\n\nsinfo\n\nThe answer will look somethign like this:\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnode*        up   infinite      5    mix node[01,29-31,33]\nnode*        up   infinite      1  alloc node32\nnode*        up   infinite     30   idle node[02-28,34-36]\nhimem        up   infinite      1    mix himem01\nhimem        up   infinite      3   idle himem[02-04]\nbionode      up   infinite      8  down* bionode[10-11,13-18]\nbionode      up   infinite      1  drain bionode12\nbionode      up   infinite     10   idle bionode[01-09,19]\nThis is a hugely informative summary for being so compact.\n\nThe output is sorted by partition.\nWithin each partition it tells us how many nodes are in different states:\nThe main states:\n\nidle: just sitting there with all cores available for checkout,\nmix: some, but not all, cores are checkout out and in use,\nalloc: all the cores on the node are checkout out and in use.\n\nOther states you might see:\n\ndown: node is unavailable because it is not working properly,\ndrain: node is not available because the sys-admins are not letting any new jobs to start because they are going to be working on the system soon, etc.\n\nThe node partition is starred node* because it is the default partition.\nNotation like node[01,29-31,33] gives the specific node numbers, compactly.\n\nIf you type sinfo on ALPINE you get a similar output, but it is much larger and more complex because there are more partitions and more, different nodes in each partition.\nThe CURC lists the partitions on a web page here.\nIf you are going to be working on Alpine, it is worth reading over the entire CURC documentation for it, starting from here.\n\n\n\n\n\n\nSelf-study\n\n\n\nReview how many nodes are currently in use."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#more-detailed-sinfo-output",
    "href": "nmfs-bioinf/slurm.html#more-detailed-sinfo-output",
    "title": "5  Sedna and SLURM intro",
    "section": "5.8 More detailed sinfo output",
    "text": "5.8 More detailed sinfo output\nSometimes you want more information about what is going on with the cluster. sinfo can do that, too, but the syntax is hard to remember and even harder to type.\nHere is a bash function we will define called slinfo for (s-long-info) that uses sinfo to provide more detailed output.\n\n\nPaste this into your shell\n\nfunction slinfo {\n  sinfo -N -O nodelist,cpusstate,memory,allocmem\n}\n\nOnce you have done that:\n\n\nType this at your command prompt\n\nslinfo\n\nThis gives information about each node.\n\nIn CPUS(A/I/O/T):\n\nA = allocated\nI = idle\nO = offline\nT = total\n\nMEMORY is total RAM available on the node (in Megabytes)\nALLOCMEM is the total RAM allocated to jobs.\n\nThis view shows each node just once. But it might be useful to also see what partition(s) each of those nodes are in (especially on ALPINE). So, here is a quick function to do that:\n\n\nPaste this into your shell\n\nfunction slinfop {\n  sinfo -N -O nodelist,partition,cpusstate,memory,allocmem\n}\n\nOnce you have done that:\n\n\nType this at your command prompt\n\nslinfop\n\nNote that many nodes are included in multiple partitions.\nSince the main partition on Alpine for general computing is amilan you can look specifically at nodes in that partition with:\n\n\nCopy this into your command prompt\n\nslinfop | awk '$2~/amilan/'\n\n\n\n\n\n\n\nALPINE Self-study\n\n\n\nQuickly scan the output of the above command and estimate the number of idle (i.e. available) cores in the amilan partition.\nThen, read about the amilan partition (here)[https://curc.readthedocs.io/en/stable/clusters/alpine/alpine-hardware.html#partitions]. In particular, note how much memory is available per core, and the default and max wall times.\nWe will discuss this."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#summarize-what-the-cluster-is-doing-by-job",
    "href": "nmfs-bioinf/slurm.html#summarize-what-the-cluster-is-doing-by-job",
    "title": "5  Sedna and SLURM intro",
    "section": "5.9 Summarize what the cluster is doing by job",
    "text": "5.9 Summarize what the cluster is doing by job\nSLURM keeps track of individual jobs that get submitted.\nFor SLURM, each job is basically a request for resources.\nIf SLURM finds the requested resources are available, it provides the resources and starts the job.\nIf resources are not available because other jobs are running, the reqested job enters the “queue” in a WAITING state.\nWe can see how many jobs are running and how many are waiting, by using the SLURM squeue command:\n(By the way, notice the pattern? All SLURM command start with an s).\n\n\nType this at your command prompt\n\nsqueue\n\nThis tells us a little about all the jobs that are currently allocated resources or are waiting for resources.\nOne thing to note: - Every job is assigned a SLURM_JOB_ID. Which is a unique integer (that gets assigned successively). - And jobs might have a job NAME, that is assigned by the user, which is the name of the script that the job runs, or is defined in the SBATCH directives (we will talk about that later!)"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#i-find-the-standard-default-squeue-output-sorely-lacking",
    "href": "nmfs-bioinf/slurm.html#i-find-the-standard-default-squeue-output-sorely-lacking",
    "title": "5  Sedna and SLURM intro",
    "section": "5.10 I find the standard, default, squeue output sorely lacking",
    "text": "5.10 I find the standard, default, squeue output sorely lacking\nThe standard way that squeue presents information is not super informative, especially if the job names are not very short, or if you are interested in how many cores (not just nodes) each job involves.\nOnce again, we make a bash function, alljobs, that runs squeue but provides more information.\n\n\nPaste this at your command prompt\n\nfunction alljobs {\n  if [ $# -ne 1 ]; then\n    JL=10;\n  else\n    JL=$1;\n  fi;\n  squeue -o  \"%.12i %.9P %.${JL}j %.15u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L\";\n}\n\nAlso, if you follow the command with an integer like 20, that is how many spaces the output will allow for the job NAME.\nFor example, try:\n\n\nType this at the command prompt\n\nalljobs 30\n\nThis is kind of fun to see the full names of the jobs.\n\n\n\n\n\n\nHot Tip!\n\n\n\nIt is worth taking special note of the columns, NODELIST(REASON) and CPUS:\n\nNODELIST(REASON): If the job is running, then this gives the name(s) of the node(s) upon which it is running. If it is not running, it says why it is not yet running. The most typical reason it:\n\nPriority: Too many other users are already using the cluster. Your job is waiting for resources to become available.\n\nCPUS: the number of CPUs the job is using\n\nIt is worth noting that, when I look at alljobs on Alpine, most of the jobs that are waiting due to Priority are jobs that are requesting all or most of the CPUs (like 64 or 32) on a single node. Many of the jobs that are not waiting are those that are requesting only one or a few cores.\nIn general, if you can break your workflows down into a small chunks that run on just one or a few cores, you have a better chance of getting compute time, than if you request all the cores on a node."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#just-tell-me-about-my-jobs",
    "href": "nmfs-bioinf/slurm.html#just-tell-me-about-my-jobs",
    "title": "5  Sedna and SLURM intro",
    "section": "5.11 Just tell me about my jobs!",
    "text": "5.11 Just tell me about my jobs!\nWe can make a similar function that only tells us about our own jobs that are running or waiting in the queue:\n\n\nPaste this into your terminal\n\nfunction myjobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -u $(whoami) -o  \"%.12i %.9P %.${JL}j %.10u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L %p\";\n}\n\nYou can run that the same way as alljobs, and you should see the 2-core job you started with srun:\n\n\nType this into your shell\n\nmyjobs"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#finding-software-on-a-cluster",
    "href": "nmfs-bioinf/slurm.html#finding-software-on-a-cluster",
    "title": "5  Sedna and SLURM intro",
    "section": "5.12 Finding Software on a Cluster",
    "text": "5.12 Finding Software on a Cluster\n\nAnalysis software is not automatically available on a cluster\nOn many clusters you have to install it yourself\nOn Sedna, we are very lucky to have a great support crew that will install software for us, and also make it available for everyone else to use.\n\nHowever, software can be complicated:\n\nSome software might conflict with other software.\nSome users might want different software versions\n\nThe solution: maintain software (and its dependencies) in (somewhat) isolated modules.\n\n\n\n\n\n\nBioinformatic modules on Sedna are only available if the MODULEPATH is specified\n\n\n\nHere we check to see if the bioinformatics paths are in the MODULEPATH:\n\n\nPaste this in your shell\n\necho $MODULEPATH | grep bioinformatics\n\nIf that command did not return anything to stdout, then you need to add a command to your ~/.bashrc that will add /opt/bioinformatics/modulefiles to your MODULEPATH. You can do that (as detailed in the Sedna Google Doc) like this:\n\n\nIf you need to, paste this into the shell\n\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' &gt;&gt; ~/.bashrc\n\nAnd when you are done with that, reload your .bashrc to get the change to take effect:\n\n\nIf you need to, paste this into the shell\n\nsource ~/.bashrc\n\n\n\nMost modules on Alpine are only available if you are on a compute node."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#example-lets-try-running-samtools",
    "href": "nmfs-bioinf/slurm.html#example-lets-try-running-samtools",
    "title": "5  Sedna and SLURM intro",
    "section": "5.13 Example: Let’s try running samtools",
    "text": "5.13 Example: Let’s try running samtools\nsamtools is a common bioinformatic utility.\nTypically, if you run it with no arguments, it gives you some usage information.\nHowever, on Sedna, unless you have installed samtools yourself, or have set your environment up to always have it available, when you type samtools at the shell, you will get a response saying that it is not available:\n\n\nType this at the command prompt\n\nsamtools\n\nThe response I get from the computer is:\nbash: samtools: command not found...\nYou will likely get that response, too.\nOn Alpine, you could install samtools via mamba or you could see if it is available in the system modules."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#check-which-software-programs-are-available-in-the-modules",
    "href": "nmfs-bioinf/slurm.html#check-which-software-programs-are-available-in-the-modules",
    "title": "5  Sedna and SLURM intro",
    "section": "5.14 Check which software programs are available in the modules",
    "text": "5.14 Check which software programs are available in the modules\nThe command module and its subcommands let you interact with the software modules.\nmodule avail tells you what software is available in the modules.\n\n\nType this into your shell\n\nmodule avail\n\nOn SEDNA, there is a lot of tasty bioinformatics software there. On Alpine, there is now a dedicated “Bioinformatics” section in the output of module avail, which is a nice change from Alpine’s predecessor, SUMMIT, which was mostly loaded up with software for geoscientists.\nThe numbers are the version numbers. The (D) after some of them tells us that version is the default."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#load-the-samtools-module",
    "href": "nmfs-bioinf/slurm.html#load-the-samtools-module",
    "title": "5  Sedna and SLURM intro",
    "section": "5.15 Load the samtools module",
    "text": "5.15 Load the samtools module\nWe will load the module for version 1.15.1 of samtools. That can be done like this:\nSEDNA\n\n\nOn SEDNA, Paste this into your shell\n\nmodule load bio/samtools/1.15.1\n\nAlpine\n\n\nOn Alpine, Paste this into your shell\n\nmodule load samtools\n\n(using the load subcommand of module)\n\n\n\n\n\n\nAbout the default versions\n\n\n\nThe same would have been achieved with\nmodule load bio/samtools\nbecause 1.15.1 is the default samtools version.\nNonetheless, it is typically best from a reproducibility standpoint to be explicit about version numbers of software you are using.\n\n\nNow, that it is loaded, you can run the samtools command and get the usage message.\n\n\nType this at your shell\n\nsamtools\n\n\n\n\n\n\n\nInventory and Unload modules\n\n\n\nTo know which modules you have loaded:\nmodule list\nTo unload all the modules you have loaded\nmodule purge\nThere are many more module commands, but those are the only ones that you really need to know."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#two-notes-about-modules",
    "href": "nmfs-bioinf/slurm.html#two-notes-about-modules",
    "title": "5  Sedna and SLURM intro",
    "section": "5.16 Two notes about modules",
    "text": "5.16 Two notes about modules\n\n\n\n\n\n\nLoaded modules are only active in the current session\n\n\n\nIf you open a new shell, or allocate an interactive session on a new core, or logout and log back in again…\n…You will have to reload the modules that you need.\nSo: Any time you write a script that needs some software, you should load the module for that software in the initial lines of the script.\n\n\n\n\n\n\n\n\nModules are not always portable\n\n\n\nNot every cluster that you use will have the same modules set up in the same way. (Though it does seem that Giles has set the modules up on Sedna according to the accepted best practices!).\nSo, if you use code you developed for Sedna (or Alpine) on another cluster, you might have to modify things to ensure that software is available.\nThe conda/mamba package manager is an option if you are going to be running your scripts on different clusters."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-sedna",
    "href": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-sedna",
    "title": "5  Sedna and SLURM intro",
    "section": "5.17 If you need more software or newer versions on Sedna",
    "text": "5.17 If you need more software or newer versions on Sedna\nYou can request software installations by using the Sedna work request form"
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-alpine",
    "href": "nmfs-bioinf/slurm.html#if-you-need-more-software-or-newer-versions-on-alpine",
    "title": "5  Sedna and SLURM intro",
    "section": "5.18 If you need more software or newer versions on Alpine",
    "text": "5.18 If you need more software or newer versions on Alpine\nYou probably can request it from the sys admins, but you will almost indubitably have your required software up and running faster if you install it with mamba."
  },
  {
    "objectID": "nmfs-bioinf/slurm.html#before-we-go-make-our-convenience-functions-available-whenever-we-are-on-our-cluster",
    "href": "nmfs-bioinf/slurm.html#before-we-go-make-our-convenience-functions-available-whenever-we-are-on-our-cluster",
    "title": "5  Sedna and SLURM intro",
    "section": "5.19 Before we go, make our convenience functions available whenever we are on our cluster",
    "text": "5.19 Before we go, make our convenience functions available whenever we are on our cluster\nWe defined four convenience functions in bash: slinfo, slinfop, alljobs, and myjobs.\nOnce we log out of the current session, or login to a different session, those convenience functions will be lost.\nTo make sure that they are accessible the next time we login, we will put them into our own .bashrc files.\nThe .bashrc file is a file in your home directory in which you can put function definitions and other configurations that will be applied whenever you open a new bash shell.\nTo add our four convenience functions to your .bashrc file, first start editing your ~/.bashrc file with nano.\n\n\nPaste this into your shell\n\nnano ~/.bashrc\n\nThen come back to this web page and copy the following text:\n\n\nCopy this onto your clipboard\n\nfunction slinfo {\n  sinfo -N -O nodelist,cpusstate,memory,allocmem\n}\n\nfunction alljobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -o  \"%.12i %.9P %.${JL}j %.15u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L\";\n}\n\nfunction myjobs {\n    if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n    squeue -u $(whoami) -o  \"%.12i %.9P %.${JL}j %.10u %.2t %.15M %.6D %.18R %.5C %.12m %.12l %.12L %p\";\n}\n\nThen paste those function definitions into the editor in your .bashrc file (before your conda/mamba block if you have one), and then do cntrl-X, Y, RETURN to save the file and get out of nano.\nNow, the next time you login you will have all those functions."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#prepare-for-this-session",
    "href": "nmfs-bioinf/sbatch.html#prepare-for-this-session",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.1 Prepare for this session",
    "text": "6.1 Prepare for this session\nGet on your cluster and navigate to the top level (con-gen-csu) of your fork of the class repository. Then, to make sure that you have all the latest updates from the repository, sync the main branch of your fork with the main branch of eriqande/con-gen-csu on GitHub, then in your shell, use git switch main to make sure you are on the main branch of your fork, and then use git pull origin main to pull down those changes."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#the-sbatch-command-and-its-options",
    "href": "nmfs-bioinf/sbatch.html#the-sbatch-command-and-its-options",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.2 The sbatch command and its options",
    "text": "6.2 The sbatch command and its options\nIf you do man sbatch you will see an insanely large number of options and all sorts of complicated stuff.\nYou can get by with a fairly minimal set of options for almost everything you need to do. They are:\n\n--cpus-per-task=&lt;n&gt;: the number of cores to use for the job. The syntax has you using it like this --cpus-per-task=2\n\nOn Sedna, the default is 1.\n\n--mem=&lt;size[units]&gt;: How much total memory for the job. Example: --mem=4G\n\nOn Sedna, the default is about 4.7 Gb for each requested core.\nOn Alpine’s amilan partition, the machines have 3.74 Gb per core.\n\n--time=[D-]HH:MM:SS: How much time are you requesting? You don’t have to specify days, so you could say, --time=1-12:00:00 for one day and twelve hours, or you could say --time=36:00:00 for 36 hours.\n\nOn Sedna, the default is 8 hours.\n\n--output=&lt;filename pattern&gt;: Where should anything on stdout that is not otherwise redirected be written to?\n--error=&lt;filename pattern&gt;: Where should anything on stderr that is not otherwise redirected be written to?\n\nOn top of the options, sbatch takes a single required argument, which must be the path to a shell script (we know about those!) that the job will run.\n\n\n\n\n\n\nFun fact:\n\n\n\nIf you pass any arguments after the name of the shell script that you want sbatch to execute, those are interpreted as arguments to the shell script itself."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#what-an-invocation-of-sbatch-could-look-like",
    "href": "nmfs-bioinf/sbatch.html#what-an-invocation-of-sbatch-could-look-like",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.3 What an invocation of sbatch could look like",
    "text": "6.3 What an invocation of sbatch could look like\nSo, if we wanted to schedule a script called MyScript.sh to run with 4 cores and memory of 80Gb, with an allowance of 16 hours, and we wanted to tell SLURM where to capture any otherwise un-redirected stdout and stderr, we would type something like this:\n\n\nDon't bother copying or pasting this.\n\nsbatch --cpus-per-task=4 --mem=80G --time=16:00:00 --output=myscript_stdout --error=myscript_error MyScript.sh\n\nSome points about that:\n\nTyping all of that is a huge hassle.\nMost of the options will be specific to the actual job in MyScript.sh\n\nSo…sbatch allows you to store the options in your shell script on lines after the shebang line that are preceded by #SBATCH."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#storing-slurm-options-in-your-shell-script",
    "href": "nmfs-bioinf/sbatch.html#storing-slurm-options-in-your-shell-script",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.4 Storing SLURM options in your shell script",
    "text": "6.4 Storing SLURM options in your shell script\nLet’s look at an example like one might do in an sbatch script:\n\n\nContents of scripts/bwa_index.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=2:00:00\n#SBATCH --mem=3G\n#SBATCH --output=bwa_index-%j.out\n#SBATCH --error=bwa_index-%j.err\n\n# this is a little script to index the genome given\n# by the first positional parameter ($1)\n\n# load the module that gives us the bwa software\nmodule load bwa\n\n# make a directory for log output if it does not already exist\nDIR=results/log/bwa_index\nmkdir -p $DIR\n\n# run bwa index on the input\nbwa index $1 &gt; $DIR/log.txt 2&gt;&1\n\n\n\n\n\n\n\nWhat’s that %j in the output and error options?\n\n\n\nIn the above script, you will see\n#SBATCH --output=bwa_index-%j.out\n#SBATCH --error=bwa_index-%j.err\nIn this context, the %j gets replaced by sbatch with the SLURM_JOB_ID.\n\n\n\n\n\n\n\n\nGet email from SLURM.\n\n\n\n\n\nOne useful feature of SLURM—especially if you are running just a few long jobs—is that you can tell it to send you email whenever some event occurs related to a job (like it Starts, or Finishes, or Fails).\nThe SBATCH directives for that look like:\n#SBATCH --mail-user=myemail@emailserver.com\n#SBATCH --mail-type=ALL\nwhere you would replace myemail@emailserver.com with your own email address.\nNote that option --mail-type can be tailored to modulate how much email you will get from SLURM.\nTBH, though, I don’t have SLURM email any news about my jobs anymore. Workflows that are optimized into a lot of smaller jobs would fill your inbox pretty quickly!"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#let-us-submit-the-bwa_index.sh-job-to-slurm",
    "href": "nmfs-bioinf/sbatch.html#let-us-submit-the-bwa_index.sh-job-to-slurm",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.5 Let us submit the bwa_index.sh job to SLURM",
    "text": "6.5 Let us submit the bwa_index.sh job to SLURM\nSince all of the sbatch options are imbedded within the shell script it is easy to write the command to launch the script.\nLet’s prepare for that first. If you are on Alpine, make sure the slurm module is loaded.\n\n\nIf you are working on Alpine, be sure to paste this into your shell\n\nmodule load slurm/alpine\n\nNote that you do not need to be on a compute node to submit a job via sbatch—you can do that from a login node.\nTo launch the script using sbatch, all you do is:\n\n\nPaste this into your shell at the top level of the con-gen-csu repo\n\nsbatch scripts/bwa_index.sh data/genome/genome.fasta\n\nThe first argument is the script scripts/bwa_index.sh and the second, resources/genome.fasta, is the path to the genome that we want to index with bwa.\nWhen this command executes, it returns the SLURM_JOB_ID. Make a note of it.\nOnce you have launched the job, try using myjobs to see your job running. (You don’t have much time, because it doesn’t take very long).\n\n\n\n\n\n\nHey! That job ran in the current working directory\n\n\n\n\n\nNote that our script ran bwa index by passing it the path of a reference genome specified as a relative path: the path was relative to our current working directory.\nOne of the wonderful features of SLURM is that, when sbatch runs your script, it does so from the current working directory of the shell in which you ran sbatch.\n(I mention this because the first cluster I used was set up differently, and you had to explicitly tell it to run from the current working directory—which was the source of endless gnashing of teeth)\n\n\n\nOnce that job is done, use ls -lrt data/genome to see all the files that were newly created by the bwa-index.sh script."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#how-many-resources-did-that-job-use-seff",
    "href": "nmfs-bioinf/sbatch.html#how-many-resources-did-that-job-use-seff",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.6 How many resources did that job use? — seff",
    "text": "6.6 How many resources did that job use? — seff\nWhen you first start doing bioinformatics, you will not be very familiar with how long each job will run, or how much memory it will need.\nThat takes some time, but one helpful utility, seff, will tell you the effective usage of the allocated resources by any completed job.\nIt’s simple to use:\n\n\nHere is the sytnax\n\nseff slurm-jobid\n\n\n\n\n\n\n\nSelf-study\n\n\n\nTry that command, seff slurm-jobid, replacing slurm-jobid with the actual SLURM_JOB_ID of the job that you just ran.\n\n\n\n\n\n\n\n\nMore tips on learning about past job resource use\n\n\n\nYou can also use the sacct command. Check it out with man sacct.\nYou can get information much like seff for your recent jobs with sacct and it is somewhat easier to look at all the jobs that have run (or are running) in the last 12 to 48 hours with it. So, here we define a function to use it easily, and also we can give it more space to print the job names:\n\n\nPaste this into your shell to get a myacct function\n\nfunction myacct {\n  if [ $# -ne 1 ]; then\n      JL=10;\n  else\n      JL=$1;\n  fi;\n  sacct  --format JobID,JobName%${JL},User,Group,State%20,Cluster,AllocCPUS,REQMEM,TotalCPU,Elapsed,MaxRSS,ExitCode,NNodes,NTasks -u $(whoami)\n}\n\nIf you like being able to use this, you ought to add it to your ~/.bashrc file."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-simple-job-with-default-sbatch-settings",
    "href": "nmfs-bioinf/sbatch.html#a-simple-job-with-default-sbatch-settings",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.7 A simple job with default sbatch settings",
    "text": "6.7 A simple job with default sbatch settings\nWe jumped right into talking about all the most useful options for sbatch.\nHowever, on Sedna (and, indeed, on most clusters), SLURM defines reasonable defaults for an sbatch job.\nIt even sends the output and error to reasonably named files, as we shall see.\nThe following lists the contents of a script called simple-15.sh that doesn’t do much:\n\nIt writes out the SLURM_JOB_ID to stdout\nIt writes a message to stderr\nThen it just sits there for 15 minutes.\n\n\n\nContents of scripts/simple-15.sh\n\n#!/bin/bash\n\n\n(\n  echo \"(stdout) Running with Slurm Job ID: $SLURM_JOB_ID\"\n  echo \"(stdout) Note that we are running this thing through\"\n  echo \"(stdout) the tee utility to try to unbuffer it.\"\n  echo\n) | tee simple-15.stdout\n\necho \"(stderr) This line is being written to stderr\" &gt; /dev/stderr\necho \"(stderr) Interestingly, stderr does not seem to get buffered.\" &gt;&gt; /dev/stderr\n\n\nsleep 900\n\n\n\n\n\n\n\nScripts running under SLURM have access to SLURM_* environment variables\n\n\n\nWhen a job is run by SLURM, it is done so in a shell environment that has a number of extra shell variables defined. In the above, we print the value of one of those: SLURM_JOB_ID.\n\n\nNow we will submit that script to run as a SLURM job with:\n\n\nPaste this into your shell\n\nsbatch scripts/simple-15.sh\n\nNow, use myjobs to see how many cores this job is using (1) and how much memory (4700 Mb on Sedna, 3840 on Alpine). Those are the default values.\nBy default, on some systems both stdout and stderr get written to slurm-%j.out (where %j% is replaced with the SLURM_JOB_ID).\nYou can see that is in that by doing:\n\n\nType this, but replace 331979 with whathever your actual SLURM job id is\n\ncat slurm-331979.out\n\nWe see that stderr gets written to slurm-%j.out immediately. The stdout stream is supposed to get written there, as well, but it seems that there is some hard-core buffering that goes on with slurm: we can see the output in simple-15.stdout, but not in slurm-%j.out.\n\n\n\n\n\n\nDon’t leave stdout and stderr un-redirected in your scripts\n\n\n\nSeeing the buffering issues above hardens my own convictions that you should not rely on SLURM to capture any output to stdout or stderr that is not otherwise redirected to a file. You should always be explicit about redirecting stdout or stderr within your own scripts to files where we want it to go.\nThat way the slurm output logs are left to mostly capture messages from SLURM itself (for example, telling you that you ran out of memory or the job was cancelled.)"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#oh-no-i-need-to-stop-my-jobs-scancel",
    "href": "nmfs-bioinf/sbatch.html#oh-no-i-need-to-stop-my-jobs-scancel",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.8 Oh no! I need to stop my job(s): scancel",
    "text": "6.8 Oh no! I need to stop my job(s): scancel\nNow, we have a job that is sitting around doing nothing for the next 15 minutes or so.\nThis is a great time to talk about how to abort jobs that are running under sbatch:\n\n\n\n\n\n\nCancelling jobs started with sbatch\n\n\n\nIf you have the job number (which is returned when sbatch launched the job or which you can see on the corresponding line of myjobs) you can use scancel followed by the job number.\nFor example:\nscancel 329656\n…but you have to replace the number above with your job’s job number.\nPlease do that now!"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-note-about-memory",
    "href": "nmfs-bioinf/sbatch.html#a-note-about-memory",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.9 A note about memory",
    "text": "6.9 A note about memory\n\n\n\n\n\n\nMemory gets allocated with cores or via --mem\n\n\n\nIt is quite clear that the --mem option to sbatch is intended to increase the memory allocated to the job; however adding cores with --cpus-per-task, without adding any options to dictate memory use, also increases the memory available.\nOn Sedna’s standard compute nodes, which each have 20 cores, if you ask for \\(x\\) cores, the total amount of memory your job will get is about \\(\\frac{x}{20} T\\), where \\(T\\) is a little less than the total memory on the machine.\nOn the standard memory compute nodes, that means your job gets roughly 4700 Mb (4.7 Gb) of RAM for each core that it is allocated. (9400 Mb or 9.4 Gb for each core on the “higher-memory” standard compute nodes, node[29-36])\nNote, however, that if the programs running in your job are not multithreaded, then you might not be able to use all those cores. In which case, it might be better to specify additional memory allocation with --mem, and leave the other cores to other users.\nHowever, on Alpine, if you ask for 3.74 Gb * 10 = 37.4 Gb of memory and only one core for computing, you will still be “charged” (i.e., you will run through your quota) as if you were using 10 cores! (i.e., billing is by the core, and you are charged for the number of cores that would give you a certain amount of memory)."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#what-happens-if-we-exceed-our-resources",
    "href": "nmfs-bioinf/sbatch.html#what-happens-if-we-exceed-our-resources",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.10 What happens if we exceed our resources?",
    "text": "6.10 What happens if we exceed our resources?\nR uses 8 bytes for each numeric value it stores in a vector. But it also seems to do some fancy stuff with delayed evaluation, etc. So it is hard to know exactly how much system RAM R’s process will use.\nNonetheless, I have found that the following R commands will exceed 4700 Mb of RAM:\nx &lt;- runif(1e9); y &lt;- x + 1; y[1:10]\nSomewhere in the y &lt;- x + 1 command, memory usage will exceed the default 4700 Mb (4.7 Gb) of RAM that SLURM allows by default (on SEDNA) or the 3.74 Gb of RAM allowed on Alpine.\nSo, what we are going to do here is run those commands in an sbatch script and see what happens. (You will probably exceed your allocated memory while doing bioinformatics, so you might as well get used to what that looks like.)\nHere is a listing of a shell script to run under sbatch to exceed our memory usage:\n\n\nContents of scripts/r-too-much-mem.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=2:00:00\n#SBATCH --output=r-too-much-mem-%j.out\n#SBATCH --error=r-too-much-mem-%j.err\n\n\nmkdir -p outputs\nEDIR=results/log/r-too-much-mem\nmkdir -p $EDIR\n\nmodule load R\nRscript --vanilla -e \"x &lt;- runif(1e9); y &lt;- x + 1; y[1:10]\" &gt; outputs/r-too-much.out 2&gt; $EDIR/log.txt\n\nRun it like this:\n\n\nPaste this into your shell\n\nsbatch scripts/r-too-much-mem.sh\n\nIt takes about a minute before it runs out of memory. During the time, check that it is still running using myjobs.\nOnce you see it is no longer running, check on the status of that job using myacct 15. The bottom line of output should show the results for that last job:\n\nThe State column will show OUT_OF_MEMORY\nThe MaxRSS column shows how much memory it used before failing. In my case, that was 4805260K.\n\n\n\n\n\n\n\nSelf study\n\n\n\nHow would you modify the scripts/r-too-much-mem.sh so that it did not run out of memory?\n\n\n\n\n\n\n\n\nSelf study answer\n\n\n\n\n\nYou need to allocate more memory to the job. You can do this by adding an sbatch directive line like:\n#SBATCH --mem=12G\namongst all the other sbatch directives. I don’t know if 12G of ram will be enough, but it might be. You could try it.\nOR as we will see below, you could add that memory option on the command line."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#sbatch-options-on-the-command-line-will-override-the-sbatch-directives-in-the-file",
    "href": "nmfs-bioinf/sbatch.html#sbatch-options-on-the-command-line-will-override-the-sbatch-directives-in-the-file",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.11 sbatch options on the command line will override the #SBATCH directives in the file",
    "text": "6.11 sbatch options on the command line will override the #SBATCH directives in the file\nYou can mix and match sbatch options on the command line with sbatch options in the #SBATCH directives in the script.\nThe option on the command line takes precedence over the same option in the file.\nSo, we could provide sufficient memory to scripts/r-too-much-mem.sh and at the same time, override the time it is allotted with this command:\n\n\nDon't paste this into your shell---it uses too much resources\n\nsbatch --mem=16G --time=01:00:00 scripts/r-too-much-mem.sh\n\nIf you launched that job, you could use your myjobs function to see that the MIN_MEMORY is 16G and the TIME_LIMIT is 1 hour.\nWhen it finished you could run myacct to see that it successfully completed, and look in the output file outputs/r-too-much.out to see that it printed the first 10 of one billion random numbers with 1 added to them."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#lets-run-out-of-time",
    "href": "nmfs-bioinf/sbatch.html#lets-run-out-of-time",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.12 Let’s run out of time",
    "text": "6.12 Let’s run out of time\nWe are going to rerun our simple-15.sh script, but only allocate one minute for it. So we should see it fail after only 1 minute.\n\n\n\n\n\n\nEric, why are making us run so many jobs that fail?\n\n\n\n\n\nLet’s be honest, as you embark on your bioinformatic career with SLURM, you are going to spend a significant amount of your time dealing with jobs that fail.\nYou might as well see jobs failing in several different ways so that you can recognize them when you see them later.\n\n\n\nSo, try this:\n\n\nPaste this into your shell\n\nsbatch --time=00:01:00 scripts/simple-15.sh\n\nThat will fail in a minute, after which, use myacct to see what it says about how it failed.\nIt will tell you that the main script hit a TIMEOUT and the batch job running in that script was CANCELLED"
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#a-major-gotcha-do-not-direct-sbatchs-output-or-error-to-a-path-that-does-not-exist",
    "href": "nmfs-bioinf/sbatch.html#a-major-gotcha-do-not-direct-sbatchs-output-or-error-to-a-path-that-does-not-exist",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.13 A major gotcha: Do not direct sbatch’s output or error to a path that does not exist",
    "text": "6.13 A major gotcha: Do not direct sbatch’s output or error to a path that does not exist\n\n\n\n\n\n\nBeware!\n\n\n\nIf you try something like:\n#SBATCH output=outputs/mydir/outfile.txt\n#SBATCH error=outputs/mydir/errfile.txt\nbut do so without having actually made the directory outputs/mydir, then sbatch will fail, and it won’t be able to write out why it failed.\nThis is dynamite. If you ever find that your sbatch jobs are failing immediately but with no reasonable error messages anywhere, check to make sure that you aren’t sending SLURM’s output or error to a directory that does not exist.\nNote that you cannot make outputs/mydir within your script, because SLURM needs those directories before your script even gets executed."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#schedule-multiple-jobs-with-sbatch-using-a-for-loop",
    "href": "nmfs-bioinf/sbatch.html#schedule-multiple-jobs-with-sbatch-using-a-for-loop",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.14 Schedule multiple jobs with sbatch using a for loop",
    "text": "6.14 Schedule multiple jobs with sbatch using a for loop\nWe now consider a small job of mapping some paired end reads to the genome that we indexed a few steps ago.\nLet’s review the shell code in scripts/bwa-map.sh:\n\n\nContents of file scripts/bwa_map.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1:00:00\n#SBATCH --output=bwa_map-%j.out\n#SBATCH --error=bwa_map-%j.err\n\n# this is a little script to map a pair of fastq files\n# that are in data/fastqs.\n#\n# The first positional parameter ($1)\n# should be something like: DPCh_plate1_B10_S22\n\n# load the module that gives us the bwa software\nmodule load bwa\nmodule load samtools\n\n# make a directory for log output if it does not already exist\nLDIR=results/log/bwa_map\nODIR=results/mapped\n\nmkdir -p $LDIR $ODIR\n\nSAMP=$1\n\n# run bwa mem on the input and pipe it to samtools to sort it\nbwa mem data/genome/genome.fasta \\\n  data/fastqs/${SAMP}_R1.fq.gz \\\n  data/fastqs/${SAMP}_R2.fq.gz  2&gt; $LDIR/bwa_mem_${SAMP}.log | \\\n(\n  samtools view -u - | \\\n  samtools sort -o $ODIR/$SAMP.bam\n) 2&gt; $LDIR/samtools_$SAMP.log\n\n\nNote that we are using backslashes (\\) at the ends of the lines so that we can break a long line up onto separate lines of text.\nAlso note that grouping of commands by parentheses.\nThis script is run by passing it DPCh_plate1_B10_S22, DPCh_plate1_B11_S23, or DPCh_plate1_B12_S24, etc., as the first postitional parameter.\nSo, if we wanted to schedule three of those mapping jobs, we could do:\n\n\nPaste this code into your shell\n\nfor S in DPCh_plate1_B10_S22 DPCh_plate1_B11_S23 DPCh_plate1_B12_S24; do sbatch scripts/bwa_map.sh $S; done\n\nWhen that is done, check what is running with myjobs and alljobs. (But do it fast! Because these jobs finish very quickly)."
  },
  {
    "objectID": "nmfs-bioinf/sbatch.html#final-thought",
    "href": "nmfs-bioinf/sbatch.html#final-thought",
    "title": "6  Submitting jobs with sbatch",
    "section": "6.15 Final thought",
    "text": "6.15 Final thought\nThough multiple jobs can be submitted via sbatch easily using this sort of for loop construct, there is another way of launching multiple repetitions of the same job in SLURM: using job arrays\nWe will discuss that in the next section."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#prepare-for-this-session",
    "href": "nmfs-bioinf/slurm-arrays.html#prepare-for-this-session",
    "title": "7  Slurm Job Arrays",
    "section": "7.1 Prepare for this session",
    "text": "7.1 Prepare for this session\nGet on your cluster and navigate to the top level (con-gen-csu) of your fork of the class repository. Then, to make sure that you have all the latest updates from the repository, sync the main branch of your fork with the main branch of eriqande/con-gen-csu on GitHub, then in your shell, use git switch main to make sure you are on the main branch of your fork, and then use git pull origin main to pull down those changes."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#sbatchs---array-option",
    "href": "nmfs-bioinf/slurm-arrays.html#sbatchs---array-option",
    "title": "7  Slurm Job Arrays",
    "section": "7.2 sbatch’s --array option",
    "text": "7.2 sbatch’s --array option\nWhen you give sbatch the --array=1-10 option, say, then it runs your job 10 separate times, each time with the environment variable SLURM_ARRAY_TASK_ID set to a different number between 1 and 10.\nLet’s see a quick example, using the following script:\n\n\nContents of scripts/array_example.sh\n\n#!/bin/bash\n#SBATCH --time=00:05:00\n#SBATCH --output=my_output_%A_%a\n#SBATCH --error=my_error_%A_%a\n#SBATCH --array=1-10\n\nODIR=results/array_example\nmkdir -p $ODIR\n\n(\n  echo \"The SLURM_ARRAY_JOB_ID is : $SLURM_ARRAY_JOB_ID\"\n  echo \"The SLURM_ARRAY_TASK_ID is: $SLURM_ARRAY_TASK_ID\"\n  echo \"The SLURM_JOB_ID is: $SLURM_JOB_ID\"\n  echo\n  echo \"You can refer to this individual SLURM array task as: ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}\"\n  echo\n) &gt; $ODIR/output_$SLURM_ARRAY_TASK_ID.txt\n\nsleep 20\n\n\nThis is helpful to see a few of the variables that are defined in the environment of an array job:\n\nSLURM_ARRAY_JOB_ID: the overall JOB_ID for the whole array\nSLURM_ARRAY_TASK_ID: the index that runs from 1 to 10 in this case\nSLURM_JOB_ID: The underlying job_id\n\nOne can run this script, but we don’t want a whole classroom full of people throwing all these jobs on the cluster. So, get into groups of three or four, talk amongst yourselves about what you think this script will do, and then have just one person from each group launch the job with the following command:\n\n\nOne person from each group, paste this into your shell\n\nsbatch scripts/array_example.sh\n\nAnd then use myjobs and alljobs to watch what is happening on the cluster.\nWhen that job is done, or when it is running, look at the values written to the first output files:\n\n\nPaste this into you shell\n\nhead results/array_example/output_{1..10}.txt\n\n\n\n\n\n\n\nCool syntax interlude: {1..10}\n\n\n\n\n\nOn the shell, if you do something like:\n\n{2..7}: that will expand to 2 3 4 5 6 7\n{a..g}: that will expand to a b c d e f g\n{0001..0015}: that will expand to 0001 0002 0003 0004 0005 0006 0007 0008 0009 0010 0011 0012 0013 0014 0015\n{F..M}: that will expand to F G H I J K L M\n\n\n\n\nFrom looking at the array_example output files, we can infer that:\n\nSLURM_ARRAY_JOB_ID: is a unique number that refers to the entire set of jobs in the array\nSLURM_ARRAY_TASK_ID: is the integer that is being cycled over in the array job\nSLURM_JOB_ID: is a unique SLURM_JOB_ID of the specific array task.\n\nAlso, in alljobs and myjobs you see that jobs can be referred to like 331989_4. That is SLURM_ARRAY_JOB_ID + underscore + SLURM_ARRAY_TASK_ID. For example: scancel 376578_12.\nIn the slurm output files specified like:\nSBATCH --output=my_output_%A_%a\n\n%A expands to SLURM_ARRAY_JOB_ID\n%a expands to SLURM_ARRAY_TASK_ID\n\nIf you need to cancel a particular array task using scancel you would typically use SLURM_ARRAY_JOB_ID + underscore + SLURM_ARRAY_TASK_ID, unless you wanted to cancel all remaining instances of an array task, in which case you would use\nscancel SLURM_ARRAY_JOB_ID"
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#variations-on-the-array_spec",
    "href": "nmfs-bioinf/slurm-arrays.html#variations-on-the-array_spec",
    "title": "7  Slurm Job Arrays",
    "section": "7.3 Variations on the <array_spec>",
    "text": "7.3 Variations on the &lt;array_spec&gt;\nThere are some important variations to how you can specify those array numbers:\n\n--array=1-50: simple, consecutive numbers\n--array=1-10:3: 1 through 10 by threes, (so 1,4,7,10)\n--array=1,2,3,6,9,15 non-consecutive numbers\n--array=1-21:10,100-200:50: non-consecutive ranges. It becomes (1,11,21,100,150,200)\n--array=1-10,4: WARNING, this becomes 1,2,3,4,5,6,7,8,9,10,4. SLURM does not check that the array numbers are unique, so task array 4 would be run twice (possibly concurrently overwriting the output.)\n--array=1-20%5 VERY IMPORTANT SYNTAX: Run the jobs, but don’t ever have more the 5 running at a time. This is useful for making sure your jobs don’t consume every last CPU on Sedna.\n\nLet’s try putting all these together. Read the following command and, talking among the members of your group, figure out what the array spec is doing. Then have one person from each group submit the job with the following command:\n\n\nOne person from each group, aste this into your shell\n\nsbatch --array=100,200,300-400:10%5 scripts/array_example.sh\n\nThen use myjobs and alljobs to see what is going on in the cluster.\nDo you ever have more than 5 jobs running?"
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#translating-array-indexes-to-job-instances",
    "href": "nmfs-bioinf/slurm-arrays.html#translating-array-indexes-to-job-instances",
    "title": "7  Slurm Job Arrays",
    "section": "7.4 Translating Array Indexes to Job Instances",
    "text": "7.4 Translating Array Indexes to Job Instances\n\nThe user is left to translate what a job array index of, say, 7, means in terms of what actions that array task should take.\nQuite often you will want to map an array index to a different file to analyze, or perhaps a different region of a chromosome to do variant calling on, etc.\nA flexible and generic way of doing this mapping from array indexes to job specifics is to first define the variables (things like filenames, etc.) required by each array task in a simple TAB-delimited text file in which the first row holds the names of the variables in different TAB-separated columns, and each row below that holds the values that those variables should take for different values of the array index.\nThe array index itself should be listed in the first column, whose name is index.\n\n\n7.4.1 An example TAB-delimited file\nLet’s explore the following file, which is at data/sample-array-info.tsv:\n\n\nContents of inputs/fq-samples.tsv\n\nindex   sample  library flowcell    platform    lane    barcode fq1 fq2\n1   DPCh_plate1_B10_S22 plate1  HTYYCBBXX   ILLUMINA    1   GACGTCAT+TCAACTGG   fastqs/DPCh_plate1_B10_S22_R1.fq.gz fastqs/DPCh_plate1_B10_S22_R2.fq.gz\n2   DPCh_plate1_B11_S23 plate1  HTYYCBBXX   ILLUMINA    1   CCGCTTAA+ACGATGAC   fastqs/DPCh_plate1_B11_S23_R1.fq.gz fastqs/DPCh_plate1_B11_S23_R2.fq.gz\n3   DPCh_plate1_B12_S24 plate1  HTYYCBBXX   ILLUMINA    1   GACGAACT+TCGCATTG   fastqs/DPCh_plate1_B12_S24_R1.fq.gz fastqs/DPCh_plate1_B12_S24_R2.fq.gz\n4   DPCh_plate1_C10_S34 plate1  HTYYCBBXX   ILLUMINA    1   AGTCACAT+CCATAATG   fastqs/DPCh_plate1_C10_S34_R1.fq.gz fastqs/DPCh_plate1_C10_S34_R2.fq.gz\n5   DPCh_plate1_C11_S35 plate1  HTYYCBBXX   ILLUMINA    1   CCTTTCAC+AAACAAGA   fastqs/DPCh_plate1_C11_S35_R1.fq.gz fastqs/DPCh_plate1_C11_S35_R2.fq.gz\n6   DPCh_plate1_C12_S36 plate1  HTYYCBBXX   ILLUMINA    1   GCACACAA+TCGTCAAG   fastqs/DPCh_plate1_C12_S36_R1.fq.gz fastqs/DPCh_plate1_C12_S36_R2.fq.gz\n7   DPCh_plate1_D09_S45 plate1  HTYYCBBXX   ILLUMINA    1   CTTTCCCT+AGTAAGCC   fastqs/DPCh_plate1_D09_S45_R1.fq.gz fastqs/DPCh_plate1_D09_S45_R2.fq.gz\n8   DPCh_plate1_D11_S47 plate1  HTYYCBBXX   ILLUMINA    1   GACAATTC+CATATCGT   fastqs/DPCh_plate1_D11_S47_R1.fq.gz fastqs/DPCh_plate1_D11_S47_R2.fq.gz\n9   DPCh_plate1_F10_S70 plate1  HTYYCBBXX   ILLUMINA    1   ACACGACT+CTGCGGAT   fastqs/DPCh_plate1_F10_S70_R1.fq.gz fastqs/DPCh_plate1_F10_S70_R2.fq.gz\n10  DPCh_plate1_F11_S71 plate1  HTYYCBBXX   ILLUMINA    1   TCCACGTT+GTTCAACC   fastqs/DPCh_plate1_F11_S71_R1.fq.gz fastqs/DPCh_plate1_F11_S71_R2.fq.gz\n11  DPCh_plate1_F12_S72 plate1  HTYYCBBXX   ILLUMINA    1   AACCAGAG+AACCGAAG   fastqs/DPCh_plate1_F12_S72_R1.fq.gz fastqs/DPCh_plate1_F12_S72_R2.fq.gz\n12  DPCh_plate1_G09_S81 plate1  HTYYCBBXX   ILLUMINA    1   CGAATACG+GTCGGTAA   fastqs/DPCh_plate1_G09_S81_R1.fq.gz fastqs/DPCh_plate1_G09_S81_R2.fq.gz\n13  DPCh_plate1_G10_S82 plate1  HTYYCBBXX   ILLUMINA    1   CAGTGCTT+ACCTGGAA   fastqs/DPCh_plate1_G10_S82_R1.fq.gz fastqs/DPCh_plate1_G10_S82_R2.fq.gz\n14  DPCh_plate1_G12_S84 plate1  HTYYCBBXX   ILLUMINA    1   TCCATTGC+AGACCGTA   fastqs/DPCh_plate1_G12_S84_R1.fq.gz fastqs/DPCh_plate1_G12_S84_R2.fq.gz\n15  DPCh_plate1_H09_S93 plate1  HTYYCBBXX   ILLUMINA    1   GTCGATTG+ACGGTCTT   fastqs/DPCh_plate1_H09_S93_R1.fq.gz fastqs/DPCh_plate1_H09_S93_R2.fq.gz\n16  DPCh_plate1_H10_S94 plate1  HTYYCBBXX   ILLUMINA    1   ATAACGCC+TGAACCTG   fastqs/DPCh_plate1_H10_S94_R1.fq.gz fastqs/DPCh_plate1_H10_S94_R2.fq.gz\n\nAn easier way to view this might be to look at in on GitHub at: here\nSomething that would be really handy would be a little shell script that would pick out a particular line of that file that corresponds to the value in the index column and then define some shell variables according to the columns names: index, sample, library, flowcell, platform, lane, barcode, fq1, fq2 so that they could be used in an array script.\nWe have already seen some of the bash and awk machinery that would make that possible, and we have wrapped it up in the line-assign.sh script.\n\n\n7.4.2 The line-assign.sh script\nWithin the repository is a script called scripts/line-assign.sh, that looks like this:\n\n\nContents of scripts/line-assign.sh\n\n#!/bin/bash\n# simple script.  Arguments are:\n#  1. The index to pick out (like 1, or 2, or 7, etc)\n#  2. Path to a TAB delimited file with the first column named index\n#     and subsequent columns named valid shell variable names.\n#\n# This script will pick out the line that matches $1, and return a command\n# line assigning the column header names as shell variables whose values are the\n# values in each cell in the file $2.\n#\n# Note: this is not written with a whole lot of error checking or catching.\n#\n\n\nif [ $# -ne 2 ]; then\n  echo \"Wrong number of arguments in $0 \" &gt; /dev/stderr\nfi\n\nawk -F\"\\t\" -v LINE=$1 '\n  $1 == \"index\" {for(i=1; i&lt;=NF; i++) vars[i]=$i; next}\n  $1 == LINE {for(i=1; i&lt;=NF; i++) printf(\"%s=\\\"%s\\\"; \", vars[i], $i); printf(\"\\n\");}\n' $2\n\nLet’s see what sorts of results this produces:\n\n\nPaste this into your shell\n\n ./scripts/line-assign.sh 3 data/sample-array-info.tsv\n\nWhoa! It returns a command line that assigns values to a lot of shell variables.\nSo, if we wanted to run that command line, we would have to precede it with the eval keyword (because the command line itself includes special characters like =). Let’s do that like this:\n\n\nPaste this into your shell\n\nCOMM=$(./scripts/line-assign.sh 3 data/sample-array-info.tsv)\neval $COMM\n\nNow, that you have done that, you can see that a lot of variables have been assigned the values on the index == 3 line of our TSV file:\n\n\nPaste this into your shell\n\n(\necho \"index:      $index\"\necho \"sample:     $sample\"\necho \"library:    $library\"\necho \"platform:   $platform\"\necho \"lane:       $lane\"\necho \"barcode:    $barcode\"\necho \"fq1:        $fq1\"\necho \"fq2:        $fq2\"\n)\n\nHoly Smokes! These are variables that we could use in a job array script.\nIn this case, we can assign values to the pesky Read Group string for bwa mem."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#putting-it-all-together-a-read-mapping-job-array",
    "href": "nmfs-bioinf/slurm-arrays.html#putting-it-all-together-a-read-mapping-job-array",
    "title": "7  Slurm Job Arrays",
    "section": "7.5 Putting it all together: a read-mapping job array",
    "text": "7.5 Putting it all together: a read-mapping job array\nWe can now elaborate on our simple bwa_map.sh shell script and turn that into a SLURM job array script. The key here is that when SLURM runs the script as a slurm array, there will be an environment variable called SLURM_ARRAY_TASK_ID that we can use to pick out the desired sample from the data/sample-array-info.tsv file.\nHere is what it looks like:\n\n\nContents of scripts/bwa_map_array.sh\n\n#!/bin/bash\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=1:00:00\n#SBATCH --output=bwa_map_array-%A_%a.out\n#SBATCH --error=bwa_map_array-%A_%a.err\n#SBATCH --array=1-16\n\n# load the module that gives us the bwa software\nmodule load bwa\nmodule load samtools\n\n# make a directory for log output if it does not already exist\nLDIR=results/log/bwa_map_array\nODIR=results/mapped\n\nmkdir -p $LDIR $ODIR\n\n# get shell variables for this array task:\nCOMM=$(./scripts/line-assign.sh $SLURM_ARRAY_TASK_ID data/sample-array-info.tsv)\neval $COMM\n\n\n# run bwa mem on the input and pipe it to samtools to sort it\nbwa mem \\\n  -R \"@RG\\tID:${sample}.${library}.${flowcell}.${lane}\\tSM:${sample}\\tLB:${library}\\tBC:${barcode}\\tPU:\\tID:${sample}.${library}.${flowcell}.${lane}.${barcode}\\tPL:ILLUMINA\" \\\n  data/genome/genome.fasta \\\n  $fq1 $fq2  2&gt; $LDIR/bwa_mem_$sample.log | \\\n(\n  samtools view -u - | \\\n  samtools sort -o $ODIR/$sample.bam\n) 2&gt; $LDIR/samtools_$sample.log\n\nThe main differences from our simple bwa_map.sh script are:\n\nThe SLURM_ARRAY_TASK_ID is used to pick out the right line from our TSV file of information\nThe slurm output and error have been changed to use %A_%a notation.\nThere is a #SBATCH --array=1-16 directive, to run it over the 16 different files.\nThe shell code that runs bwa and samtools uses the variables that were defined by eval-ing the results of the call to the line-assign.sh script.\n\nYou can launch that job array and see how it goes:\n\n\nPaste this into your shell\n\nsbatch scripts/bwa_map_array.sh\n\nThen check with myjobs and alljobs to see what is happening on Sedna.\nThat runs really fast.\nAfterward you can check that the results exist:\n\n\nPaste this into your shell\n\nls -l results/mapped\n\nAnd you can look at the logs for bwa mem:\n\n\nPaste this into your shell\n\ntail  results/log/bwa_map_array/bwa_mem_*\n\nTo see what one of the output files looks like, you can use samtools. On Alpine, you have to get onto a compute node to do that, so you would first do:\n\n\nDo this if you are on the Alpine cluster\n\nmodule load slurm/alpine\nsrun --partition=atesting --pty /bin/bash\n\nThen do like this:\n\n\nPaste this into your shell\n\nmodule load samtools\nsamtools view results/mapped/DPCh_plate1_F10_S70.bam | less -S\n\nUse the left and right arrows, and space bar and backspace to see all parts of the file.\nRemember to hit q to get out of the less viewer."
  },
  {
    "objectID": "nmfs-bioinf/slurm-arrays.html#wrap-up",
    "href": "nmfs-bioinf/slurm-arrays.html#wrap-up",
    "title": "7  Slurm Job Arrays",
    "section": "7.6 Wrap Up",
    "text": "7.6 Wrap Up\nSo, that was a quick tour of the capabilities of sbatch.\nOddly enough, I haven’t directly launched any jobs using sbatch (apart from the example jobs for this course) in many months, because I drive all my bioinformatics projects using Snakemake, which sends my jobs to SLURM for me.\nEventually we will have an introduction to Snakemake!"
  },
  {
    "objectID": "nmfs-bioinf/snake.html",
    "href": "nmfs-bioinf/snake.html",
    "title": "8  Snakemake Tutorial Introduction",
    "section": "",
    "text": "Snakemake is a Python-based system for orchestrating bioinformatic workflows.\nIt is loosely based on the GNU utility make that was designed for coordinating the compilation of large software projects.\nBut, it is tailored to bioinformatic problems and it is much easier to use the make (in my opinion).\nNonetheless, it has a bit of a learning curve.\n\nOur goal today is to let y’all interact with Snakemake on a small example project, to get a sense for some of its features.\nThis part of the presentation is structured as a set of slides, rather than as a book.\nPlease clink on this following link:\nSnakemake Slides"
  }
]